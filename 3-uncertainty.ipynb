{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10403948,"sourceType":"datasetVersion","datasetId":6190985}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Trustworthy Machine Learning**\n\n### Winter Semester 2024-2025\n\n### Lecturer: Seong Joon Oh\n\n### Tutor: Lennart Bramlage\n\n### **Exercise 3 -- Uncertainty**","metadata":{"_uuid":"b1b49b09-160d-46f5-9554-e8606d83f464","_cell_guid":"7f2f76ff-3a01-409b-91ba-ec6ac36b2634","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---\n\n\n\n**Group number**: >>> PLEASE FILL IN <<<\n\n\n\n**Student names**: >>> PLEASE FILL IN <<<\n\n\n\n**Student emails**: >>> PLEASE FILL IN <<<\n\n\n\n\n\n---","metadata":{"_uuid":"3d95c1ba-7191-4bfc-a5c0-04b1857f69a7","_cell_guid":"bfda75fb-e824-4e92-8bc7-b73d79d22713","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"\n\n\n\n#### **Submission deadline: 29/01/2025 at 23:59.**\n\nThis exercise is a **group exercise**. The same grade will be conferred to each member of the group based on the submission. Please report cases where any team member contributes significantly less than the other members of the same group. the The grade from this exercise will count towards the final grade.\n\n\n\n#### **How to use GPUs**\n\n- Verify your phone number.\n\n- Select your preferred GPU at `Settings > Accelerator`.\n\n- Put parameters and tensors on CUDA via `tensor.to(device)` etc.\n\n- Double check if the parameters and tensors are on CUDA via `tensor.device` etc.\n\n\n\n#### **Submission**\n\nFollow the below three steps.\n\n\n\n(1) Click on `File > Download notebook`;\n\n\n\n(2) Send the `.ipynb` file to `stai.there@gmail.com` before the deadline.","metadata":{"_uuid":"937a5a11-ba7a-4fb4-8729-c0c3cb416d7a","_cell_guid":"780577f5-b7f7-4b28-9eea-125ab84e1a67","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"First, let's install additional libraries we will need in this exercise.","metadata":{"_uuid":"562844a9-0547-4f2d-8b89-d7509f6ee9c5","_cell_guid":"b1ce2a84-f0a8-45ae-a07b-ca7ce0ca38d9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%pip install -q torchmetrics datasets transformers","metadata":{"_uuid":"2d57da96-5709-41e0-a2e2-753187fdb208","_cell_guid":"19ebef7b-2df9-4ca6-83a9-8b540acc45ce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:31:24.578739Z","iopub.execute_input":"2025-01-29T15:31:24.579146Z","iopub.status.idle":"2025-01-29T15:31:38.073816Z","shell.execute_reply.started":"2025-01-29T15:31:24.579112Z","shell.execute_reply":"2025-01-29T15:31:38.072267Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"PATH = \"../input/miscellaneous\"\nfrom __future__ import annotations\nimport os\nimport random\nimport sys\nimport time\nfrom copy import deepcopy\nfrom typing import Callable, Optional\n\n%config InlineBackend.figure_formats = ['svg']\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lrs\nimport torchvision.transforms as transforms\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score\nfrom torch import Tensor\nfrom torch.distributions import Normal\nfrom torch.nn import Module\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchmetrics import AUROC, Accuracy\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n)\n\nsys.path.insert(0, PATH)\nfrom unc_utils import entropy, get_cifar_loaders\n\ndef apply_random_seed(random_seed: int) -> None:\n    \"\"\"Sets seed to ``random_seed`` in random, numpy and torch.\"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\napply_random_seed(2025)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device}\")","metadata":{"_uuid":"fe7be74e-8525-4ce5-a013-186a27fa7440","_cell_guid":"011de68b-9e94-4ef7-8a03-a7d93d12eaab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:42:15.563787Z","iopub.execute_input":"2025-01-29T15:42:15.564593Z","iopub.status.idle":"2025-01-29T15:42:15.583217Z","shell.execute_reply.started":"2025-01-29T15:42:15.564552Z","shell.execute_reply":"2025-01-29T15:42:15.581799Z"}},"outputs":[{"name":"stdout","text":"Using cpu\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **1. Calibration and Proper Scoring (32 points)**\n**Recommended start: 09.01.2025**","metadata":{"_uuid":"d8d1724c-ff8e-4dd9-bbdf-e74c918ed9ea","_cell_guid":"d9f4830a-1114-44e6-83ba-411521a8ac33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1.1 Proper scoring rules (6 points)\n\n### 1.1.1 Proper scoring rule (3 points)\nProve that in a binary classification setup, the negative binary cross-entropy loss is a strictly proper scoring rule.\n\nThe binary cross-entropy (BCE) loss is defined as $\\mathcal{L}(q, y) = - (y \\log(q) + (1-y) \\log(1-q))$, where $y \\in \\{0, 1\\}$ is the binary label and $q = \\hat{P}(Y=1)$ our predicted probability.\n\n*Hint: Here, our target is predicting the correct $P(Y = 1)$. Predicting the correct $P(L = 1)$ as defined in the lecture would also be a valid target, but you do not need to do that in this exercise.*","metadata":{"_uuid":"f4d50986-2814-4678-942b-a7d4365e284f","_cell_guid":"604a618c-529a-49eb-8d55-f8644e3946ba","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{"_uuid":"c193ec54-b1d7-4513-8b31-24146cb2d813","_cell_guid":"13d2b9a6-2120-4bdf-80f5-d6a3d50f6513","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 1.1.2 A non-proper scoring rule (3 points)\n\nShow that, unlike BCE, the following loss function is not a strictly proper scoring rule. \n$\\mathcal{L}(q,y) = - y q + (1 - y) (1 - q)$, where $y \\in \\{0, 1\\}$ is again the binary label and $q = \\hat P(Y=1)$ our predicted probability.","metadata":{}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Detecting uncalibrated models (26 points)\n\nIn this exercise, we will investigate the predictions of four binary classifiers,\n\n* a calibrated and accurate model,\n* a calibrated but inaccurate model,\n* an uncalibrated but accurate model, and\n* an uncalibrated and inaccurate model\n\nnoted `model1_outputs.csv`, `model2_outputs.csv`, `model3_outputs.csv`, and `model4_outputs.csv`. They contain model predictions $\\hat{P}(Y=1)$ in the first column and the ground-truth $Y$ values in the second. Using evaluation methods of probabilistic forecasts, you will identify which is which.","metadata":{"_uuid":"781154ce-832e-4475-a0f0-967ddc062719","_cell_guid":"604f4b0a-5856-4222-95f8-c11363abebe2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model1 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model1_outputs.csv\"), delimiter=\",\")\n)\nmodel2 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model2_outputs.csv\"), delimiter=\",\")\n)\nmodel3 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model3_outputs.csv\"), delimiter=\",\")\n)\nmodel4 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model4_outputs.csv\"), delimiter=\",\")\n)","metadata":{"_uuid":"671b2f8c-8f68-4f1f-b1f2-668615d1e495","_cell_guid":"d5b22d65-a07d-412d-a39a-cb19764f82ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.1 Implement uncertainty measures (4 + 4 + 2 + 5 points)\n\n**Implement the following evaluation methods by filling out the code below.**\n\n* Expected calibration error (ECE)\n* Reliability diagram\n* Negative log-likelihood score (NLL)\n* Area under the ROC curve (AUROC)\n\nYou may use `matplotlib.pyplot, torch, numpy`, but not `torch.nn` or any other libraries. Feel free to implement any other metric for investigative purposes in later tasks.\n\n*Hint: You may want to write a helper function that you can use for both ECE and the reliability diagram.*","metadata":{"_uuid":"581817c2-129d-48fe-98e2-f23ffb8f3868","_cell_guid":"59444e8f-eca0-43be-9474-8e3e41f3633f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def ece_score(pred_prob: Tensor, y: Tensor, n_bins: int = 10) -> Tensor:\n    \"\"\"Computes the expected calibration error.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n        n_bins: Number of (equally sized) bins.\n\n    Returns:\n        The ECE in a float tensor of shape (1,).\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return ece_score","metadata":{"_uuid":"c5140928-16a3-4b96-8a8c-0bffbe04b922","_cell_guid":"c56cc691-7342-424e-8eb0-9f8298cb3e40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reliability_diagram(pred_prob: Tensor, y: Tensor, n_bins: int = 10) -> None:\n    \"\"\"Visualizes a reliability diagram.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n        n_bins: Number of (equally sized) bins.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    plt.gca().set_aspect(\"equal\")","metadata":{"_uuid":"9c6cc4c1-2004-4ac0-9ba7-852cc06c0978","_cell_guid":"9b8ca68a-4ada-4510-9733-527204a6316d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def nll_score(pred_prob, y):\n    \"\"\"Computes the negative log-likelihood score.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n        n_bins: Number of (equally sized) bins.\n\n    Returns:\n        The NLL in a float tensor of shape (1,).\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return nll_score","metadata":{"_uuid":"34f19dba-df1a-45e4-936b-7ec1b871f1ff","_cell_guid":"8461a2f4-9402-42a4-9bca-1fbcd00e80a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are several ways to calculate the AUROC. We'll use a simplified one for the sake of this exercise. First, calculate false-positive and true-positive rates for all possible thresholds (Which are these?). The AUROC is an integral over this curve. You don't need to implement fancy trapezoidal rules nor triangles to approximate this integral, but may simply use rectangles below the curve.","metadata":{"_uuid":"490703e2-6b21-4b05-baa6-9b19d5a4de83","_cell_guid":"af8987b0-e53c-47e1-89cf-07f9bd5fa3df","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def auroc_score(pred_prob: Tensor, y: Tensor) -> Tensor:\n    \"\"\"Computes the area under the ROC curve score.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n\n    Returns:\n        The AUROC in a float tensor of shape (1,).\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return auroc_score","metadata":{"_uuid":"6907ddb7-ca23-4723-b92c-1756db69c83b","_cell_guid":"422096ae-a0a1-4648-8c87-12c1e0151ddd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.2 Interpret the results (4 points)\nApply your scores implemented above to the model results and **argue which model is which and why.**\nFor uncalibrated models, also answer **whether they are overconfident, underconfident, or a mixture**.\n\nIf you could not implement the models in 1.2.1, you may use existing libraries to gain points here. Feel free to add additional metrics of existing libraries if you find them useful for finding out which model is which (or implement them yourself if you feel adventurous).\n\n*Hint: Be critical of your metrics! Each metric only evaluates a certain property of the model. Some metrics might even be misleading due to other properties of the model that are not easily visible.*","metadata":{"_uuid":"bc7fd7da-c0d1-4344-9ab2-a4c9f8f8639a","_cell_guid":"055ce582-ffd6-4028-810b-0a47d0faef50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(f\"Model1 ECE: {ece_score(model1[:,0], model1[:,1]).item()}\")\nprint(f\"Model1 NLL: {nll_score(model1[:,0], model1[:,1]).item()}\")\nprint(f\"Model1 AUROC: {auroc_score(model1[:,0], model1[:,1]).item()}\")\nreliability_diagram(model1[:, 0], model1[:, 1])\nplt.show()","metadata":{"_uuid":"fb2cfdee-2742-4b47-96d9-42bab9a8f071","_cell_guid":"a5ca62e4-ab94-429e-8de0-ef8735549705","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model2 ECE: {ece_score(model2[:,0], model2[:,1]).item()}\")\nprint(f\"Model2 NLL: {nll_score(model2[:,0], model2[:,1]).item()}\")\nprint(f\"Model2 AUROC: {auroc_score(model2[:,0], model2[:,1]).item()}\")\nreliability_diagram(model2[:, 0], model2[:, 1])\nplt.show()","metadata":{"_uuid":"e2474d03-3df8-4d13-abfb-39af33b1e9ec","_cell_guid":"bd8d5a7b-0ede-494e-bcaa-8439a8858787","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model3 ECE: {ece_score(model3[:,0], model3[:,1]).item()}\")\nprint(f\"Model3 NLL: {nll_score(model3[:,0], model3[:,1]).item()}\")\nprint(f\"Model3 AUROC: {auroc_score(model3[:,0], model3[:,1]).item()}\")\nreliability_diagram(model3[:, 0], model3[:, 1])\nplt.show()","metadata":{"_uuid":"330cb41d-d676-4ef1-8bf5-0028cf69871e","_cell_guid":"e1c2a202-0b1a-4eab-9f66-8a7c20b10082","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model4 ECE: {ece_score(model4[:,0], model4[:,1]).item()}\")\nprint(f\"Model4 NLL: {nll_score(model4[:,0], model4[:,1]).item()}\")\nprint(f\"Model4 AUROC: {auroc_score(model4[:,0], model4[:,1]).item()}\")\nreliability_diagram(model4[:, 0], model4[:, 1])\nplt.show()","metadata":{"_uuid":"c3468355-c3de-47c0-ab7b-bb6d53c263b8","_cell_guid":"dd744f60-1ba3-44ff-8486-d55c689436fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model XY is calibrated and accurate** because **WRITE YOUR ANSWER HERE**.\n\n**Model XY is uncalibrated but accurate** because **WRITE YOUR ANSWER HERE**. **It is overconfident/underconfident/mixture of both** because **WRITE YOUR ANSWER HERE**.\n\n**Model XY is calibrated but inaccurate** because **WRITE YOUR ANSWER HERE**.\n\n**Model XY is uncalibrated and inaccurate** because **WRITE YOUR ANSWER HERE**. **It is overconfident/underconfident/mixture of both** because **WRITE YOUR ANSWER HERE**.\n\n**Bonus question (2 points)**: Why does the reliability diagram of model 3 look so weird?","metadata":{"_uuid":"1e01d30b-11be-43ff-b1de-7594fd63b4c4","_cell_guid":"ae77ae7c-e755-48ed-b78a-2af9f676bbd9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"### 1.2.3 Cheating the ECE (4 points)\n\nThe final model, `model5_outputs.csv`, has a very bad looking ECE. The ground-truth $y$ values were unfortunately randomly shuffled (`model5_truths_shuffled.csv`) so there is no option to improve performance in a scientifically rigorous way. Let's cheat (Yikes!) to make the score look better on paper than the model is.\n\n**Write a function that adapts the predicted probabilities to increase the ECE score of the model** (with `n_bins = 10`). To make sure that you don't cheat while cheating, `model5_truths_shuffled.csv` only contains a randomly permuted variant of the ground-truth values. The current ECE of model5 on the real ground-truth values is 0.25. You get full points if you manage to decrease it below 0.2 by any means.\n\n*Hint: Test your function by applying it to models 1-4 and calculating their ECE.*","metadata":{"_uuid":"f651cf59-447c-4262-b393-10a85d6df21b","_cell_guid":"a833e5f1-d7f1-4e3d-9f40-06a4594063a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model5 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model5_outputs.csv\"), delimiter=\",\")\n)\nmodel5_shuffled_y = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model5_truths_shuffled.csv\"), delimiter=\",\")\n)","metadata":{"_uuid":"2178a14e-d5bf-4b35-bba6-82c8acfae683","_cell_guid":"78d6a220-0558-4c8e-854e-d43c6a80b5ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def increase_ece(pred_prob: Tensor, shuffled_y: Tensor):\n    \"\"\"Adapts the predicted probabilities to make them look better under the ECE measure.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing randomly permuted 0 and 1 labels.\n\n    Returns:\n        A float tensor of the same shape as pred_prob with adapted probabilities.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return new_pred_prob\n\n\n# A small verification for yourselves on data where you know the GT y:\nold_ece = ece_score(model4[:, 0], model4[:, 1]).item()\nnew_ece = ece_score(\n    increase_ece(model4[:, 0], model4[torch.randperm(model4.shape[0]), 1]), model4[:, 1]\n).item()\nprint(f\"Model 4: Old ECE was {old_ece} and new ECE is {new_ece}\")","metadata":{"_uuid":"b140d702-7a36-4cab-8666-724284369859","_cell_guid":"218aa156-da14-40ff-9111-6220d97b310c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.4 Cheat prevention (3 points)\nA model can abuse the above adversarial techniques to decrease the ECE by returning probabilities that aren't its true beliefs. **Argue why any cheat like this is impossible for NLL (unless the model truly becomes better).**","metadata":{"_uuid":"6bfa2b5f-df76-4761-b8e0-54b96112b926","_cell_guid":"783d802f-1b8a-46f2-a731-a8ced71e4b8f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE** (no proof required)","metadata":{}},{"cell_type":"markdown","source":"**Bonus question (4 points)**: Comparing scoring rules. In the lecture, we have seen a few strictly proper scoring rules for predictive uncertainty, and also some lower bounds thereof. Below, we provide some boilerplate for downloading the CIFAR-10 dataset.\n\nTry out any two proper scoring rules from the lecture as training objectives. Make sure you provide a fair comparison: some scoring rules might require different hyperparameters than others. Do you get different accuracies and calibration? Use the evaluation methods you have implemented above to support your findings.","metadata":{"_uuid":"2b100e5c-14e1-4fe4-a79a-a5bafa43c849","_cell_guid":"1e6cbdd4-0975-4494-955b-3e367017a978","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"LEARNING_RATE = 0.001\nBATCH_SIZE = 64\nEPOCHS = 10","metadata":{"_uuid":"d994c13c-1d4a-42b2-a750-f358c21374d9","_cell_guid":"a63b331f-d791-4c5c-937d-e7876626d289","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_cifar_loaders()","metadata":{"_uuid":"095c90fb-2be4-407a-8f6f-438aa2ba77d9","_cell_guid":"1c06d5f9-8c56-489a-821b-7cda506b9b50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleNet(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5)\n        self.conv2 = nn.Conv2d(in_channels=10, out_channels=30, kernel_size=3)\n        self.conv3 = nn.Conv2d(in_channels=30, out_channels=50, kernel_size=3)\n        self.fc = nn.Linear(in_features=50 * 4 * 4, out_features=10)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = self.fc(x)\n\n        return x","metadata":{"_uuid":"bd565ce5-22ba-4b18-8aad-09d1699685da","_cell_guid":"8f88a845-14cb-4d75-9af0-23c466592291","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_single_loop(\n    model: SimpleNet,\n    device: str,\n    train_loader: DataLoader,\n    optimizer: Adam,\n    loss_fn: Callable,\n) -> None:\n    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n\n\ndef val(\n    model: SimpleNet,\n    device: str,\n    val_loader: DataLoader,\n    epoch: int,\n    loss_fn: Callable,\n) -> None:\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = loss_fn(output, target)\n            pred = output.argmax(dim=-1)\n\n            val_loss += loss_fn(output, target).item()\n            correct += (pred == target).sum().item()\n\n    val_loss /= len(val_loader)\n    val_accuracy = 100 * correct / len(val_loader.dataset)\n\n    print(\n        f\"[Epoch {epoch:2d}]: Average loss: {val_loss:.4f}, \"\n        f\"Accuracy: {correct}/{len(val_loader.dataset)} ({val_accuracy:.0f}%)\"\n    )\n\n\ndef train(\n    model: SimpleNet,\n    device: str,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    optimizer: Adam,\n    loss_fn: Callable,\n    scheduler: Optional[MultiStepLR] = None,\n    num_epochs: int = 10,\n) -> None:\n    model = model.to(device)\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_single_loop(model, device, train_loader, optimizer, loss_fn)\n\n        model.eval()\n        val(model, device, val_loader, epoch, loss_fn)\n\n        if scheduler is not None:\n            scheduler.step()\n\n    print(\"Done!\")","metadata":{"_uuid":"49be55f7-e670-4960-939e-c9467bfffe5a","_cell_guid":"fb5128cc-af3e-48c0-9cb3-094fb23a9ae6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### >>>> PUT YOUR SOLUTION HERE <<<<\n# Implement your losses/scores of choice from the lecture.\n# Train the models, tune the hyperparameters.\n# Evaluate the models using the metrics you implemented above (on the test dataloader).\n#### >>>> END OF YOUR SOLUTION <<<<","metadata":{"_uuid":"42023879-ff64-4db9-9732-4913c0142d36","_cell_guid":"91eb19d3-6871-4d65-81d7-00c10097629f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. Epistemic Uncertainty and Ensembles (25 points)**\n**Recommended start: 16.01.2025**","metadata":{"_uuid":"6c6f633b-970b-4756-ab0f-14845ba3a69b","_cell_guid":"cf211fbc-a64e-4452-92dd-89f706e96045","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 2.1 Epistemic Uncertainty Applications (6 points)\n\nFind two papers that are concerned with two different applications of epistemic uncertainty. You're allowed to cite any paper except those mentioned in the lecture. You don't need to read the whole paper, but can instead practice quick [paper skimming](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf): Read only the title, abstract, introduction, section titles, figures, and the conclusion. Once you found compelling papers, **shortly address the following tasks for each of your two chosen papers:**\n\n0. Provide a link to the paper.\n1. Summarize the motivating problem of the paper.\n2. How is this problem related to epistemic uncertainty?\n3. How relevant do you find this problem in practice?","metadata":{"_uuid":"efe2fe24-ec00-425e-9518-0dc3c538ce3a","_cell_guid":"b6c64f3d-9bef-45ff-8448-9612338b3a00","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Setup for Experiments (0 points)","metadata":{"_uuid":"6b9a6235-3689-4d86-a623-020f70ad2a55","_cell_guid":"623ce60b-7f03-45d8-a91b-45df44bf2060","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this exercise, we want to train ensemble methods to output epistemic uncertainties in an OOD task. In particular, we want to train on CIFAR-10 as in-distribution and test on CIFAR-100 as out-of-distribution dataset, so let's first load these two datasets:","metadata":{"_uuid":"05c28ce3-bac0-41f3-8c88-103462d208fe","_cell_guid":"2fffd47c-18db-4261-bd3f-557272340287","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n    ]\n)\n\ntraindata_10 = datasets.CIFAR10(\n    root=\"./data_CIFAR10_train\", train=True, download=True, transform=transform\n)\ntrainloader_10 = DataLoader(\n    traindata_10, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n)\n\ntestdata_10 = datasets.CIFAR10(\n    root=\"./data_CIFAR10_test\", train=False, download=True, transform=transform\n)\ntestloader_10 = DataLoader(\n    testdata_10, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n)\n\n# We will not need the CIFAR100 train dataset\n\ntestdata_100 = datasets.CIFAR100(\n    root=\"./data_CIFAR100_test\", train=False, download=True, transform=transform\n)\ntestloader_100 = DataLoader(\n    testdata_100, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n)","metadata":{"_uuid":"45df002b-ad47-4612-aac8-2e76feb6ea4c","_cell_guid":"6abb410b-85fb-4f4d-8639-e074e9f9d56e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:42:39.051332Z","iopub.execute_input":"2025-01-29T15:42:39.051747Z","iopub.status.idle":"2025-01-29T15:43:26.666845Z","shell.execute_reply.started":"2025-01-29T15:42:39.051713Z","shell.execute_reply":"2025-01-29T15:43:26.665724Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10_train/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:12<00:00, 13890760.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data_CIFAR10_train/cifar-10-python.tar.gz to ./data_CIFAR10_train\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10_test/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:11<00:00, 14625517.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data_CIFAR10_test/cifar-10-python.tar.gz to ./data_CIFAR10_test\nDownloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data_CIFAR100_test/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:11<00:00, 14832269.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data_CIFAR100_test/cifar-100-python.tar.gz to ./data_CIFAR100_test\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"We will use a simple CNN as architecture.","metadata":{"_uuid":"e563cc1b-2652-49f6-8ccc-0fe8dc14e6be","_cell_guid":"b57c693d-35be-480b-a99b-f833e84efd60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, x):\n        # x: (b, 3, 32, 32)\n        x = self.pool(F.relu(self.conv1(x)))\n        # x: (b, 6, 14, 14)\n        x = self.pool(F.relu(self.conv2(x)))\n        # x: (b, 16, 5, 5)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch.\n        # x: (b, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        # x: (b, 120)\n        x = F.relu(self.fc2(x))\n        # x: (b, 84)\n        x = self.fc3(x)\n        # x: (b, 10)\n        return x","metadata":{"_uuid":"1fbf16ef-abe7-4fe4-9d80-9a9af129f1ae","_cell_guid":"42be5014-da9f-4d3c-8944-5799cf319861","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:43:26.698253Z","iopub.execute_input":"2025-01-29T15:43:26.698588Z","iopub.status.idle":"2025-01-29T15:43:26.716792Z","shell.execute_reply.started":"2025-01-29T15:43:26.698557Z","shell.execute_reply":"2025-01-29T15:43:26.715565Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"As a baseline, we train the above `Net`. Let's train for 5 epochs. This might lead to imperfect performance, but we also have to take the fairness of comparison into account. We also train one for 25 epochs.","metadata":{"_uuid":"4211c3a4-b810-430f-85a4-cf2b41f81ad0","_cell_guid":"53b08cf8-c07b-4307-b3d3-baca4025715e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def train_loop(\n    net: nn.Module,\n    dataloader: DataLoader,\n    criterion: Callable[[Tensor, Tensor], Tensor] = nn.CrossEntropyLoss(),\n    n_epochs: int = 5,\n    lr: float = 1e-3,\n) -> nn.Module:\n    \"\"\"Implements a basic, general training loop for supervised learning.\n\n    Args:\n        net: Neural network to train.\n        dataloader: DataLoader object used for training.\n        criterion: Criterion (loss function) used for training.\n        n_epochs: Number of epochs to train the model.\n        lr: Learning rate used for Adam gradient updates.\n\n    Returns:\n        The trained neural network.\n\n    \"\"\"\n    optimizer = optim.Adam(net.parameters(), lr=lr)\n\n    net = net.to(device)\n    net.train()\n    for epoch in range(n_epochs):  # Loop over the dataset multiple times.\n        running_loss = 0.0\n        for batch in tqdm(dataloader):\n            # Zero the parameter gradients.\n            optimizer.zero_grad()\n\n            # Forward + backward + optimize.\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print(f\"Finished epoch {epoch}. Loss: {running_loss / len(dataloader):.3f}\")\n\n    net.eval()\n    print(\"Finished Training\")\n    return net\n\n\nnet_baseline_5 = train_loop(Net(), trainloader_10, n_epochs=5)\nnet_baseline_25 = train_loop(Net(), trainloader_10, n_epochs=25)","metadata":{"_uuid":"1fce590b-6d09-491f-b71b-82773b80abb5","_cell_guid":"c64bd790-7417-4693-99cc-204caba07155","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:43:33.719011Z","iopub.execute_input":"2025-01-29T15:43:33.719399Z","iopub.status.idle":"2025-01-29T15:50:51.542316Z","shell.execute_reply.started":"2025-01-29T15:43:33.719364Z","shell.execute_reply":"2025-01-29T15:50:51.540706Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 50.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.615\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.331\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.211\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 1.066\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.637\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.333\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.214\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.120\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 1.055\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 5. Loss: 1.001\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 6. Loss: 0.956\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 7. Loss: 0.920\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 8. Loss: 0.889\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 9. Loss: 0.852\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 10. Loss: 0.827\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 11. Loss: 0.798\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 12. Loss: 0.773\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 13. Loss: 0.754\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 14. Loss: 0.731\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 15. Loss: 0.708\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 16. Loss: 0.685\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 17. Loss: 0.673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 49.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 18. Loss: 0.655\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 19. Loss: 0.637\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 20. Loss: 0.624\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 21. Loss: 0.608\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 22. Loss: 0.591\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 23. Loss: 0.577\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.74it/s]","output_type":"stream"},{"name":"stdout","text":"Finished epoch 24. Loss: 0.562\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Let's see how good it performs on OOD data:","metadata":{"_uuid":"2539de50-5ac9-49fa-b85c-6cd4624116e3","_cell_guid":"34437a1c-f93c-4694-93b2-0a51aab8821e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def evaluate(\n    net: nn.Module, in_distr_dataloader: DataLoader, out_distr_dataloader: DataLoader\n) -> tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Evaluates ``net``'s accuracy and AUROC scores wrt. both maximum probability\n    and entropy based on how well the OOD-ness of the data can be determined from\n    these scores.\n\n    Args:\n        net: Neural network to evaluate.\n        in_distr_dataloader: DataLoader object containing the in-distribution test dataset.\n        out_distr_dataloader: DataLoader object containing the out-of-distribution\n            test dataset.\n\n    Returns:\n        The accuracy of ``net`` on the in-distribution test dataset, and the AUROC scores\n        wrt. both maximum probability and entropy.\n\n    \"\"\"\n\n    def pred_dataset(net, dataloader):\n        # Iterates over a dataloader and delivers predictions of net and the GT labels.\n        preds = []\n        gts = []\n\n        with torch.no_grad():\n            for batch in tqdm(dataloader):\n                images, labels = batch\n                images, labels = images.to(device), labels.to(device)\n                pred = net(images)\n                preds.append(pred)\n                gts.append(labels)\n\n        preds = torch.cat(preds, dim=0)\n        # Transform logits to probabilities.\n        preds = F.softmax(preds, dim=1)\n        gts = torch.cat(gts, dim=0)\n\n        return preds, gts\n\n    def calc_entropy(x, dim=1):\n        return -torch.sum(x * torch.log(x), dim=dim)\n\n    # Predict the datasets.\n    pred_in, gt_in = pred_dataset(net, in_distr_dataloader)\n    pred_out, gt_out = pred_dataset(net, out_distr_dataloader)\n\n    # Calculate the accuracy.\n    accuracy = Accuracy(task=\"multiclass\", num_classes=10, top_k=1).to(device)\n    acc = accuracy(pred_in, gt_in)\n\n    # Calculate AUROC for distinguishing ID from OOD data\n    # ... based on the maximum probability.\n    max_prob = torch.cat(\n        [torch.max(pred_in, dim=1)[0], torch.max(pred_out, dim=1)[0]], dim=0\n    )\n    id_or_ood = torch.cat(\n        [\n            torch.ones(pred_in.shape[0], device=pred_in.device),\n            torch.zeros(pred_out.shape[0], device=pred_out.device),\n        ],\n        dim=0,\n    )\n    id_or_ood = id_or_ood.int()\n    auroc = AUROC(task=\"binary\")\n    auroc_max_prob = auroc(max_prob, id_or_ood)\n\n    # ... based on prediction entropy.\n    entropy = -torch.cat((calc_entropy(pred_in), calc_entropy(pred_out)), dim=0)\n    auroc_entropy = auroc(entropy, id_or_ood)\n\n    print(\n        f\"\\nAccuracy: {acc:.3f}, AUROC max_prob: {auroc_max_prob:.3f}, \"\n        f\"AUROC entropy: {auroc_entropy:.3f}\"\n    )\n\n    return acc, auroc_max_prob, auroc_entropy\n\n\nacc_baseline, auroc_max_prob_baseline, auroc_entropy_baseline = evaluate(\n    net_baseline_5, testloader_10, testloader_100\n)\nacc_baseline, auroc_max_prob_baseline, auroc_entropy_baseline = evaluate(\n    net_baseline_25, testloader_10, testloader_100\n)","metadata":{"_uuid":"6f537f01-1964-4bf3-a2b3-6a7721c3212c","_cell_guid":"9ae4edc6-8a0b-40ec-a837-819503c8e442","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:51:11.079350Z","iopub.execute_input":"2025-01-29T15:51:11.079797Z","iopub.status.idle":"2025-01-29T15:51:21.454283Z","shell.execute_reply.started":"2025-01-29T15:51:11.079755Z","shell.execute_reply":"2025-01-29T15:51:21.452898Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 62.57it/s]\n100%|██████████| 157/157 [00:02<00:00, 64.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.600, AUROC max_prob: 0.641, AUROC entropy: 0.658\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 65.31it/s]\n100%|██████████| 157/157 [00:02<00:00, 54.35it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.637, AUROC max_prob: 0.647, AUROC entropy: 0.657\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Now we have some baseline results. Please copy/paste them down into the results section below, in case your notebook crashes. Let's implement and train some ensembles that hopefully do better!","metadata":{"_uuid":"53e3cd4c-8112-4e5c-b704-3f567937a634","_cell_guid":"5ecd0f0d-0897-444b-9f52-8ae4c30c8c37","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 2.3 Bagging Ensemble (4 points)\n\nFirst, **implement a simple bagging ensemble** comprising 5 ensemble members. For this, you need bootstrapped dataloaders. Train your ensemble members on them. Finally, wrap them into an ensemble module that calculates the average of their predicted probability distributions and then, for compatibility with the above code, return logits of these. **(Make sure you follow this instruction correctly!)**\n\nFeel free to use the above code to your own needs. As always, you're not allowed to import any libraries other than for basic arithmetics. You're allowed to use `torch.utils.data.Subset`.\n\n*I suggest not copy-pasting code. If you need the exact same code twice, you should make it a function that you can call multiple times. If you need a modified version of the code, first think whether making it a function with different options would make it too complicated. If so, make a separate function while consulting the original code snippet.*","metadata":{"_uuid":"a3a1a0b8-f3a8-4129-b306-fc3ac4c0919d","_cell_guid":"711c52f2-c371-4141-a03f-137375fa069e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"N_ENSEMBLE_MEMBERS = 5\n\n\ndef prepare_dataloaders(\n    dataset: Dataset, n_subsets: int, batch_size: int = 64\n) -> list[DataLoader]:\n    \"\"\"Creates a list of ``DataLoader``s by bootstrapping ``dataset``.\n\n    Args:\n        dataset: The entire training dataset that is bootstrapped.\n        n_subsets: The number of bootstrapped subsets of ``dataset`` that are prepared.\n        batch_size: The size of minibatches the ``DataLoader``s return.\n\n    Returns:\n        A list of bootstrapped ``DataLoader``s of length ``n_subsets``.\n\n    \"\"\"\n    dataloaders = []  # This list should contain 5 dataloaders eventually.\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    dataset_size = len(dataset)\n    num_samples = dataset_size # In bootstrapping, number of samples equals dataset size\n    for _ in range(n_subsets):\n        # Bootstrap the sample indices\n        bootstrapped_inds = torch.randint(0, dataset_size, (num_samples,)) \n        # Dataset with bootstrapped indices\n        bootstrapped_dataset = torch.utils.data.Subset(dataset, bootstrapped_inds)\n        # Dataloader that uses the bootstrapped dataset\n        dataloaders.append(DataLoader(\n            bootstrapped_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n        ))\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return dataloaders\n\n\ndataloaders = prepare_dataloaders(traindata_10, N_ENSEMBLE_MEMBERS)\nmembers = [Net() for i in range(N_ENSEMBLE_MEMBERS)]","metadata":{"_uuid":"daae0940-f3ed-4c9b-80ee-ec756fa92a60","_cell_guid":"bd33d398-46ec-4239-a69d-12bc16de72ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:51:51.473036Z","iopub.execute_input":"2025-01-29T15:51:51.474452Z","iopub.status.idle":"2025-01-29T15:51:51.493960Z","shell.execute_reply.started":"2025-01-29T15:51:51.474396Z","shell.execute_reply":"2025-01-29T15:51:51.492810Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Train the ensemble.\nstart = time.time()\nfor member, dataloader in zip(members, dataloaders):\n    train_loop(member, dataloader)\nend = time.time()\nprint(f\"Total training time (s): {end - start}\")","metadata":{"_uuid":"efab3f5e-50ff-4a50-bb35-a9a47069d49e","_cell_guid":"08a36757-6d50-4335-905e-ed3c1e8157db","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:51:55.256068Z","iopub.execute_input":"2025-01-29T15:51:55.256504Z","iopub.status.idle":"2025-01-29T15:57:57.072891Z","shell.execute_reply.started":"2025-01-29T15:51:55.256466Z","shell.execute_reply":"2025-01-29T15:57:57.071322Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.607\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.156\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.058\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.976\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.608\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.273\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.111\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 0.999\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.910\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.605\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.288\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.153\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.055\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.966\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.577\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.278\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 50.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.134\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.028\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.937\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.617\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.262\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.111\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 0.999\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.911\nFinished Training\nTotal training time (s): 361.8100960254669\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Let's write a wrapper so that we can use the ensemble like any other network. Make sure to read the instructions in the ``forward()`` method carefully.","metadata":{"_uuid":"3b0e6308-e35d-4a4f-b208-b5276cee4900","_cell_guid":"c2df7390-d690-45d6-8efc-3e35d5b31d09","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class AverageEnsemble(nn.Module):\n    def __init__(self, members: list[nn.Module]) -> None:\n        super().__init__()\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        # A list of nn.Modules that have a .forward() function.\n        self.members = nn.ModuleList(members)\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Calculates logits for each member and each image, transforms them into\n        probabilities, averages those probabilities across members, then transforms them\n        back into logits for each image.\n\n        Args:\n            x: Float tensor of shape (batch_size, *input_dims) containing the input batch\n                of data for which each member of the ensemble predicts logits that are\n                further processed as described above.\n\n        Returns:\n            A float tensor of shape (batch_size, n_classes) containing the logits\n            corresponding to the average predicted probabilities by each member of the\n            ensemble.\n\n        \"\"\"\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        member_probabs = []\n        for member in self.members:\n            logits  = member(x)\n            probabs = nn.functional.softmax(logits, dim=1)\n            member_probabs.append(probabs)\n\n        avg_probabs = torch.mean(torch.stack(member_probabs), dim=0)\n\n        # Note that the softmax function is not strictly invertible since it is many-to-one\n        avg_logit = torch.log(avg_probabs)\n        # However, avg_probabs can be recovered by taking the softmax of avg_logit\n        \n        #### >>>> END OF YOUR SOLUTION <<<<\n        return avg_logit","metadata":{"_uuid":"3aff7d06-b40e-48b6-9721-42a439de37f8","_cell_guid":"54e343ba-3ce5-49aa-80a0-e0f7d1a8ed84","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T16:24:02.483442Z","iopub.execute_input":"2025-01-29T16:24:02.483899Z","iopub.status.idle":"2025-01-29T16:24:02.493209Z","shell.execute_reply.started":"2025-01-29T16:24:02.483862Z","shell.execute_reply":"2025-01-29T16:24:02.491660Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Evaluate the ensemble. We'll do the comparison further down below.\n# Copy-paste your results to the results section below.\nacc_bagging, auroc_max_prob_bagging, auroc_entropy_bagging = evaluate(\n    AverageEnsemble(members), testloader_10, testloader_100\n)","metadata":{"_uuid":"2bc8e2e4-89b8-48b3-967c-79415ddba6af","_cell_guid":"5299cb6f-f364-47dc-96e6-37541853c278","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T16:24:09.220882Z","iopub.execute_input":"2025-01-29T16:24:09.221536Z","iopub.status.idle":"2025-01-29T16:24:20.419248Z","shell.execute_reply.started":"2025-01-29T16:24:09.221417Z","shell.execute_reply":"2025-01-29T16:24:20.417773Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:05<00:00, 27.93it/s]\n100%|██████████| 157/157 [00:05<00:00, 28.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.670, AUROC max_prob: 0.685, AUROC entropy: 0.696\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Bonus question (2 points):** A [Balaji ensemble](https://arxiv.org/abs/1612.01474) is very similar to this. Which line(s) of the code would you need to change in order to get a Balaji ensemble instead (apart from the adversarial training proposed by Balaji et al.)?","metadata":{"_uuid":"5ea4a0b8-2edf-461c-b55b-489835dc2f0f","_cell_guid":"85219e32-98ff-4c7e-9217-f68773dcec33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Dropout (5 points)\n\nNext, **add a variation of our `Net()` that uses Monte Carlo Dropout (p=0.25)** after both convolution and linear layers. Make sure the dropout is used both at train and test time to ensure we actually test on an ensemble.\n\nYou are allowed to copy-paste code from above and use the Dropout Modules of `torch.nn`.","metadata":{"_uuid":"215e5984-ea66-4c40-ba37-a5194a47e2f4","_cell_guid":"74f5398b-1c61-4ab6-8f7f-2b490f19851f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DropoutNet(nn.Module):\n    def __init__(self, p=0.25):\n        super().__init__()\n        # Hint: start from the code of the baseline model, then add the dropout layers\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.pool = nn.MaxPool2d(2, 2)      \n        self.dropout1 = nn.Dropout2d(p=p) # Apply after the first conv block\n        self.dropout2 = nn.Dropout2d(p=p) # Apply after the second conv block\n        self.dropout3 = nn.Dropout(p=p)   # Apply after fc1\n        self.dropout4 = nn.Dropout(p=p)   # Apply after fc2\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Implements the forward propagation in a DropoutNet.\n\n        Args:\n            x: Tensor of shape (batch_size, *input_dims) containing the image batch.\n\n        Returns:\n            A tensor of shape (batch_size, n_classes) containing the logits corresponding\n            to the predicted probability of the randomly masked network for each input\n            image.\n\n        \"\"\"\n        # Hint: start from the code of the baseline model, then add the dropout layers\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        # Luckily, we can re-use code from above\n        # x: (b, 3, 32, 32)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.dropout1(x)\n        # x: (b, 6, 14, 14)\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout2(x)\n        # x: (b, 16, 5, 5)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch.\n        # x: (b, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = self.dropout3(x)\n        # x: (b, 120)\n        x = F.relu(self.fc2(x))\n        x = self.dropout4(x)\n        # x: (b, 84)\n        x = self.fc3(x)\n        # x: (b, 10)\n        return x\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n        return x","metadata":{"_uuid":"7ad9da2c-79bb-435e-9c2c-ec72ac130642","_cell_guid":"48cedbb5-470c-47fc-aa66-dd5214c72ac2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T17:01:06.137461Z","iopub.execute_input":"2025-01-29T17:01:06.137942Z","iopub.status.idle":"2025-01-29T17:01:06.148695Z","shell.execute_reply.started":"2025-01-29T17:01:06.137906Z","shell.execute_reply":"2025-01-29T17:01:06.147383Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"For fairness and to counteract the stochasticity, we'll give this network 5*5=25 epochs.","metadata":{"_uuid":"eb201fbb-87ae-4f20-9345-9b5a54bb5da0","_cell_guid":"d43a66a2-2988-4742-8274-ee2ae83692e0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"net_dropout = train_loop(DropoutNet(), trainloader_10, n_epochs=25)","metadata":{"_uuid":"6c607ff3-aa0f-4d37-bafc-f029df6416d4","_cell_guid":"5eced266-dba2-470e-90a9-1adbcfe2602d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T17:01:09.735007Z","iopub.execute_input":"2025-01-29T17:01:09.735377Z","iopub.status.idle":"2025-01-29T17:07:09.052040Z","shell.execute_reply.started":"2025-01-29T17:01:09.735345Z","shell.execute_reply":"2025-01-29T17:07:09.050660Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 49.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.912\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.715\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.636\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.595\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 1.557\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 5. Loss: 1.531\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 6. Loss: 1.517\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 7. Loss: 1.498\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 8. Loss: 1.482\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 9. Loss: 1.473\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 10. Loss: 1.463\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 11. Loss: 1.457\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 12. Loss: 1.450\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 13. Loss: 1.439\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 14. Loss: 1.435\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 15. Loss: 1.425\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 57.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 16. Loss: 1.417\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 17. Loss: 1.413\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 18. Loss: 1.404\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 19. Loss: 1.404\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 20. Loss: 1.397\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 21. Loss: 1.392\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 22. Loss: 1.382\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 23. Loss: 1.381\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.65it/s]","output_type":"stream"},{"name":"stdout","text":"Finished epoch 24. Loss: 1.380\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Now, **set your model to eval mode but keep the Dropout activated**. Then, build a wrapper that predicts 5 times per input, thus simulating five \"ensemble members\". The wrapper should return the logits of the averaged predicted probabilities of your 5 \"ensemble members\", just as in the `AverageEnsemble`.\n\n*If you're feeling fancy,* you can try to find a trick to re-use the whole `AverageEnsemble` class from above.","metadata":{"_uuid":"4891c9e0-1bcb-46e5-ac10-c34098ba05d4","_cell_guid":"f3a7a2e5-bb41-4a66-875b-246b958893d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def enable_dropout(model: nn.Module) -> None:\n    \"\"\"Enables the dropout layers of ``model`` during test time.\n\n    Args:\n        model: Module for which the dropout layer should be activated.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    def enable_dropout(submodule):\n        if isinstance(submodule, nn.Dropout2d) or isinstance(submodule, nn.Dropout):\n            # print(submodule)\n            submodule.train()\n\n    model.apply(enable_dropout)\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n\nnet_dropout.eval()\nenable_dropout(net_dropout)\n#### >>>> PUT YOUR SOLUTION HERE <<<<\ndropout_ensemble = AverageEnsemble([deepcopy(net_dropout).to(device) for i in range(N_ENSEMBLE_MEMBERS)])\n#### >>>> END OF YOUR SOLUTION <<<<","metadata":{"_uuid":"57b7dcac-ee5a-4109-95bd-90ed3fe1e1c3","_cell_guid":"377732eb-44df-4df3-abc3-53da2693365c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:18:03.929775Z","iopub.execute_input":"2025-01-29T18:18:03.930450Z","iopub.status.idle":"2025-01-29T18:18:03.949272Z","shell.execute_reply.started":"2025-01-29T18:18:03.930385Z","shell.execute_reply":"2025-01-29T18:18:03.948031Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"Last, evaluate and copy-paste your results to the results section below.","metadata":{"_uuid":"be72a213-9e37-4d3b-a8bf-8b759cddd755","_cell_guid":"d0468ebb-23b3-446b-ae8b-6549d9dc520c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"acc_dropout, auroc_max_prob_dropout, auroc_entropy_dropout = evaluate(\n    dropout_ensemble, testloader_10, testloader_100\n)","metadata":{"_uuid":"b8bc2923-a3bb-40fe-a101-f663ac798860","_cell_guid":"c0bb734f-edf4-487f-bac8-a201f497ea9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:18:08.732876Z","iopub.execute_input":"2025-01-29T18:18:08.733789Z","iopub.status.idle":"2025-01-29T18:18:19.005456Z","shell.execute_reply.started":"2025-01-29T18:18:08.733750Z","shell.execute_reply":"2025-01-29T18:18:19.004197Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:05<00:00, 30.54it/s]\n100%|██████████| 157/157 [00:05<00:00, 30.84it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.564, AUROC max_prob: 0.650, AUROC entropy: 0.654\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## 2.5 Fast Geometric Ensembling (6 points)\n\nLast, we want to implement [fast geometric ensembling](https://papers.nips.cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf). For this, start with the already trained `net_baseline` and save it as a snapshot. Then, train it with a learning rate increasing from `1e-4` to `1e-2` for two epochs, and then with a decreasing learning rate from `1e-2` to `1e-4` for two epochs. Save the new model as another snapshot. Repeat this process until you have the baseline model and four additional models.\n\nThe code below implements this training procedure. **Fill it out.** You may use `copy.deepcopy()` and any function in `torch.optim.lr_scheduler`.\n\n**Bonus question (2 points)**: Why do we use SGD here instead of Adam?\n\n*Hint: Think about what the learning rate means in Adam.*","metadata":{"_uuid":"7115dd3f-9499-4515-b82d-9eab533cf840","_cell_guid":"81020a08-a09c-4307-a3cd-ba4e0f3bc726","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"code","source":"def fast_geometric_ensembling(\n    net: nn.Module,\n    dataloader: DataLoader,\n    criterion: Callable[[Tensor, Tensor], Tensor] = nn.CrossEntropyLoss(),\n    n_cycles: int = 4,\n) -> list[nn.Module]:\n    \"\"\"Implements fast geometric ensembling of an already trained model.\n\n    Args:\n        net: Network that is already trained.\n        dataloader: Dataloader of the dataset we wish to further train ``net``.\n        criterion: Criterion (loss function) used for training.\n        n_cycles: Number of times we repeated the learning rate increase and\n            decrease procedure (where each repeat gives an additional model).\n\n    Returns:\n        A list of models of length ``n_cycles + 1`` obtained by repeating the learning rate\n        increase and decrease procedure.\n\n    \"\"\"\n\n    def train_loop_one_epoch(\n        net: nn.Module,\n        dataloader: DataLoader,\n        criterion: Callable[[Tensor, Tensor], Tensor],\n        optimizer: optim.SGD,\n        scheduler: lrs.ChainedScheduler,\n    ) -> None:\n        \"\"\"Performs training of ``net`` for one epoch over ``dataloader``, using\n        ``scheduler`` to update the learning rate throughout.\n\n        Args:\n            net: Network to be trained further.\n            dataloader: Dataloader of the dataset we wish to further train ``net``.\n            criterion: Criterion (loss function) used for training.\n            optimizer: Optimizer object for SGD.\n            scheduler: Scheduler object that linearly increases learning rate from 1e-4\n                to 1e-2 and decreases it from 1e-2 back to 1e-4 over 2+2 epochs.\n\n        \"\"\"\n        net = net.to(device)\n\n        for batch in tqdm(dataloader):\n            # Zero the parameter gradients.\n            optimizer.zero_grad()\n\n            # Forward + backward + optimize.\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n    optimizer = optim.SGD(net.parameters(), lr=1e-3)\n    models = [deepcopy(net).to(device)]\n\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    step_size_up, step_size_down = 2, 2\n    scheduler = lrs.CyclicLR(\n        optimizer, base_lr=1e-4, max_lr=1e-2, step_size_up=step_size_up, \n        step_size_down=step_size_down, mode='triangular'\n    )\n    net.train()\n    cycle_len = step_size_up + step_size_down\n    total_epochs = n_cycles * cycle_len\n    for epoch in range(1, total_epochs+1):\n        train_loop_one_epoch(net, dataloader, criterion, optimizer, scheduler)\n        if epoch % cycle_len == 0:\n            models.append(deepcopy(net).to(device))\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    print(\"Finished Training\")\n    return models\n\n\nnets_fge = fast_geometric_ensembling(net_baseline_5, trainloader_10)","metadata":{"_uuid":"6c375814-f59e-41f2-b79b-335d2b2d350c","_cell_guid":"e1fe92fc-e761-490c-8a83-0ddc1779adbd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:32:01.435823Z","iopub.execute_input":"2025-01-29T18:32:01.436484Z","iopub.status.idle":"2025-01-29T18:35:48.669254Z","shell.execute_reply.started":"2025-01-29T18:32:01.436438Z","shell.execute_reply":"2025-01-29T18:35:48.667813Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 58.84it/s]\n100%|██████████| 782/782 [00:14<00:00, 54.87it/s]\n100%|██████████| 782/782 [00:13<00:00, 57.21it/s]\n100%|██████████| 782/782 [00:13<00:00, 58.29it/s]\n100%|██████████| 782/782 [00:14<00:00, 54.00it/s]\n100%|██████████| 782/782 [00:13<00:00, 56.96it/s]\n100%|██████████| 782/782 [00:14<00:00, 53.59it/s]\n100%|██████████| 782/782 [00:13<00:00, 56.72it/s]\n100%|██████████| 782/782 [00:15<00:00, 51.64it/s]\n100%|██████████| 782/782 [00:15<00:00, 49.23it/s]\n100%|██████████| 782/782 [00:15<00:00, 51.41it/s]\n100%|██████████| 782/782 [00:13<00:00, 57.06it/s]\n100%|██████████| 782/782 [00:13<00:00, 55.96it/s]\n100%|██████████| 782/782 [00:14<00:00, 54.04it/s]\n100%|██████████| 782/782 [00:13<00:00, 57.78it/s]\n100%|██████████| 782/782 [00:13<00:00, 55.92it/s]","output_type":"stream"},{"name":"stdout","text":"Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"We can now simply re-use our `AverageEnsemble` to wrap up and test our models.","metadata":{"_uuid":"0176b2bc-7241-4d4e-8885-c586c2f16e26","_cell_guid":"b6c9561f-d91a-4810-bb9a-c7916ed1ee87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(len(nets_fge))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:36:24.758138Z","iopub.execute_input":"2025-01-29T18:36:24.758543Z","iopub.status.idle":"2025-01-29T18:36:24.764412Z","shell.execute_reply.started":"2025-01-29T18:36:24.758506Z","shell.execute_reply":"2025-01-29T18:36:24.763320Z"}},"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"acc_fge, auroc_max_prob_fge, auroc_entropy_fge = evaluate(\n    AverageEnsemble(nets_fge), testloader_10, testloader_100\n)","metadata":{"_uuid":"cff1b3e5-4a52-466f-a825-7093244c8a8f","_cell_guid":"10cf5db7-f82c-45c7-8379-b767d3162cb8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:37:07.983039Z","iopub.execute_input":"2025-01-29T18:37:07.983430Z","iopub.status.idle":"2025-01-29T18:37:18.569140Z","shell.execute_reply.started":"2025-01-29T18:37:07.983397Z","shell.execute_reply":"2025-01-29T18:37:18.565643Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:05<00:00, 30.33it/s]\n100%|██████████| 157/157 [00:05<00:00, 29.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.650, AUROC max_prob: 0.666, AUROC entropy: 0.681\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## 2.5 Interpret the results (4 points)\n\nIn this section, we will compare the results of the above methods. Please **copy-paste the results of your models here**, in case I can't rerun your code:\n\nBaseline 5 epochs:\n\nBaseline 25 epochs:\n\nBagging:\n\nDropout:\n\nFast geometric ensembling:","metadata":{"_uuid":"7b21bdd7-621e-4df3-a722-ede0a9f8f65f","_cell_guid":"205d4ef2-dcfd-45b3-b176-55a4efca4366","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Analyze the numerical results. Which model would you select in which situation and why? Next, comment on their computational and time complexity during train and test. Which practical implications do they have? What models are fair to compare?","metadata":{"_uuid":"ca97fb90-0bf0-4d3e-8926-d5a2c098c561","_cell_guid":"e589e0d7-bb8a-400f-b62d-095b1ff27a37","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"# **3. Aleatoric Uncertainty and Multiple Choices (26 points)**\n**Recommended start: 23.01.2025**","metadata":{"_uuid":"2ab4e609-0bf9-4424-aaa9-a9bb3164a01d","_cell_guid":"d6550145-a642-4589-afae-0d133314a789","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 3.1 Can cross-entropy do it? (3 points)\n\nSuppose you have a classification problem and there is a particular datapoint $x$ with two reasonable classes. Indeed, your annotators provided you both with equal probability in your training data. If your model was trained with a cross-entropy loss on this data (and converged to the global minimum), what prediction will it make, and why? Argue mathematically, but you don't need to provide a complete formal proof.","metadata":{"_uuid":"caa389e5-1a8b-4697-a312-fbe26abff303","_cell_guid":"6a7b4670-4dbe-4210-82b0-31568b43ed64","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Comparing distributions (11 points)","metadata":{"_uuid":"07e59f50-5d10-44bf-ba2a-1193c1065e1f","_cell_guid":"566e7999-ea6f-4668-877d-d3ee954c3ea5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 3.2.1 Analytical solution of Expected Likelihood (7 points)\n\nSometimes, we need to compare distributions to distributions in uncertainty quantification. One way you probably know is the KL divergence. There are, however, other approaches to this. A prominent alternative is the Expected Likelihood Kernel (ELK). For two distributions $p$, $q$ over the same carrier $\\mathcal{X}$, it is defined as\n\n$ELK(p, q) = \\int_\\mathcal{X} p(x) q(x) dx$\n\nLet $p(x) = \\mathcal{N}(x \\mid \\mu_1, \\sigma_1^2)$ and $q(x) = \\mathcal{N}(x \\mid \\mu_2, \\sigma_2^2)$. \n\n**Show that**\n\n$ELK(p, q) = \\frac{1}{\\sqrt{2\\pi}} \\frac{\\tilde{\\sigma}}{\\sigma_1 \\sigma_2} \\exp\\left( -\\frac{1}{2} \\left[ \\mu_1^2 \\frac{1}{\\sigma_1^2} + \\mu_2^2 \\frac{1}{\\sigma_2^2} - \\tilde{\\mu}^2 \\frac{1}{\\tilde{\\sigma}^2} \\right] \\right)$\n\nwith $\\tilde{\\mu} := \\frac{\\mu_1 \\frac{1}{\\sigma_1^2} + \\mu_2\\frac{1}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}$ and $\\tilde{\\sigma}^2 := \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} \\right)^{-1}$.\n\n*Hint: Try to add zeros, multiply ones, and drag out terms until what you have in the integral is the density of a $\\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma}^2)$ distribution.*","metadata":{"_uuid":"a94ba44e-8c56-4c98-b5be-96fba42aed56","_cell_guid":"eb9d79ab-c43e-4647-8bd8-377a3ce207d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"### 3.2.2 Monte Carlo Approximation of Expected Likelihood (4 points)\n\nSometimes, we can't find an analytical solution and need to Monte Carlo estimate the ELK. Rethink why the ELK is called ELK and implement a Monte Carlo estimator of the ELK (between two 1D normal distributions) below.\n\nYou're allowed to use `torch.distributions.Normal`.","metadata":{"_uuid":"83f6e5b6-271b-4d5b-8d8a-f4d6519496fd","_cell_guid":"fcef8b9f-a9b7-4a3c-8ffb-769b92a6052c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def mc_elk(\n    mu1: Tensor, sigma1: Tensor, mu2: Tensor, sigma2: Tensor, n_mc_samples: int = 1000\n) -> Tensor:\n    \"\"\"Implements a Monte Carlo estimator for the Expected Likelihood Kernel.\n\n    Args:\n        mu1, sigma1, mu2, sigma2: Float tensors of shape (n,) where the ELK is approximated\n            for normals with the corresponding means and standard deviations, resulting in\n            n ELK approximations.\n\n    Returns:\n        A float tensor of shape (n,), containing the Monte Carlo approximation of the ELK\n        for normals with the corresponding means and standard deviations.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return estimate\n\n\n# Test:\nprint(mc_elk(torch.zeros(1), torch.ones(1), torch.ones(1), torch.ones(1) * 2))\n# Should return approx 0.16 +/- 0.01.","metadata":{"_uuid":"4c006270-798a-46c9-9e96-1af5eb4d99a6","_cell_guid":"d2d91cc3-cb23-4010-bbca-2cb496748feb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implement a unit test: Calculate the analytical solution and the Monte-Carlo approximation for some normal distributions. Confirm that they are approximately the same by plotting them in a 2D scatter plot.","metadata":{"_uuid":"6b7b5f7f-918a-4bd6-a6c8-4e692d399509","_cell_guid":"0ae16f83-1e0d-49df-a72f-e4c147fa257b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def exact_elk(mu1: Tensor, sigma1: Tensor, mu2: Tensor, sigma2: Tensor) -> Tensor:\n    \"\"\"Implements the exact, analytical ELK of two Gaussians as seen in 3.2.1\n\n    Args:\n        mu1, sigma1, mu2, sigma2: Float tensors of shape (n,) where the ELK is calculated\n            for normals with  the corresponding means and standard deviations, resulting in\n            n ELKs.\n\n    Returns:\n        A float tensor of shape (n,), containing the exact ELKs for normals with the\n        corresponding means and standard deviations.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return elk\n\n\ndef plot_comparison(n: int = 100) -> None:\n    \"\"\"Calculates the exact and approximate ELK for n pairs of normal distributions with\n    different parameters. Then plots the exact vs. approximate results in a scatterplot.\n\n    Args:\n        n: Number of pairs of distributions.\n\n    \"\"\"\n    # Generate some random parameters for our distributions\n    mu1s = torch.zeros(n).uniform_()\n    mu2s = torch.zeros(n).uniform_()\n    sigma1s = torch.zeros(n).uniform_() * 5\n    sigma2s = torch.zeros(n).uniform_() * 5\n\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    # Calculate exact and approx ELK for each entry in (mu1s, sigma1s) vs (mu2s, sigma2s)\n    exact = ...\n    approx = ...\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    # Plot\n    plt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"dashed\")\n    plt.scatter(exact, approx)\n    # You will probably run into numerical issues when using the non-logarithmed ELK for\n    # high values\n    plt.xlim(0, 0.3)\n    plt.ylim(0, 0.3)\n    plt.gca().set_aspect(\"equal\")\n\n\nplot_comparison()","metadata":{"_uuid":"876b3c9c-c4a6-416c-9fb2-f4922e61f5a5","_cell_guid":"8bb71b2b-7342-4074-97d6-c038db705158","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Large Language Models and Uncertainty (12 points)","metadata":{"_uuid":"3a8f51ae-fcf1-4540-b538-fd5b60add397","_cell_guid":"2b3522b4-7e1c-4eb9-aecb-3101d6c59ddb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this exercise, we will use the already familiar HuggingFace interface to inject uncertainty into LLMs and evaluate their calibration.","metadata":{"_uuid":"a3709696-ff27-4e59-b049-44893cfc0f86","_cell_guid":"1f36fd77-56ea-4ac1-b1c9-7b32fe1f1349","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 3.3.1 Text Generation (8 points)\n\nLet's first load `GPT-2`, a fairly small autoregressive model we will work with for the first part of the exercise.","metadata":{"_uuid":"0134745b-0541-4f7d-8604-29569f4bf89f","_cell_guid":"43d794fb-382e-4f62-babd-1bd239a985c1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nmodel = model.to(device)","metadata":{"_uuid":"c6ef4f99-ba47-4be2-8cfc-6bc9f1fb010f","_cell_guid":"af75479b-ac84-48b3-adb6-6f4db3c0c2e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The simplest and cheapest way of integrating uncertainty estimation in an LLM and approximating the model posterior is using MC dropout to obtain several models. We use this solely for simplicity: there are much more sophisticated model posteriors (such as [HET-XL](https://arxiv.org/abs/2301.12860) or [SNGP](https://arxiv.org/abs/2006.10108)) out there.\n\nComplete the implementation of the function `predict_with_uncertainty` that returns the epistemic, aleatoric, and predictive uncertainty per each token. Mathematically, for a token $x$ and models $f_1, \\dots, f_M$ sampled from the dropout posterior,\n$$\n\\begin{align*}\n\\text{EU}(x) &= \\text{JSD}_{f_1, \\dots, f_M}(x) = \\mathbb{H}\\left(\\frac{1}{M}\\sum_{i=1}^M f_i(x)\\right) - \\frac{1}{M}\\sum_{i=1}^M \\mathbb{H}\\left(f_i(x)\\right)\\\\\n\\text{AU}(x) &= \\frac{1}{M}\\sum_{i=1}^M \\mathbb{H}\\left(f_i(x)\\right)\\\\\n\\text{PU}(x) &= \\mathbb{H}\\left(\\frac{1}{M}\\sum_{i=1}^M f_i(x)\\right).\n\\end{align*}\n$$\nWith this setup, it trivially holds that $\\text{PU}(x) = \\text{EU}(x) + \\text{AU}(x)$. Take a moment to understand why (1) $\\text{EU}(x)$ measures parameter uncertainty (as model disagreement), (2) $\\text{AU}(x)$ approximates the stochasticity of the generative process, and $\\text{PU}(x)$ contains both previous sources of uncertainty. If you feel convinced and find the metrics plausible, let's proceed.\n\n*Hint: It is often useful to think about extreme scenarios: What is the highest possible $\\text{EU}(x)$? How do the models behave in that case? What happens to $\\text{PU}(x)$ when the aleatoric uncertainty is high but the epistemic uncertainty is low? What happens in the opposite case?*\n\n*Question to the curious reader: Is $\\text{PU}(x)$ connected to the notion of correctness that was introduced in the lecture? Your findings might point you to two separate interpretations of the predictive uncertainty.*","metadata":{"_uuid":"f851331d-a339-49db-9f4f-0fc1eaa9388d","_cell_guid":"a80abb75-c273-4303-ac25-dcd39a2c3805","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Function to get model predictions with uncertainties\n@torch.no_grad()\ndef predict_with_uncertainty(model, text, n_samples=50):\n    model.eval()\n    enable_dropout(model)\n    logits_list = []\n\n    # Get multiple predictions using dropout\n    for _ in range(n_samples):\n        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n        logits = model(**inputs).logits\n        logits_list.append(logits)\n\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    # Implement the above formula for PU(x) and AU(x).\n    # You get EU(x) trivially from these.\n    # Don't aggregate the uncertainties over the tokens.\n    # You should construct normalized probabilities.\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return eu_per_token, au_per_token, pu_per_token","metadata":{"_uuid":"ff773326-114b-443a-88f6-19ffcaba7130","_cell_guid":"1a629446-1e56-4d77-879e-448214ef9b80","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we have a tool to obtain token-wise uncertainties. Below, we provide a function to visualize the epistemic and aleatoric uncertainties. (Predictive uncertainty is left out, as it is just the sum of the previous two following the above definition.)","metadata":{"_uuid":"f2ccc7a9-a76d-4a4e-a7a2-9b85c182d501","_cell_guid":"fd9a1b7a-6c08-45b1-93ac-6126dbab0498","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def visualize_uncertainty(sentence, epistemic, aleatoric):\n    # Tokenize the sentence to obtain individual words\n    epistemic = epistemic.cpu()\n    aleatoric = aleatoric.cpu()\n    tokens = tokenizer.tokenize(sentence)\n\n    # Create a bar plot\n    x = np.arange(len(tokens))\n    width = 0.35\n\n    fig, ax = plt.subplots()\n\n    rects1 = ax.bar(\n        x - width / 2, epistemic, width, label=\"Epistemic Uncertainty\", alpha=0.8\n    )\n    rects2 = ax.bar(\n        x + width / 2, aleatoric, width, label=\"Aleatoric Uncertainty\", alpha=0.8\n    )\n\n    ax.set_xlabel(\"Tokens\")\n    ax.set_ylabel(\"Uncertainty\")\n    ax.set_title(\"Uncertainties per token in the input sentence\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(tokens, rotation=45, ha=\"right\")\n    ax.legend()\n\n    # Function to auto label the bars with their height values\n    def autolabel(rects):\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate(\n                f\"{height:.2f}\",\n                xy=(rect.get_x() + rect.get_width() / 2, height),\n                xytext=(0, 1),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"bottom\",\n            )\n\n    autolabel(rects1)\n    autolabel(rects2)\n\n    fig.tight_layout()\n    plt.show()","metadata":{"_uuid":"929b3d4e-4d2a-48d4-9ecd-ab265e4367f2","_cell_guid":"3cc61236-80e9-46e7-a5c2-ea8b11a14def","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it's your turn. Experiment with the aleatoric and epistemic uncertainties and write a report in which you address the following points. You can be brief.\n- How do these behave on \"strange\" sentences? Based on your intuition of what kind of texts GPT-2 was trained on, what sentences would you consider out-of-distribution?\n- Do you see a tendency of how the token index affects the associated uncertainties?\n- Are the magnitudes of the two uncertainty sources comparable?\n- How stable are the uncertainty values across different runs of the same cell?\n- Think of a simple, factually correct sentence. Try to modify a single word to make a completely nonsensical sentence. Do you see a change in the uncertainties between the two sentences? Are these changes stable w.r.t. stochasticity?","metadata":{"_uuid":"5c039237-24de-421c-9241-caa8eec72636","_cell_guid":"63451ea8-a845-46d1-8bdd-49b2d4adb2d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Test on ambiguous sentences, incorrect sentences, etc.\n#### >>>> PUT YOUR SOLUTION HERE <<<<\nsentence = ...\n#### >>>> END OF YOUR SOLUTION <<<<\nepistemic_uncertainty, aleatoric_uncertainty, _ = predict_with_uncertainty(model, sentence)\n\nvisualize_uncertainty(sentence, epistemic_uncertainty, aleatoric_uncertainty)\nprint(f\"Average Epistemic Uncertainty: {epistemic_uncertainty.mean().item():.4f}\")\nprint(f\"Average Aleatoric Uncertainty: {aleatoric_uncertainty.mean().item():.4f}\")","metadata":{"_uuid":"72eaaf3b-90fb-442d-b7ff-b3996f6d5d71","_cell_guid":"e7811fd1-dc84-4618-9b64-5982dc62b2da","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's spend a bit more time on trying to detect factual errors in the input using uncertainty. First, complete the code below using the function `roc_auroc_score`. Check where it's imported from and try to understand its inputs. Then, run the cell and observe the results. Which type of uncertainty is the most predictive of correctness? Now, run the cell multiple times. Are the results stable over stochasticity? Would you trust a system that tries to detect falsehood using some of these uncertainties?\n\n*Note: While the results might be interesting on their own, consider the fact that the MC dropout is a simple approximate posterior and the chosen model is also far from being state-of-the-art.*","metadata":{"_uuid":"691ac2a4-3299-48ea-aee8-fbad91d7631b","_cell_guid":"3fae0898-7bfa-42e4-826e-90938d374367","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"sentences = [\n    \"Donald Trump is a British songwriter.\",\n    \"Donald Trump is an American politician and businessman.\",\n    \"Albert Einstein is an Indian physicist.\",\n    \"Edward Teller is a Hungarian physicist.\",\n    \"The first World War was in the 14th century.\",\n    \"The Declaration of Independence was signed in 1776.\",\n    \"John von Neumann has been to Los Alamos.\",\n    \"The Sun has an extremely cold temperature.\",\n    \"Germany is a country in America.\",\n    \"Michael Jackson was an extremely popular black singer.\",\n    # Feel free to extend/change this list if you get more interesting results that way.\n]\ncorrectness = np.array([0, 1, 0, 1, 0, 1, 1, 0, 0, 1])\n\neus = []\naus = []\npus = []\nfor sentence in sentences:\n    (\n        epistemic_uncertainty,\n        aleatoric_uncertainty,\n        predictive_uncertainty,\n    ) = predict_with_uncertainty(model, sentence)\n    eu = epistemic_uncertainty.mean().item()\n    au = aleatoric_uncertainty.mean().item()\n    pu = predictive_uncertainty.mean().item()\n    eus.append(eu)\n    aus.append(au)\n    pus.append(pu)\n\neus = np.array(eus)\naus = np.array(aus)\npus = np.array(pus)\n#### >>>> PUT YOUR SOLUTION HERE <<<<\n# Print the AUROC scores of the negative epistemic, aleatoric, and predictive uncertainties\n# against the correctness of prediction.\n#### >>>> END OF YOUR SOLUTION <<<<","metadata":{"_uuid":"d30319f2-f243-42d0-875f-305a719ac963","_cell_guid":"8b2f406d-f38a-42a0-b022-8bb863d0f052","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3.2 Named Entity Recognition (4 points)\n\nLet's turn to Named Entity Recognition. Don't worry, this part doesn't need new ideas. We'll use our uncertainty estimators from above in a different setting: Named Entity Recognition, where we aim to detect different types of named entities in the input (such as people, geographical locations, institutions, or brands).","metadata":{"_uuid":"748c5fc2-5a65-4073-b16b-7a7e2028467f","_cell_guid":"8843f4c7-f969-4dfb-a9e2-f766d14acd3c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\nmodel = model.to(device)","metadata":{"_uuid":"07abd97d-41a9-455a-9899-7381e0f08e69","_cell_guid":"86ceca82-8cff-422d-8cf8-152e9118447f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What uncertainties do we obtain for the individual tokens in a toy example?","metadata":{"_uuid":"eb151e03-2412-4dbe-a3e4-7e3a8106d73c","_cell_guid":"f9f63f1b-732c-4732-b98b-56e0e6536481","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"text = \"HuggingFace is a company based in New York.\"\ntokens = tokenizer.tokenize(text)\n\neu, au, _ = predict_with_uncertainty(model, text)\n\nfor token, epistemic, aleatoric in zip(tokens, eu, au):\n    print(f\"{token}\\t\\tEpistemic: {epistemic:.4f}\\tAleatoric: {aleatoric:.4f}\")","metadata":{"_uuid":"e28ae206-788d-4093-899c-8d1902b26b35","_cell_guid":"05bdfe2f-f260-470c-8fc8-e78e6e4a1f2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we can check how well the model's correctness correlates with our uncertainty estimates on the `conll2003` dataset. First, we need some boilerplate code to align the dataset labels with the tokens. This is needed because the tokenizer for our model is a subword tokenizer whereas the dataset labels correspond to words. This means that our model predicts a named entity class for each subword token, but the labels for the sentences are for individual words.\n\nYou don't need to understand the boilerplate code, but feel free to explore it if you feel confused by the subsequent code cell.","metadata":{"_uuid":"fbd98cbb-3c47-4847-9ba1-f8e0ac576f02","_cell_guid":"a68e5c77-6f53-4782-8bd6-8aed2535c7f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load the dataset\nraw_datasets = load_dataset(\"conll2003\")\n\n\n# Function to align labels with tokens\ndef align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n\n\n# Function to tokenize and align labels\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs\n\n\n# Tokenize and align the dataset\ntokenized_datasets = raw_datasets.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)","metadata":{"_uuid":"5d627880-e1c7-460f-93fe-bd520d252fb9","_cell_guid":"b0d9d747-e62d-40b5-a8d9-766ceb82b733","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below, we evaluate our uncertainty estimators on how well they can predict the correctness of our model's per-token classification. Which uncertainty metric results in the highest AUROC? How stable are these AUROC results with the stochasticity in the model posterior? Would you trust these estimates compared to the GPT-2 ones?","metadata":{"_uuid":"7a5cf7b3-e604-4577-bf8f-77625817312a","_cell_guid":"83241e2b-2d48-4e14-9a87-fb6a7235337c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def evaluate_aurocs(tokenized_datasets, model, tokenizer):\n    # Initial lists to store results\n    correctness = []\n    eus = []\n    aus = []\n    pus = []\n\n    # Evaluate on 16 test examples\n    # These are 16 test samples that might actually appear in natural language\n    for idx in tqdm(range(300, 317)):\n        sample = tokenized_datasets[\"test\"][idx]\n\n        # Get the text sentence from token IDs\n        sentence = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n\n        # Get uncertainties\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        eu, au, pu = ...\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n        # Get model's predictions\n        inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n        logits = model(**inputs).logits\n        predictions = torch.argmax(logits, dim=-1)[0].tolist()\n\n        # Check correctness and store results into\n        # `correctness`, `eus`, `aus`, and `pus`\n        for true_label, pred, eu_val, au_val, pu_val in zip(\n            sample[\"labels\"], predictions, eu, au, pu\n        ):\n            if true_label != -100:  # Exclude special tokens\n                #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n                #### >>>> END OF YOUR SOLUTION <<<<\n\n    # Compute AUROC\n    epistemic_auroc = roc_auc_score(correctness, -np.array(eus))\n    aleatoric_auroc = roc_auc_score(correctness, -np.array(aus))\n    predictive_auroc = roc_auc_score(correctness, -np.array(pus))\n\n    print(\"\\nAccuracy:\", sum(map(int, correctness)) / len(correctness))\n    print(\"Epistemic AUROC:\", epistemic_auroc)\n    print(\"Aleatoric AUROC:\", aleatoric_auroc)\n    print(\"Predictive AUROC:\", predictive_auroc)\n\n    return correctness, eus, aus, pus\n\ncorrectness, eus, aus, pus = evaluate_aurocs(tokenized_datasets, model, tokenizer)","metadata":{"_uuid":"a0b25b58-7ffe-4cb9-a955-9fa5f174a15d","_cell_guid":"dcfc2677-6f8d-41b5-9c5d-ffae532b87b5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's verify our results visually as well. Make a plot containing the epistemic uncertainties separately for correct and incorrect tokens, and see whether you find linear separability. Choose your plot type and limits carefully to convey your message. Interpret your results.","metadata":{"_uuid":"80e2a7de-1da0-4f6f-9c6e-5fd26f43f41f","_cell_guid":"73b22211-3460-4b3d-9923-e2ce6478f3a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n#### >>>> PUT YOUR SOLUTION HERE <<<<\n\n#### >>>> END OF YOUR SOLUTION <<<<\nplt.show()","metadata":{"_uuid":"1da00f67-8650-4e79-8077-2427b3eb73c6","_cell_guid":"9c4181c6-70f9-4041-97b0-2999189851ae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Relationships Between Predictive, Aleatoric, and Epistemic Uncertainty (17 points)**\n**Recommended start: 23.01.2025**\n\nIn the last exercise, we will relate different sources of uncertainty by exact formulas. We will gain an understanding for the relationship of predictive, aleatoric, and epistemic uncertainty through various uncertainty decompositions. Note that \"the\" uncertainty decomposition doesn't exist. There are many sensible formulations; what performs best for your specific use case should be given preference.","metadata":{"_uuid":"dfcf3f47-ff4f-421f-8de0-0e5520d71a78","_cell_guid":"d737ae39-5024-4c3a-a79a-fff3db6a3d27","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.1 A Risk-based Predictive Uncertainty Decomposition (6 points)\n\n#### 4.1.1 Regression (3 points)\n\nConsider a regression problem with label space $\\mathcal{Y} = \\mathbb{R}$ and pointwise generative process $P(Y \\mid X = x) = \\mathcal{N}\\left(Y; \\mu(x), \\sigma^2(x)\\right)$. We choose the squared loss $\\ell(y, a) = (y - a)^2$ where $a \\in \\mathcal{A} = \\mathbb{R}$ is an element of the action space.\n\n(a) Show that the pointwise risk, defined as $R(f, x) = \\mathbb{E}_{P(Y \\mid X = x)}\\left[\\ell(Y, f(x))\\right]$ is equal to $\\sigma^2(x) + (f(x) - \\mu(x))^2$. Does this value capture our desiderata from predictive uncertainty? Argue based on intuitive definitions from the lecture.\n\n**WRITE YOUR ANSWER HERE**\n\n(b) Argue whether $R(f^*, x)$ is a reasonable aleatoric uncertainty metric based on the informal definition from the lecture.\n\n**WRITE YOUR ANSWER HERE**\n\n#### 4.1.2 Classification (3 points)\n\nConsider a classification problem with label space $\\mathcal{Y} = \\{1, \\dots, K\\}$ and pointwise generative process $P(Y \\mid X = x) = \\operatorname{Cat}(\\mu(x))$. We choose the negative log-likelihood (NLL) loss $\\ell(y, a) = -\\log a_y$ where $a \\in \\mathcal{A} = \\Delta^K$ is an element of the action space, i.e., the $K-1$-dimensional probability simplex.\n\n(a) Show that the pointwise risk is equal to $\\mathbb{H}\\left(\\mu(x), f(x)\\right)$. Does this value capture our desiderata from predictive uncertainty?\n\n**WRITE YOUR ANSWER HERE**\n\n(b) Argue whether $R(f^*, x)$ is a reasonable aleatoric uncertainty metric.\n\n**WRITE YOUR ANSWER HERE**","metadata":{"_uuid":"013db93f-e411-4f9f-b083-c192a36d835e","_cell_guid":"f04eac1b-1a3a-4837-abb9-e2a244f9320c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.2 An Information-Theoretical Predictive Uncertainty Decomposition (5 points)\n\nLet the distribution $P(Y \\mid x, \\omega)$ denote model $\\omega$s prediction for input $x$ (e.g., by considering the model's softmax output). Further, let $P(Y \\mid x, \\mathcal{D}) := \\int_\\Omega P(Y \\mid x, \\omega) dP(\\omega \\mid \\mathcal{D})$ be the so-called predictive distribution. Let $\\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) := D_\\text{KL}\\left(P(Y, \\omega \\mid x, \\mathcal{D})\\ \\Vert\\ P(Y \\mid x)P(\\omega \\mid \\mathcal{D})\\right)$ be the mutual information of the variables $Y \\mid x$ and $\\omega \\mid \\mathcal{D}$. *Note: the decomposition we discussed for the NLP models has close ties with this one.*\n\n(a) Try to gain an intuition for the mutual information. What is its minimizer and for what $P(Y, \\omega \\mid x, \\mathcal{D})$ does it happen?\n\n**WRITE YOUR ANSWER HERE**\n\n(b) Show that $$\\mathbb{H}_{P(Y \\mid x, \\mathcal{D})}(Y) = \\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) + \\mathbb{E}_{P(\\omega \\mid \\mathcal{D})}\\left[\\mathbb{H}_{P(Y \\mid x, \\omega)}(Y)\\right].$$\n\n**WRITE YOUR ANSWER HERE**\n\n(c) Based on your findings and the intuition from (a), what different sources of uncertainty do the individual components measure? Like in the previous sections, it suffices to argue based on informal desiderata from the lecture.\n\n**WRITE YOUR ANSWER HERE**","metadata":{"_uuid":"3c940915-98e2-4f2b-a165-025e4f81793c","_cell_guid":"ed138227-78a3-4a14-8edd-d2ae523faa29","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.3 Evidential methods as a straightforward Bayesian decomposition (6 points)\n\nEvidential Deep Learning allows the inference of epistemic uncertainty in a single forward pass (i.e., without Monte-Carlo samples) by fitting a prior distribution over likelihood functions. Conjugacy allows us to formulate a tractable posterior predictive given a likelihood and conjugate prior distribution. We use the negative log-likelihood of this posterior predictive as an objective function for training.","metadata":{}},{"cell_type":"markdown","source":"### 4.3.1 Fundamentals of Evidential Deep Learning (3 points)\n\nIn the following, we will employ EDL in a regression setting. For simplicity, we assume a Gaussian likelihood and a Normal-Inverse-Gamma (NIG) prior. The NLL loss-function is composed as follows.\n\n$$\n\\begin{split}\n\\mathcal{L}_{nll} &= - \\log \\int_{\\sigma^2=0}^{\\infty} \\int_{\\mu=-\\infty}^{\\infty} \\mathcal{N}(y \\vert \\mu, \\sigma^2) \\mathrm{NIG}(\\mu, \\sigma^2 \\vert \\mu_0, \\alpha, \\beta, \\nu) d\\mu d\\sigma^2 \\\\\n&= - \\log \\mathrm{St}_{2\\alpha}\\left(y; \\mu_0, \\frac{\\beta (1 + \\nu)}{\\nu \\alpha} \\right)\n\\end{split}\n$$\n\nPost-training, the relevant quantities can be computed from the predicted parameters.\n\n$$\n\\hat y = \\mu_0, \\quad \\underbrace{\\mathbb{E}[\\sigma^2] = \\frac{\\beta}{\\alpha - 1}}_{aleatoric}, \\quad \\underbrace{\\mathrm{Var}[\\mu] = \\frac{\\beta}{\\nu(\\alpha-1)}}_{epistemic}\n$$\n\n(a) Discuss the role of the evidential parameters $\\{\\mu_0, \\nu, \\alpha, \\beta\\}$ in the EDL regression model with regards to their impact on uncertainty estimates. Provide examples of how these parameters might change under varied data scenarios, such as high noise or OOD samples.\n\n**WRITE YOUR ANSWER HERE**\n\n(b) Explain how EDL differs from alternative uncertainty estimation methods, such as Monte Carlo dropout, deep ensembles, or other Bayesian approximations. Focus on computational complexity and the ability to distinguish between aleatoric and epistemic uncertainty.\n\n**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"### 4.3.2 Limitations of EDL (3 points)\n\nWhile the formulation of EDL is analytically sound, the approach is plagued by a lack of proper scoring rules. A common, partial, remedy is the addition of heuristic regularization. In the following exercise, you will investigate the impact of regularization on a simple EDL toy experiment, there will be no coding required.\n\nFirst, we will define both loss functions.","metadata":{}},{"cell_type":"code","source":"# Define NLL loss functions\ndef NIG_NLL(y, mu, nu, alpha, beta):\n    two_b_lambda = 2 * beta * (1 + nu)\n    nll = 0.5 * torch.log(torch.pi / (nu + 1e-8)) \\\n        - alpha * torch.log(two_b_lambda) \\\n        + (alpha + 0.5) * torch.log(nu * torch.square(y - mu) + two_b_lambda) \\\n        + torch.lgamma(alpha) \\\n        - torch.lgamma(alpha + 0.5)\n\n    return nll.mean()\n\n\ndef reg_NIG_NLL(y, mu, nu, alpha, beta):\n    two_b_lambda = 2 * beta * (1 + nu)\n    nll = 0.5 * torch.log(torch.pi / (nu + 1e-8)) \\\n        - alpha * torch.log(two_b_lambda) \\\n        + (alpha + 0.5) * torch.log(nu * torch.square(y - mu) + two_b_lambda) \\\n        + torch.lgamma(alpha) \\\n        - torch.lgamma(alpha + 0.5)\n\n    reg = torch.square(y - mu).detach() * torch.reciprocal(beta / (alpha - 1.)) + 2 * alpha\n    loss = nll + 1e-2 * reg\n    return loss.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, some simple code to run and visualize the experiments.","metadata":{}},{"cell_type":"code","source":"from unc_utils import edl_train, edl_visualize, UnivariateNonUniformData, DenseInverseGamma, UnivariateDerNet\n\nEPOCHS=120\nOOD_lower = -10.\nOOD_upper = 20.\n\nnet = UnivariateDerNet()\nnet.to(device)\n\n# Generate simple toy problem\ntrain_data = UnivariateNonUniformData(N=2000, X_intervals=[(-2., 10.)], X_distribution=\"gaussian\")\ntest_data = UnivariateNonUniformData(N=100, X_intervals=[(OOD_lower, OOD_upper)])\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n\n# Define optimizer\noptimizer_params = {\n    \"lr\": 1e-03,\n    \"betas\": (0.9, 0.999),\n    \"eps\": 1e-8,\n    \"weight_decay\": 1e-2,\n    \"amsgrad\": False}\n\noptimizer = torch.optim.AdamW(net.parameters(), **optimizer_params)\n\n# Initialize figures\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=optimizer_params[\"lr\"], steps_per_epoch=len(train_loader), epochs=EPOCHS)\n\n# Run training\nlosses = edl_train(net=net,\n    criterion=NIG_NLL,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    train_loader=train_loader,\n    test_data=test_data,\n    epochs=EPOCHS,\n    device=device)\n\n# Evaluate on larger subspace including unseen data\nline_x = np.linspace(OOD_lower, OOD_upper, num=200)\noutputs = net(torch.Tensor(np.expand_dims(line_x, axis=1)).to(device))\nmu0, nu, alpha, beta = (out.detach().cpu().numpy() for out in outputs)\n\n# Compute uncertainties according to Normal-Inverse-Gamma definition\naleatoric = np.sqrt(beta / (alpha - 1.))\nepistemic = np.sqrt(beta / (nu * (alpha - 1.)))\n\nedl_visualize(mu0, aleatoric, epistemic, train_data, test_data, axes[0])\n\n# Reset scheduler and network parameters\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=optimizer_params[\"lr\"], steps_per_epoch=len(train_loader), epochs=EPOCHS)\nnet.reset()\n\n# Run training\nlosses = edl_train(net=net,\n    criterion=reg_NIG_NLL,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    train_loader=train_loader,\n    test_data=test_data,\n    epochs=EPOCHS,\n    device=device)\n\n# Evaluate on larger subspace including unseen data\nline_x = np.linspace(OOD_lower, OOD_upper, num=200)\noutputs = net(torch.Tensor(np.expand_dims(line_x, axis=1)).to(device))\nmu0, nu, alpha, beta = (out.detach().cpu().numpy() for out in outputs)\n\n# Compute uncertainties according to Normal-Inverse-Gamma definition\naleatoric = np.sqrt(beta / (alpha - 1.))\nepistemic = np.sqrt(beta / (nu * (alpha - 1.)))\n\nedl_visualize(mu0, aleatoric, epistemic, train_data, test_data, axes[1])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that you've executed the code,\n\n(a) study the figures and report your observations. What does the regularizer improve exactly?\n\n**WRITE YOUR ANSWER HERE**\n\n(b) The provided regularizer $\\lfloor (y - \\mu_0)^2 \\rfloor \\left( \\frac{\\beta}{\\alpha - 1} \\right)^{-1} + 2\\alpha$ mirrors another well known negative log-likelihood function closely. State which one and how this might help obtain accurate uncertainty forecasts.\n\n**WRITE YOUR ANSWER HERE**\n\n(c) Propose a novel regularizer based on the interpretations of the estimated parameters $\\{\\mu_0, \\nu, \\alpha, \\beta\\}$ seen in the lecture. You may further use $y, \\hat y$ in your formulation. Argue why your proposed regularizer should be a reasonable choice. You can test your proposition using the code above. (Sound argumentation will net full points, even if your regularizer does not improve performance empirically.)\n\n**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"You did it! This was the last exercise of the course. I hope you had a great time! Best of luck on the exam!","metadata":{"_uuid":"432b839d-69fb-447b-bd6a-0132bfd37f57","_cell_guid":"c280149b-4196-41dd-9e8f-bfd6994b1e05","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}