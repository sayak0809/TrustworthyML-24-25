{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10403948,"sourceType":"datasetVersion","datasetId":6190985}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Trustworthy Machine Learning**\n\n### Winter Semester 2024-2025\n\n### Lecturer: Seong Joon Oh\n\n### Tutor: Lennart Bramlage\n\n### **Exercise 3 -- Uncertainty**","metadata":{"_uuid":"b1b49b09-160d-46f5-9554-e8606d83f464","_cell_guid":"7f2f76ff-3a01-409b-91ba-ec6ac36b2634","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---\n\n\n\n**Group number**: >>> PLEASE FILL IN <<<\n\n\n\n**Student names**: >>> PLEASE FILL IN <<<\n\n\n\n**Student emails**: >>> PLEASE FILL IN <<<\n\n\n\n\n\n---","metadata":{"_uuid":"3d95c1ba-7191-4bfc-a5c0-04b1857f69a7","_cell_guid":"bfda75fb-e824-4e92-8bc7-b73d79d22713","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"\n\n\n\n#### **Submission deadline: 29/01/2025 at 23:59.**\n\nThis exercise is a **group exercise**. The same grade will be conferred to each member of the group based on the submission. Please report cases where any team member contributes significantly less than the other members of the same group. the The grade from this exercise will count towards the final grade.\n\n\n\n#### **How to use GPUs**\n\n- Verify your phone number.\n\n- Select your preferred GPU at `Settings > Accelerator`.\n\n- Put parameters and tensors on CUDA via `tensor.to(device)` etc.\n\n- Double check if the parameters and tensors are on CUDA via `tensor.device` etc.\n\n\n\n#### **Submission**\n\nFollow the below three steps.\n\n\n\n(1) Click on `File > Download notebook`;\n\n\n\n(2) Send the `.ipynb` file to `stai.there@gmail.com` before the deadline.","metadata":{"_uuid":"937a5a11-ba7a-4fb4-8729-c0c3cb416d7a","_cell_guid":"780577f5-b7f7-4b28-9eea-125ab84e1a67","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"First, let's install additional libraries we will need in this exercise.","metadata":{"_uuid":"562844a9-0547-4f2d-8b89-d7509f6ee9c5","_cell_guid":"b1ce2a84-f0a8-45ae-a07b-ca7ce0ca38d9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%pip install -q torchmetrics datasets transformers","metadata":{"_uuid":"2d57da96-5709-41e0-a2e2-753187fdb208","_cell_guid":"19ebef7b-2df9-4ca6-83a9-8b540acc45ce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-30T13:52:24.625339Z","iopub.execute_input":"2025-01-30T13:52:24.625760Z","iopub.status.idle":"2025-01-30T13:52:36.216224Z","shell.execute_reply.started":"2025-01-30T13:52:24.625681Z","shell.execute_reply":"2025-01-30T13:52:36.215037Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"PATH = \"../input/miscellaneous\"\nfrom __future__ import annotations\nimport os\nimport random\nimport sys\nimport time\nfrom copy import deepcopy\nfrom typing import Callable, Optional\n\n%config InlineBackend.figure_formats = ['svg']\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lrs\nimport torchvision.transforms as transforms\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score\nfrom torch import Tensor\nfrom torch.distributions import Normal\nfrom torch.nn import Module\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchmetrics import AUROC, Accuracy\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n)\n\nsys.path.insert(0, PATH)\nfrom unc_utils import entropy, get_cifar_loaders\n\ndef apply_random_seed(random_seed: int) -> None:\n    \"\"\"Sets seed to ``random_seed`` in random, numpy and torch.\"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\napply_random_seed(2025)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device}\")","metadata":{"_uuid":"fe7be74e-8525-4ce5-a013-186a27fa7440","_cell_guid":"011de68b-9e94-4ef7-8a03-a7d93d12eaab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-30T13:52:46.579576Z","iopub.execute_input":"2025-01-30T13:52:46.579968Z","iopub.status.idle":"2025-01-30T13:52:54.566018Z","shell.execute_reply.started":"2025-01-30T13:52:46.579933Z","shell.execute_reply":"2025-01-30T13:52:54.564885Z"}},"outputs":[{"name":"stdout","text":"Using cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **1. Calibration and Proper Scoring (32 points)**\n**Recommended start: 09.01.2025**","metadata":{"_uuid":"d8d1724c-ff8e-4dd9-bbdf-e74c918ed9ea","_cell_guid":"d9f4830a-1114-44e6-83ba-411521a8ac33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1.1 Proper scoring rules (6 points)\n\n### 1.1.1 Proper scoring rule (3 points)\nProve that in a binary classification setup, the negative binary cross-entropy loss is a strictly proper scoring rule.\n\nThe binary cross-entropy (BCE) loss is defined as $\\mathcal{L}(q, y) = - (y \\log(q) + (1-y) \\log(1-q))$, where $y \\in \\{0, 1\\}$ is the binary label and $q = \\hat{P}(Y=1)$ our predicted probability.\n\n*Hint: Here, our target is predicting the correct $P(Y = 1)$. Predicting the correct $P(L = 1)$ as defined in the lecture would also be a valid target, but you do not need to do that in this exercise.*","metadata":{"_uuid":"f4d50986-2814-4678-942b-a7d4365e284f","_cell_guid":"604a618c-529a-49eb-8d55-f8644e3946ba","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE** <br>\n$\\mathbb{E}_Y [-\\mathcal{L}(q, Y)] = -\\mathcal{L}(q, 0). P(Y = 0) - \\mathcal{L}(q, 1). P(Y = 1)$ <br>\n$\\mathbb{E}_Y [-\\mathcal{L}(q, Y)] = P(Y = 0). log(1-q) + P(Y = 1). log(q) = (1-P(Y = 1)). log(1-q) + P(Y = 1). log(q)$ <br><br>\nOptimizing $\\mathbb{E}_Y [-\\mathcal{L}(q, Y)]$ w.r.t. $q$ <br>\n$\\frac{\\partial \\mathbb{E}_Y [-\\mathcal{L}(q, Y)]}{\\partial q} = 0$ <br>\n$-(1-P(Y=1)). \\frac{1}{1-q} + P(Y=1). \\frac{1}{q} = 0$ <br>\n$P(Y = 1). (\\frac{1}{q} + \\frac{1}{1-q}) = \\frac{1}{1-q}$ <br>\n$P(Y = 1). \\frac{1-q+q}{q(1-q)} = \\frac{1}{1-q}$ <br>\n$q^* = P(Y=1)$ is the critical point. <br><br>\nNote that $log(q)$ and $log(1-q)$ are strictly concave functions over the domain $q \\in (0, 1)$. Hence, $-\\mathcal{L}(q, Y)$ is also a strictly concave function over $q \\in (0, 1)$ because it is a linear combination of two concave functions with non-negative coefficients $P(Y = 1)$ and $(1-P(Y = 1))$. Thus, $-\\mathcal{L}(q, Y)$ must have a unique critical point that is also the maximizer. <br>\nThus, $q^* = P(Y=1)$ is the optimal prediction that maximizes the expected value of $-\\mathcal{L}(q, Y)$. This proves that the negative binary cross-entropy is a strictly proper scoring rule.","metadata":{"_uuid":"c193ec54-b1d7-4513-8b31-24146cb2d813","_cell_guid":"13d2b9a6-2120-4bdf-80f5-d6a3d50f6513","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 1.1.2 A non-proper scoring rule (3 points)\n\nShow that, unlike BCE, the following loss function is not a strictly proper scoring rule. \n$\\mathcal{L}(q,y) = - y q + (1 - y) (1 - q)$, where $y \\in \\{0, 1\\}$ is again the binary label and $q = \\hat P(Y=1)$ our predicted probability.","metadata":{}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**<br>\n$\\mathbb{E}_Y [\\mathcal{L}(q, Y)] = P(Y = 0).\\mathcal{L}(q, 0) + P(Y = 1).\\mathcal{L}(q, 1)$ <br>\n$\\mathbb{E}_Y [\\mathcal{L}(q, Y)] = P(Y = 0). (1-q) - P(Y = 1). q = (1-P(Y=1)). (1-q) - P(Y=1). q$ <br>\n$\\mathbb{E}_Y [\\mathcal{L}(q, Y)] = (1-P(Y=1)) - q$ <br> where $q \\in [0, 1]$ <br><br>\n\n$\\mathbb{E}_Y [\\mathcal{L}(q, Y)]$ can be maximized by setting $q^* = 0$ irrespective of the correct probability $P(Y = 1)$. <br>\nSince $q^* \\neq P(Y = 1)$, we conclude that the given $\\mathcal{L}(q, Y)$ is not a strictly proper scoring rule.","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Detecting uncalibrated models (26 points)\n\nIn this exercise, we will investigate the predictions of four binary classifiers,\n\n* a calibrated and accurate model,\n* a calibrated but inaccurate model,\n* an uncalibrated but accurate model, and\n* an uncalibrated and inaccurate model\n\nnoted `model1_outputs.csv`, `model2_outputs.csv`, `model3_outputs.csv`, and `model4_outputs.csv`. They contain model predictions $\\hat{P}(Y=1)$ in the first column and the ground-truth $Y$ values in the second. Using evaluation methods of probabilistic forecasts, you will identify which is which.","metadata":{"_uuid":"781154ce-832e-4475-a0f0-967ddc062719","_cell_guid":"604f4b0a-5856-4222-95f8-c11363abebe2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model1 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model1_outputs.csv\"), delimiter=\",\")\n)\nmodel2 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model2_outputs.csv\"), delimiter=\",\")\n)\nmodel3 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model3_outputs.csv\"), delimiter=\",\")\n)\nmodel4 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model4_outputs.csv\"), delimiter=\",\")\n)","metadata":{"_uuid":"671b2f8c-8f68-4f1f-b1f2-668615d1e495","_cell_guid":"d5b22d65-a07d-412d-a39a-cb19764f82ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.1 Implement uncertainty measures (4 + 4 + 2 + 5 points)\n\n**Implement the following evaluation methods by filling out the code below.**\n\n* Expected calibration error (ECE)\n* Reliability diagram\n* Negative log-likelihood score (NLL)\n* Area under the ROC curve (AUROC)\n\nYou may use `matplotlib.pyplot, torch, numpy`, but not `torch.nn` or any other libraries. Feel free to implement any other metric for investigative purposes in later tasks.\n\n*Hint: You may want to write a helper function that you can use for both ECE and the reliability diagram.*","metadata":{"_uuid":"581817c2-129d-48fe-98e2-f23ffb8f3868","_cell_guid":"59444e8f-eca0-43be-9474-8e3e41f3633f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def ece_score(pred_prob: Tensor, y: Tensor, n_bins: int = 10) -> Tensor:\n    \"\"\"Computes the expected calibration error.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n        n_bins: Number of (equally sized) bins.\n\n    Returns:\n        The ECE in a float tensor of shape (1,).\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return ece_score","metadata":{"_uuid":"c5140928-16a3-4b96-8a8c-0bffbe04b922","_cell_guid":"c56cc691-7342-424e-8eb0-9f8298cb3e40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reliability_diagram(pred_prob: Tensor, y: Tensor, n_bins: int = 10) -> None:\n    \"\"\"Visualizes a reliability diagram.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n        n_bins: Number of (equally sized) bins.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    plt.gca().set_aspect(\"equal\")","metadata":{"_uuid":"9c6cc4c1-2004-4ac0-9ba7-852cc06c0978","_cell_guid":"9b8ca68a-4ada-4510-9733-527204a6316d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def nll_score(pred_prob, y):\n    \"\"\"Computes the negative log-likelihood score.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n        n_bins: Number of (equally sized) bins.\n\n    Returns:\n        The NLL in a float tensor of shape (1,).\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return nll_score","metadata":{"_uuid":"34f19dba-df1a-45e4-936b-7ec1b871f1ff","_cell_guid":"8461a2f4-9402-42a4-9bca-1fbcd00e80a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are several ways to calculate the AUROC. We'll use a simplified one for the sake of this exercise. First, calculate false-positive and true-positive rates for all possible thresholds (Which are these?). The AUROC is an integral over this curve. You don't need to implement fancy trapezoidal rules nor triangles to approximate this integral, but may simply use rectangles below the curve.","metadata":{"_uuid":"490703e2-6b21-4b05-baa6-9b19d5a4de83","_cell_guid":"af8987b0-e53c-47e1-89cf-07f9bd5fa3df","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def auroc_score(pred_prob: Tensor, y: Tensor) -> Tensor:\n    \"\"\"Computes the area under the ROC curve score.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing the ground-truth 0 and 1 labels.\n\n    Returns:\n        The AUROC in a float tensor of shape (1,).\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return auroc_score","metadata":{"_uuid":"6907ddb7-ca23-4723-b92c-1756db69c83b","_cell_guid":"422096ae-a0a1-4648-8c87-12c1e0151ddd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.2 Interpret the results (4 points)\nApply your scores implemented above to the model results and **argue which model is which and why.**\nFor uncalibrated models, also answer **whether they are overconfident, underconfident, or a mixture**.\n\nIf you could not implement the models in 1.2.1, you may use existing libraries to gain points here. Feel free to add additional metrics of existing libraries if you find them useful for finding out which model is which (or implement them yourself if you feel adventurous).\n\n*Hint: Be critical of your metrics! Each metric only evaluates a certain property of the model. Some metrics might even be misleading due to other properties of the model that are not easily visible.*","metadata":{"_uuid":"bc7fd7da-c0d1-4344-9ab2-a4c9f8f8639a","_cell_guid":"055ce582-ffd6-4028-810b-0a47d0faef50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(f\"Model1 ECE: {ece_score(model1[:,0], model1[:,1]).item()}\")\nprint(f\"Model1 NLL: {nll_score(model1[:,0], model1[:,1]).item()}\")\nprint(f\"Model1 AUROC: {auroc_score(model1[:,0], model1[:,1]).item()}\")\nreliability_diagram(model1[:, 0], model1[:, 1])\nplt.show()","metadata":{"_uuid":"fb2cfdee-2742-4b47-96d9-42bab9a8f071","_cell_guid":"a5ca62e4-ab94-429e-8de0-ef8735549705","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model2 ECE: {ece_score(model2[:,0], model2[:,1]).item()}\")\nprint(f\"Model2 NLL: {nll_score(model2[:,0], model2[:,1]).item()}\")\nprint(f\"Model2 AUROC: {auroc_score(model2[:,0], model2[:,1]).item()}\")\nreliability_diagram(model2[:, 0], model2[:, 1])\nplt.show()","metadata":{"_uuid":"e2474d03-3df8-4d13-abfb-39af33b1e9ec","_cell_guid":"bd8d5a7b-0ede-494e-bcaa-8439a8858787","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model3 ECE: {ece_score(model3[:,0], model3[:,1]).item()}\")\nprint(f\"Model3 NLL: {nll_score(model3[:,0], model3[:,1]).item()}\")\nprint(f\"Model3 AUROC: {auroc_score(model3[:,0], model3[:,1]).item()}\")\nreliability_diagram(model3[:, 0], model3[:, 1])\nplt.show()","metadata":{"_uuid":"330cb41d-d676-4ef1-8bf5-0028cf69871e","_cell_guid":"e1c2a202-0b1a-4eab-9f66-8a7c20b10082","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model4 ECE: {ece_score(model4[:,0], model4[:,1]).item()}\")\nprint(f\"Model4 NLL: {nll_score(model4[:,0], model4[:,1]).item()}\")\nprint(f\"Model4 AUROC: {auroc_score(model4[:,0], model4[:,1]).item()}\")\nreliability_diagram(model4[:, 0], model4[:, 1])\nplt.show()","metadata":{"_uuid":"c3468355-c3de-47c0-ab7b-bb6d53c263b8","_cell_guid":"dd744f60-1ba3-44ff-8486-d55c689436fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model XY is calibrated and accurate** because **WRITE YOUR ANSWER HERE**.\n\n**Model XY is uncalibrated but accurate** because **WRITE YOUR ANSWER HERE**. **It is overconfident/underconfident/mixture of both** because **WRITE YOUR ANSWER HERE**.\n\n**Model XY is calibrated but inaccurate** because **WRITE YOUR ANSWER HERE**.\n\n**Model XY is uncalibrated and inaccurate** because **WRITE YOUR ANSWER HERE**. **It is overconfident/underconfident/mixture of both** because **WRITE YOUR ANSWER HERE**.\n\n**Bonus question (2 points)**: Why does the reliability diagram of model 3 look so weird?","metadata":{"_uuid":"1e01d30b-11be-43ff-b1de-7594fd63b4c4","_cell_guid":"ae77ae7c-e755-48ed-b78a-2af9f676bbd9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"### 1.2.3 Cheating the ECE (4 points)\n\nThe final model, `model5_outputs.csv`, has a very bad looking ECE. The ground-truth $y$ values were unfortunately randomly shuffled (`model5_truths_shuffled.csv`) so there is no option to improve performance in a scientifically rigorous way. Let's cheat (Yikes!) to make the score look better on paper than the model is.\n\n**Write a function that adapts the predicted probabilities to increase the ECE score of the model** (with `n_bins = 10`). To make sure that you don't cheat while cheating, `model5_truths_shuffled.csv` only contains a randomly permuted variant of the ground-truth values. The current ECE of model5 on the real ground-truth values is 0.25. You get full points if you manage to decrease it below 0.2 by any means.\n\n*Hint: Test your function by applying it to models 1-4 and calculating their ECE.*","metadata":{"_uuid":"f651cf59-447c-4262-b393-10a85d6df21b","_cell_guid":"a833e5f1-d7f1-4e3d-9f40-06a4594063a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model5 = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model5_outputs.csv\"), delimiter=\",\")\n)\nmodel5_shuffled_y = torch.from_numpy(\n    np.loadtxt(os.path.join(PATH, \"model5_truths_shuffled.csv\"), delimiter=\",\")\n)","metadata":{"_uuid":"2178a14e-d5bf-4b35-bba6-82c8acfae683","_cell_guid":"78d6a220-0558-4c8e-854e-d43c6a80b5ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def increase_ece(pred_prob: Tensor, shuffled_y: Tensor):\n    \"\"\"Adapts the predicted probabilities to make them look better under the ECE measure.\n\n    Args:\n        pred_prob: Float tensor of shape (n,) containing predicted probabilities.\n        y: Float tensor of shape (n,) containing randomly permuted 0 and 1 labels.\n\n    Returns:\n        A float tensor of the same shape as pred_prob with adapted probabilities.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return new_pred_prob\n\n\n# A small verification for yourselves on data where you know the GT y:\nold_ece = ece_score(model4[:, 0], model4[:, 1]).item()\nnew_ece = ece_score(\n    increase_ece(model4[:, 0], model4[torch.randperm(model4.shape[0]), 1]), model4[:, 1]\n).item()\nprint(f\"Model 4: Old ECE was {old_ece} and new ECE is {new_ece}\")","metadata":{"_uuid":"b140d702-7a36-4cab-8666-724284369859","_cell_guid":"218aa156-da14-40ff-9111-6220d97b310c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.4 Cheat prevention (3 points)\nA model can abuse the above adversarial techniques to decrease the ECE by returning probabilities that aren't its true beliefs. **Argue why any cheat like this is impossible for NLL (unless the model truly becomes better).**","metadata":{"_uuid":"6bfa2b5f-df76-4761-b8e0-54b96112b926","_cell_guid":"783d802f-1b8a-46f2-a731-a8ced71e4b8f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE** (no proof required)","metadata":{}},{"cell_type":"markdown","source":"**Bonus question (4 points)**: Comparing scoring rules. In the lecture, we have seen a few strictly proper scoring rules for predictive uncertainty, and also some lower bounds thereof. Below, we provide some boilerplate for downloading the CIFAR-10 dataset.\n\nTry out any two proper scoring rules from the lecture as training objectives. Make sure you provide a fair comparison: some scoring rules might require different hyperparameters than others. Do you get different accuracies and calibration? Use the evaluation methods you have implemented above to support your findings.","metadata":{"_uuid":"2b100e5c-14e1-4fe4-a79a-a5bafa43c849","_cell_guid":"1e6cbdd4-0975-4494-955b-3e367017a978","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"LEARNING_RATE = 0.001\nBATCH_SIZE = 64\nEPOCHS = 10","metadata":{"_uuid":"d994c13c-1d4a-42b2-a750-f358c21374d9","_cell_guid":"a63b331f-d791-4c5c-937d-e7876626d289","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_cifar_loaders()","metadata":{"_uuid":"095c90fb-2be4-407a-8f6f-438aa2ba77d9","_cell_guid":"1c06d5f9-8c56-489a-821b-7cda506b9b50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleNet(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5)\n        self.conv2 = nn.Conv2d(in_channels=10, out_channels=30, kernel_size=3)\n        self.conv3 = nn.Conv2d(in_channels=30, out_channels=50, kernel_size=3)\n        self.fc = nn.Linear(in_features=50 * 4 * 4, out_features=10)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = self.fc(x)\n\n        return x","metadata":{"_uuid":"bd565ce5-22ba-4b18-8aad-09d1699685da","_cell_guid":"8f88a845-14cb-4d75-9af0-23c466592291","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_single_loop(\n    model: SimpleNet,\n    device: str,\n    train_loader: DataLoader,\n    optimizer: Adam,\n    loss_fn: Callable,\n) -> None:\n    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n\n\ndef val(\n    model: SimpleNet,\n    device: str,\n    val_loader: DataLoader,\n    epoch: int,\n    loss_fn: Callable,\n) -> None:\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = loss_fn(output, target)\n            pred = output.argmax(dim=-1)\n\n            val_loss += loss_fn(output, target).item()\n            correct += (pred == target).sum().item()\n\n    val_loss /= len(val_loader)\n    val_accuracy = 100 * correct / len(val_loader.dataset)\n\n    print(\n        f\"[Epoch {epoch:2d}]: Average loss: {val_loss:.4f}, \"\n        f\"Accuracy: {correct}/{len(val_loader.dataset)} ({val_accuracy:.0f}%)\"\n    )\n\n\ndef train(\n    model: SimpleNet,\n    device: str,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    optimizer: Adam,\n    loss_fn: Callable,\n    scheduler: Optional[MultiStepLR] = None,\n    num_epochs: int = 10,\n) -> None:\n    model = model.to(device)\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_single_loop(model, device, train_loader, optimizer, loss_fn)\n\n        model.eval()\n        val(model, device, val_loader, epoch, loss_fn)\n\n        if scheduler is not None:\n            scheduler.step()\n\n    print(\"Done!\")","metadata":{"_uuid":"49be55f7-e670-4960-939e-c9467bfffe5a","_cell_guid":"fb5128cc-af3e-48c0-9cb3-094fb23a9ae6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### >>>> PUT YOUR SOLUTION HERE <<<<\n# Implement your losses/scores of choice from the lecture.\n# Train the models, tune the hyperparameters.\n# Evaluate the models using the metrics you implemented above (on the test dataloader).\n#### >>>> END OF YOUR SOLUTION <<<<","metadata":{"_uuid":"42023879-ff64-4db9-9732-4913c0142d36","_cell_guid":"91eb19d3-6871-4d65-81d7-00c10097629f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. Epistemic Uncertainty and Ensembles (25 points)**\n**Recommended start: 16.01.2025**","metadata":{"_uuid":"6c6f633b-970b-4756-ab0f-14845ba3a69b","_cell_guid":"cf211fbc-a64e-4452-92dd-89f706e96045","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 2.1 Epistemic Uncertainty Applications (6 points)\n\nFind two papers that are concerned with two different applications of epistemic uncertainty. You're allowed to cite any paper except those mentioned in the lecture. You don't need to read the whole paper, but can instead practice quick [paper skimming](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf): Read only the title, abstract, introduction, section titles, figures, and the conclusion. Once you found compelling papers, **shortly address the following tasks for each of your two chosen papers:**\n\n0. Provide a link to the paper.\n1. Summarize the motivating problem of the paper.\n2. How is this problem related to epistemic uncertainty?\n3. How relevant do you find this problem in practice?","metadata":{"_uuid":"efe2fe24-ec00-425e-9518-0dc3c538ce3a","_cell_guid":"b6c64f3d-9bef-45ff-8448-9612338b3a00","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**\n\n**Paper 1**\nWang, H., & Ji, Q. (2024). Epistemic Uncertainty Quantification For Pre-Trained Neural Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11052-11061). <br>\n**0. Link**: https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Epistemic_Uncertainty_Quantification_For_Pre-Trained_Neural_Networks_CVPR_2024_paper.pdf <br>\n**1**:\nMost existing techniques for uncertainty quantification use Bayesian Neural Networks or require an ensemble of models obtained by introducing some random changes to the original model. However, these techniques are not feasible for large pretrained models because we may not have access to their training datasets. This paper introduces a novel technique for epsitemic uncertainty quantification by analyzing the gradients of the model parameters and without performing any modifications to the pretrained model. <br>\n**2**:\nThis problem involves finding an efficient technique for uncertainty quantification of pretrained models. Using their proposed technique, the authors also demonstrate an improved performance in applications of epistemic uncertainty such as OOD detection and active learning. <br>\n**3**:\nSince pretrained models are used in many downstream applications, it is crucial to compute epistemic uncertainty estimates for these models. However, existing uncertainty quantification techniques (like the ones discussed in the lecture) may not be suitable for these pretrained models. This paper proposes a model-agnostic gradient-based method for uncertainty quantification in pretrained models, which in turn could improve the uncertainty estimates of downstream tasks. For example, we could quantify the epistemic uncertainty of ViT (Vision Transformer) representations, which could improve the uncertainty quantification of downstream applications like object detection and semantic segmentation. Hence, this problem is highly relevant in practice. <br> <br>\n\n**Paper 2**\nLing, C., Zhao, X., Zhang, X., Cheng, W., Liu, Y., Sun, Y., ... & Chen, H. (2024, June). Uncertainty Quantification for In-Context Learning of Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (pp. 3357-3370). <br>\n**0. Link**:\nhttps://aclanthology.org/2024.naacl-long.184.pdf <br>\n**1**: Existing works that quantify the uncertainty in LLM responses do not clearly address the underlying cause of these uncertainties. This paper considers LLM responses for in-context learning and decomposes the uncertainty into two components: epistemic uncertainty (attributed to the model itself) and aleatoric uncertainty (attributed to the in-context examples). In partcular, they consider a latent concept based on the in-context examples and then compute the epistemic uncertainty as the entropy of generated outputs given the latent concept. Using their computed epistemic uncertainty, they demonstrate an improved perfomance on in-context classification tasks in various LLM benchmarks. <br>\n**2**: The uncertainty of outputs in LLM-based in-context learning could be caused by inadequate model training (epistemic uncertainty) or uncertainty in the in-context examples (aleatoric uncertainty). Unlike earlier works that only compute the overall uncertainty of LLM outputs, this paper develops a technique to explicitly compute the epistemic uncertainty which can be attributed to inadequate data in the model's training corpus. <br>\n**3**: The proposed technique can help identify and mitigate the sources of uncertainty in LLM outputs. If the epistemic uncertainty is high, we could finetune the LLM by adding more relevant data to the training corpus. On the other hand, if the aleatoric uncertainty is high, we could use better in-context examples. Hence, the proposed uncertainty quantification technique is practically useful to achieve an improved performance in LLM-based in-context learning. <br>","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Setup for Experiments (0 points)","metadata":{"_uuid":"6b9a6235-3689-4d86-a623-020f70ad2a55","_cell_guid":"623ce60b-7f03-45d8-a91b-45df44bf2060","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this exercise, we want to train ensemble methods to output epistemic uncertainties in an OOD task. In particular, we want to train on CIFAR-10 as in-distribution and test on CIFAR-100 as out-of-distribution dataset, so let's first load these two datasets:","metadata":{"_uuid":"05c28ce3-bac0-41f3-8c88-103462d208fe","_cell_guid":"2fffd47c-18db-4261-bd3f-557272340287","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n    ]\n)\n\ntraindata_10 = datasets.CIFAR10(\n    root=\"./data_CIFAR10_train\", train=True, download=True, transform=transform\n)\ntrainloader_10 = DataLoader(\n    traindata_10, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n)\n\ntestdata_10 = datasets.CIFAR10(\n    root=\"./data_CIFAR10_test\", train=False, download=True, transform=transform\n)\ntestloader_10 = DataLoader(\n    testdata_10, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n)\n\n# We will not need the CIFAR100 train dataset\n\ntestdata_100 = datasets.CIFAR100(\n    root=\"./data_CIFAR100_test\", train=False, download=True, transform=transform\n)\ntestloader_100 = DataLoader(\n    testdata_100, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n)","metadata":{"_uuid":"45df002b-ad47-4612-aac8-2e76feb6ea4c","_cell_guid":"6abb410b-85fb-4f4d-8639-e074e9f9d56e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:42:39.051332Z","iopub.execute_input":"2025-01-29T15:42:39.051747Z","iopub.status.idle":"2025-01-29T15:43:26.666845Z","shell.execute_reply.started":"2025-01-29T15:42:39.051713Z","shell.execute_reply":"2025-01-29T15:43:26.665724Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10_train/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:12<00:00, 13890760.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data_CIFAR10_train/cifar-10-python.tar.gz to ./data_CIFAR10_train\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10_test/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:11<00:00, 14625517.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data_CIFAR10_test/cifar-10-python.tar.gz to ./data_CIFAR10_test\nDownloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data_CIFAR100_test/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:11<00:00, 14832269.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data_CIFAR100_test/cifar-100-python.tar.gz to ./data_CIFAR100_test\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"We will use a simple CNN as architecture.","metadata":{"_uuid":"e563cc1b-2652-49f6-8ccc-0fe8dc14e6be","_cell_guid":"b57c693d-35be-480b-a99b-f833e84efd60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, x):\n        # x: (b, 3, 32, 32)\n        x = self.pool(F.relu(self.conv1(x)))\n        # x: (b, 6, 14, 14)\n        x = self.pool(F.relu(self.conv2(x)))\n        # x: (b, 16, 5, 5)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch.\n        # x: (b, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        # x: (b, 120)\n        x = F.relu(self.fc2(x))\n        # x: (b, 84)\n        x = self.fc3(x)\n        # x: (b, 10)\n        return x","metadata":{"_uuid":"1fbf16ef-abe7-4fe4-9d80-9a9af129f1ae","_cell_guid":"42be5014-da9f-4d3c-8944-5799cf319861","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:43:26.698253Z","iopub.execute_input":"2025-01-29T15:43:26.698588Z","iopub.status.idle":"2025-01-29T15:43:26.716792Z","shell.execute_reply.started":"2025-01-29T15:43:26.698557Z","shell.execute_reply":"2025-01-29T15:43:26.715565Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"As a baseline, we train the above `Net`. Let's train for 5 epochs. This might lead to imperfect performance, but we also have to take the fairness of comparison into account. We also train one for 25 epochs.","metadata":{"_uuid":"4211c3a4-b810-430f-85a4-cf2b41f81ad0","_cell_guid":"53b08cf8-c07b-4307-b3d3-baca4025715e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def train_loop(\n    net: nn.Module,\n    dataloader: DataLoader,\n    criterion: Callable[[Tensor, Tensor], Tensor] = nn.CrossEntropyLoss(),\n    n_epochs: int = 5,\n    lr: float = 1e-3,\n) -> nn.Module:\n    \"\"\"Implements a basic, general training loop for supervised learning.\n\n    Args:\n        net: Neural network to train.\n        dataloader: DataLoader object used for training.\n        criterion: Criterion (loss function) used for training.\n        n_epochs: Number of epochs to train the model.\n        lr: Learning rate used for Adam gradient updates.\n\n    Returns:\n        The trained neural network.\n\n    \"\"\"\n    optimizer = optim.Adam(net.parameters(), lr=lr)\n\n    net = net.to(device)\n    net.train()\n    for epoch in range(n_epochs):  # Loop over the dataset multiple times.\n        running_loss = 0.0\n        for batch in tqdm(dataloader):\n            # Zero the parameter gradients.\n            optimizer.zero_grad()\n\n            # Forward + backward + optimize.\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print(f\"Finished epoch {epoch}. Loss: {running_loss / len(dataloader):.3f}\")\n\n    net.eval()\n    print(\"Finished Training\")\n    return net\n\n\nnet_baseline_5 = train_loop(Net(), trainloader_10, n_epochs=5)\nnet_baseline_25 = train_loop(Net(), trainloader_10, n_epochs=25)","metadata":{"_uuid":"1fce590b-6d09-491f-b71b-82773b80abb5","_cell_guid":"c64bd790-7417-4693-99cc-204caba07155","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:43:33.719011Z","iopub.execute_input":"2025-01-29T15:43:33.719399Z","iopub.status.idle":"2025-01-29T15:50:51.542316Z","shell.execute_reply.started":"2025-01-29T15:43:33.719364Z","shell.execute_reply":"2025-01-29T15:50:51.540706Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 50.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.615\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.331\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.211\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 1.066\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.637\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.333\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.214\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.120\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 1.055\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 5. Loss: 1.001\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 6. Loss: 0.956\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 7. Loss: 0.920\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 8. Loss: 0.889\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 9. Loss: 0.852\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 10. Loss: 0.827\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 11. Loss: 0.798\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 12. Loss: 0.773\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 13. Loss: 0.754\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 14. Loss: 0.731\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 15. Loss: 0.708\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 16. Loss: 0.685\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 17. Loss: 0.673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 49.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 18. Loss: 0.655\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 19. Loss: 0.637\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 20. Loss: 0.624\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 21. Loss: 0.608\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 22. Loss: 0.591\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 23. Loss: 0.577\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.74it/s]","output_type":"stream"},{"name":"stdout","text":"Finished epoch 24. Loss: 0.562\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Let's see how good it performs on OOD data:","metadata":{"_uuid":"2539de50-5ac9-49fa-b85c-6cd4624116e3","_cell_guid":"34437a1c-f93c-4694-93b2-0a51aab8821e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def evaluate(\n    net: nn.Module, in_distr_dataloader: DataLoader, out_distr_dataloader: DataLoader\n) -> tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Evaluates ``net``'s accuracy and AUROC scores wrt. both maximum probability\n    and entropy based on how well the OOD-ness of the data can be determined from\n    these scores.\n\n    Args:\n        net: Neural network to evaluate.\n        in_distr_dataloader: DataLoader object containing the in-distribution test dataset.\n        out_distr_dataloader: DataLoader object containing the out-of-distribution\n            test dataset.\n\n    Returns:\n        The accuracy of ``net`` on the in-distribution test dataset, and the AUROC scores\n        wrt. both maximum probability and entropy.\n\n    \"\"\"\n\n    def pred_dataset(net, dataloader):\n        # Iterates over a dataloader and delivers predictions of net and the GT labels.\n        preds = []\n        gts = []\n\n        with torch.no_grad():\n            for batch in tqdm(dataloader):\n                images, labels = batch\n                images, labels = images.to(device), labels.to(device)\n                pred = net(images)\n                preds.append(pred)\n                gts.append(labels)\n\n        preds = torch.cat(preds, dim=0)\n        # Transform logits to probabilities.\n        preds = F.softmax(preds, dim=1)\n        gts = torch.cat(gts, dim=0)\n\n        return preds, gts\n\n    def calc_entropy(x, dim=1):\n        return -torch.sum(x * torch.log(x), dim=dim)\n\n    # Predict the datasets.\n    pred_in, gt_in = pred_dataset(net, in_distr_dataloader)\n    pred_out, gt_out = pred_dataset(net, out_distr_dataloader)\n\n    # Calculate the accuracy.\n    accuracy = Accuracy(task=\"multiclass\", num_classes=10, top_k=1).to(device)\n    acc = accuracy(pred_in, gt_in)\n\n    # Calculate AUROC for distinguishing ID from OOD data\n    # ... based on the maximum probability.\n    max_prob = torch.cat(\n        [torch.max(pred_in, dim=1)[0], torch.max(pred_out, dim=1)[0]], dim=0\n    )\n    id_or_ood = torch.cat(\n        [\n            torch.ones(pred_in.shape[0], device=pred_in.device),\n            torch.zeros(pred_out.shape[0], device=pred_out.device),\n        ],\n        dim=0,\n    )\n    id_or_ood = id_or_ood.int()\n    auroc = AUROC(task=\"binary\")\n    auroc_max_prob = auroc(max_prob, id_or_ood)\n\n    # ... based on prediction entropy.\n    entropy = -torch.cat((calc_entropy(pred_in), calc_entropy(pred_out)), dim=0)\n    auroc_entropy = auroc(entropy, id_or_ood)\n\n    print(\n        f\"\\nAccuracy: {acc:.3f}, AUROC max_prob: {auroc_max_prob:.3f}, \"\n        f\"AUROC entropy: {auroc_entropy:.3f}\"\n    )\n\n    return acc, auroc_max_prob, auroc_entropy\n\n\nacc_baseline, auroc_max_prob_baseline, auroc_entropy_baseline = evaluate(\n    net_baseline_5, testloader_10, testloader_100\n)\nacc_baseline, auroc_max_prob_baseline, auroc_entropy_baseline = evaluate(\n    net_baseline_25, testloader_10, testloader_100\n)","metadata":{"_uuid":"6f537f01-1964-4bf3-a2b3-6a7721c3212c","_cell_guid":"9ae4edc6-8a0b-40ec-a837-819503c8e442","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:51:11.079350Z","iopub.execute_input":"2025-01-29T15:51:11.079797Z","iopub.status.idle":"2025-01-29T15:51:21.454283Z","shell.execute_reply.started":"2025-01-29T15:51:11.079755Z","shell.execute_reply":"2025-01-29T15:51:21.452898Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 62.57it/s]\n100%|██████████| 157/157 [00:02<00:00, 64.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.600, AUROC max_prob: 0.641, AUROC entropy: 0.658\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 65.31it/s]\n100%|██████████| 157/157 [00:02<00:00, 54.35it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.637, AUROC max_prob: 0.647, AUROC entropy: 0.657\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Now we have some baseline results. Please copy/paste them down into the results section below, in case your notebook crashes. Let's implement and train some ensembles that hopefully do better!","metadata":{"_uuid":"53e3cd4c-8112-4e5c-b704-3f567937a634","_cell_guid":"5ecd0f0d-0897-444b-9f52-8ae4c30c8c37","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 2.3 Bagging Ensemble (4 points)\n\nFirst, **implement a simple bagging ensemble** comprising 5 ensemble members. For this, you need bootstrapped dataloaders. Train your ensemble members on them. Finally, wrap them into an ensemble module that calculates the average of their predicted probability distributions and then, for compatibility with the above code, return logits of these. **(Make sure you follow this instruction correctly!)**\n\nFeel free to use the above code to your own needs. As always, you're not allowed to import any libraries other than for basic arithmetics. You're allowed to use `torch.utils.data.Subset`.\n\n*I suggest not copy-pasting code. If you need the exact same code twice, you should make it a function that you can call multiple times. If you need a modified version of the code, first think whether making it a function with different options would make it too complicated. If so, make a separate function while consulting the original code snippet.*","metadata":{"_uuid":"a3a1a0b8-f3a8-4129-b306-fc3ac4c0919d","_cell_guid":"711c52f2-c371-4141-a03f-137375fa069e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"N_ENSEMBLE_MEMBERS = 5\n\n\ndef prepare_dataloaders(\n    dataset: Dataset, n_subsets: int, batch_size: int = 64\n) -> list[DataLoader]:\n    \"\"\"Creates a list of ``DataLoader``s by bootstrapping ``dataset``.\n\n    Args:\n        dataset: The entire training dataset that is bootstrapped.\n        n_subsets: The number of bootstrapped subsets of ``dataset`` that are prepared.\n        batch_size: The size of minibatches the ``DataLoader``s return.\n\n    Returns:\n        A list of bootstrapped ``DataLoader``s of length ``n_subsets``.\n\n    \"\"\"\n    dataloaders = []  # This list should contain 5 dataloaders eventually.\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    dataset_size = len(dataset)\n    num_samples = dataset_size # In bootstrapping, number of samples equals dataset size\n    for _ in range(n_subsets):\n        # Bootstrap the sample indices\n        bootstrapped_inds = torch.randint(0, dataset_size, (num_samples,)) \n        # Dataset with bootstrapped indices\n        bootstrapped_dataset = torch.utils.data.Subset(dataset, bootstrapped_inds)\n        # Dataloader that uses the bootstrapped dataset\n        dataloaders.append(DataLoader(\n            bootstrapped_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n        ))\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return dataloaders\n\n\ndataloaders = prepare_dataloaders(traindata_10, N_ENSEMBLE_MEMBERS)\nmembers = [Net() for i in range(N_ENSEMBLE_MEMBERS)]","metadata":{"_uuid":"daae0940-f3ed-4c9b-80ee-ec756fa92a60","_cell_guid":"bd33d398-46ec-4239-a69d-12bc16de72ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:51:51.473036Z","iopub.execute_input":"2025-01-29T15:51:51.474452Z","iopub.status.idle":"2025-01-29T15:51:51.493960Z","shell.execute_reply.started":"2025-01-29T15:51:51.474396Z","shell.execute_reply":"2025-01-29T15:51:51.492810Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Train the ensemble.\nstart = time.time()\nfor member, dataloader in zip(members, dataloaders):\n    train_loop(member, dataloader)\nend = time.time()\nprint(f\"Total training time (s): {end - start}\")","metadata":{"_uuid":"efab3f5e-50ff-4a50-bb35-a9a47069d49e","_cell_guid":"08a36757-6d50-4335-905e-ed3c1e8157db","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T15:51:55.256068Z","iopub.execute_input":"2025-01-29T15:51:55.256504Z","iopub.status.idle":"2025-01-29T15:57:57.072891Z","shell.execute_reply.started":"2025-01-29T15:51:55.256466Z","shell.execute_reply":"2025-01-29T15:57:57.071322Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.607\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.156\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.058\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.976\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.608\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.273\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.111\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 0.999\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.910\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.605\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.288\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.153\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.055\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.966\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.577\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.278\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 50.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.134\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.028\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.937\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.617\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.262\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.111\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 0.999\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 0.911\nFinished Training\nTotal training time (s): 361.8100960254669\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Let's write a wrapper so that we can use the ensemble like any other network. Make sure to read the instructions in the ``forward()`` method carefully.","metadata":{"_uuid":"3b0e6308-e35d-4a4f-b208-b5276cee4900","_cell_guid":"c2df7390-d690-45d6-8efc-3e35d5b31d09","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class AverageEnsemble(nn.Module):\n    def __init__(self, members: list[nn.Module]) -> None:\n        super().__init__()\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        # A list of nn.Modules that have a .forward() function.\n        self.members = nn.ModuleList(members)\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Calculates logits for each member and each image, transforms them into\n        probabilities, averages those probabilities across members, then transforms them\n        back into logits for each image.\n\n        Args:\n            x: Float tensor of shape (batch_size, *input_dims) containing the input batch\n                of data for which each member of the ensemble predicts logits that are\n                further processed as described above.\n\n        Returns:\n            A float tensor of shape (batch_size, n_classes) containing the logits\n            corresponding to the average predicted probabilities by each member of the\n            ensemble.\n\n        \"\"\"\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        member_probabs = []\n        for member in self.members:\n            logits  = member(x)\n            probabs = nn.functional.softmax(logits, dim=1)\n            member_probabs.append(probabs)\n\n        avg_probabs = torch.mean(torch.stack(member_probabs), dim=0)\n\n        # Note that the softmax function is not strictly invertible since it is many-to-one\n        avg_logit = torch.log(avg_probabs)\n        # However, avg_probabs can be recovered by taking the softmax of avg_logit\n        \n        #### >>>> END OF YOUR SOLUTION <<<<\n        return avg_logit","metadata":{"_uuid":"3aff7d06-b40e-48b6-9721-42a439de37f8","_cell_guid":"54e343ba-3ce5-49aa-80a0-e0f7d1a8ed84","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T16:24:02.483442Z","iopub.execute_input":"2025-01-29T16:24:02.483899Z","iopub.status.idle":"2025-01-29T16:24:02.493209Z","shell.execute_reply.started":"2025-01-29T16:24:02.483862Z","shell.execute_reply":"2025-01-29T16:24:02.491660Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Evaluate the ensemble. We'll do the comparison further down below.\n# Copy-paste your results to the results section below.\nacc_bagging, auroc_max_prob_bagging, auroc_entropy_bagging = evaluate(\n    AverageEnsemble(members), testloader_10, testloader_100\n)","metadata":{"_uuid":"2bc8e2e4-89b8-48b3-967c-79415ddba6af","_cell_guid":"5299cb6f-f364-47dc-96e6-37541853c278","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T16:24:09.220882Z","iopub.execute_input":"2025-01-29T16:24:09.221536Z","iopub.status.idle":"2025-01-29T16:24:20.419248Z","shell.execute_reply.started":"2025-01-29T16:24:09.221417Z","shell.execute_reply":"2025-01-29T16:24:20.417773Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:05<00:00, 27.93it/s]\n100%|██████████| 157/157 [00:05<00:00, 28.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.670, AUROC max_prob: 0.685, AUROC entropy: 0.696\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Bonus question (2 points):** A [Balaji ensemble](https://arxiv.org/abs/1612.01474) is very similar to this. Which line(s) of the code would you need to change in order to get a Balaji ensemble instead (apart from the adversarial training proposed by Balaji et al.)?","metadata":{"_uuid":"5ea4a0b8-2edf-461c-b55b-489835dc2f0f","_cell_guid":"85219e32-98ff-4c7e-9217-f68773dcec33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE** <br>\nIn our ensemble, we use a different bootstrapped sample to train each model in the ensemble. A Balaji ensemble instead uses a randomly sampled minibatch (see Algorithm 1, line 4 in the paper) to train each model in the ensemble. <br><br>\nTo get a Balaji ensemble, we would change the following lines in our code to use random minibatches instead of bootstrapping:\n\n```\nbootstrapped_inds = torch.randint(0, dataset_size, (num_samples,))\nbootstrapped_dataset = torch.utils.data.Subset(dataset, bootstrapped_inds)\n\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Dropout (5 points)\n\nNext, **add a variation of our `Net()` that uses Monte Carlo Dropout (p=0.25)** after both convolution and linear layers. Make sure the dropout is used both at train and test time to ensure we actually test on an ensemble.\n\nYou are allowed to copy-paste code from above and use the Dropout Modules of `torch.nn`.","metadata":{"_uuid":"215e5984-ea66-4c40-ba37-a5194a47e2f4","_cell_guid":"74f5398b-1c61-4ab6-8f7f-2b490f19851f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DropoutNet(nn.Module):\n    def __init__(self, p=0.25):\n        super().__init__()\n        # Hint: start from the code of the baseline model, then add the dropout layers\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.pool = nn.MaxPool2d(2, 2)      \n        self.dropout1 = nn.Dropout2d(p=p) # Apply after the first conv block\n        self.dropout2 = nn.Dropout2d(p=p) # Apply after the second conv block\n        self.dropout3 = nn.Dropout(p=p)   # Apply after fc1\n        self.dropout4 = nn.Dropout(p=p)   # Apply after fc2\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Implements the forward propagation in a DropoutNet.\n\n        Args:\n            x: Tensor of shape (batch_size, *input_dims) containing the image batch.\n\n        Returns:\n            A tensor of shape (batch_size, n_classes) containing the logits corresponding\n            to the predicted probability of the randomly masked network for each input\n            image.\n\n        \"\"\"\n        # Hint: start from the code of the baseline model, then add the dropout layers\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        # Luckily, we can re-use code from above\n        # x: (b, 3, 32, 32)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.dropout1(x)\n        # x: (b, 6, 14, 14)\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout2(x)\n        # x: (b, 16, 5, 5)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch.\n        # x: (b, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = self.dropout3(x)\n        # x: (b, 120)\n        x = F.relu(self.fc2(x))\n        x = self.dropout4(x)\n        # x: (b, 84)\n        x = self.fc3(x)\n        # x: (b, 10)\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n        return x","metadata":{"_uuid":"7ad9da2c-79bb-435e-9c2c-ec72ac130642","_cell_guid":"48cedbb5-470c-47fc-aa66-dd5214c72ac2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T17:01:06.137461Z","iopub.execute_input":"2025-01-29T17:01:06.137942Z","iopub.status.idle":"2025-01-29T17:01:06.148695Z","shell.execute_reply.started":"2025-01-29T17:01:06.137906Z","shell.execute_reply":"2025-01-29T17:01:06.147383Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"For fairness and to counteract the stochasticity, we'll give this network 5*5=25 epochs.","metadata":{"_uuid":"eb201fbb-87ae-4f20-9345-9b5a54bb5da0","_cell_guid":"d43a66a2-2988-4742-8274-ee2ae83692e0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"net_dropout = train_loop(DropoutNet(), trainloader_10, n_epochs=25)","metadata":{"_uuid":"6c607ff3-aa0f-4d37-bafc-f029df6416d4","_cell_guid":"5eced266-dba2-470e-90a9-1adbcfe2602d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T17:01:09.735007Z","iopub.execute_input":"2025-01-29T17:01:09.735377Z","iopub.status.idle":"2025-01-29T17:07:09.052040Z","shell.execute_reply.started":"2025-01-29T17:01:09.735345Z","shell.execute_reply":"2025-01-29T17:07:09.050660Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 49.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 0. Loss: 1.912\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 1. Loss: 1.715\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 2. Loss: 1.636\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:15<00:00, 51.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 3. Loss: 1.595\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 4. Loss: 1.557\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 5. Loss: 1.531\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 52.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 6. Loss: 1.517\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 7. Loss: 1.498\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 8. Loss: 1.482\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 9. Loss: 1.473\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 10. Loss: 1.463\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 11. Loss: 1.457\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 12. Loss: 1.450\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 13. Loss: 1.439\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 55.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 14. Loss: 1.435\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 53.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 15. Loss: 1.425\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 57.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 16. Loss: 1.417\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 17. Loss: 1.413\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 18. Loss: 1.404\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 19. Loss: 1.404\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 20. Loss: 1.397\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 21. Loss: 1.392\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 22. Loss: 1.382\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 56.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch 23. Loss: 1.381\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 782/782 [00:14<00:00, 54.65it/s]","output_type":"stream"},{"name":"stdout","text":"Finished epoch 24. Loss: 1.380\nFinished Training\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Now, **set your model to eval mode but keep the Dropout activated**. Then, build a wrapper that predicts 5 times per input, thus simulating five \"ensemble members\". The wrapper should return the logits of the averaged predicted probabilities of your 5 \"ensemble members\", just as in the `AverageEnsemble`.\n\n*If you're feeling fancy,* you can try to find a trick to re-use the whole `AverageEnsemble` class from above.","metadata":{"_uuid":"4891c9e0-1bcb-46e5-ac10-c34098ba05d4","_cell_guid":"f3a7a2e5-bb41-4a66-875b-246b958893d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def enable_dropout(model: nn.Module) -> None:\n    \"\"\"Enables the dropout layers of ``model`` during test time.\n\n    Args:\n        model: Module for which the dropout layer should be activated.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    def enable_dropout(submodule):\n        if isinstance(submodule, nn.Dropout2d) or isinstance(submodule, nn.Dropout):\n            # print(submodule)\n            submodule.train()\n\n    model.apply(enable_dropout)\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n\nnet_dropout.eval()\nenable_dropout(net_dropout)\n#### >>>> PUT YOUR SOLUTION HERE <<<<\ndropout_ensemble = AverageEnsemble([deepcopy(net_dropout).to(device) for i in range(N_ENSEMBLE_MEMBERS)])\n#### >>>> END OF YOUR SOLUTION <<<<","metadata":{"_uuid":"57b7dcac-ee5a-4109-95bd-90ed3fe1e1c3","_cell_guid":"377732eb-44df-4df3-abc3-53da2693365c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:18:03.929775Z","iopub.execute_input":"2025-01-29T18:18:03.930450Z","iopub.status.idle":"2025-01-29T18:18:03.949272Z","shell.execute_reply.started":"2025-01-29T18:18:03.930385Z","shell.execute_reply":"2025-01-29T18:18:03.948031Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"Last, evaluate and copy-paste your results to the results section below.","metadata":{"_uuid":"be72a213-9e37-4d3b-a8bf-8b759cddd755","_cell_guid":"d0468ebb-23b3-446b-ae8b-6549d9dc520c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"acc_dropout, auroc_max_prob_dropout, auroc_entropy_dropout = evaluate(\n    dropout_ensemble, testloader_10, testloader_100\n)","metadata":{"_uuid":"b8bc2923-a3bb-40fe-a101-f663ac798860","_cell_guid":"c0bb734f-edf4-487f-bac8-a201f497ea9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:18:08.732876Z","iopub.execute_input":"2025-01-29T18:18:08.733789Z","iopub.status.idle":"2025-01-29T18:18:19.005456Z","shell.execute_reply.started":"2025-01-29T18:18:08.733750Z","shell.execute_reply":"2025-01-29T18:18:19.004197Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:05<00:00, 30.54it/s]\n100%|██████████| 157/157 [00:05<00:00, 30.84it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.564, AUROC max_prob: 0.650, AUROC entropy: 0.654\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## 2.5 Fast Geometric Ensembling (6 points)\n\nLast, we want to implement [fast geometric ensembling](https://papers.nips.cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf). For this, start with the already trained `net_baseline` and save it as a snapshot. Then, train it with a learning rate increasing from `1e-4` to `1e-2` for two epochs, and then with a decreasing learning rate from `1e-2` to `1e-4` for two epochs. Save the new model as another snapshot. Repeat this process until you have the baseline model and four additional models.\n\nThe code below implements this training procedure. **Fill it out.** You may use `copy.deepcopy()` and any function in `torch.optim.lr_scheduler`.\n\n**Bonus question (2 points)**: Why do we use SGD here instead of Adam?\n\n*Hint: Think about what the learning rate means in Adam.*","metadata":{"_uuid":"7115dd3f-9499-4515-b82d-9eab533cf840","_cell_guid":"81020a08-a09c-4307-a3cd-ba4e0f3bc726","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**\nAdam uses a different learning rate per parameter based on the momentum, whereas SGD uses the same learning rate for every parameter. In FGE, we need to vary the learning rate cyclically. However, we may not be able to achieve this cyclic variation using Adam since we can't directly control the learning rate of each parameter. Hence, we use SGD to achieve the desired cyclic variation. ","metadata":{}},{"cell_type":"code","source":"def fast_geometric_ensembling(\n    net: nn.Module,\n    dataloader: DataLoader,\n    criterion: Callable[[Tensor, Tensor], Tensor] = nn.CrossEntropyLoss(),\n    n_cycles: int = 4,\n) -> list[nn.Module]:\n    \"\"\"Implements fast geometric ensembling of an already trained model.\n\n    Args:\n        net: Network that is already trained.\n        dataloader: Dataloader of the dataset we wish to further train ``net``.\n        criterion: Criterion (loss function) used for training.\n        n_cycles: Number of times we repeated the learning rate increase and\n            decrease procedure (where each repeat gives an additional model).\n\n    Returns:\n        A list of models of length ``n_cycles + 1`` obtained by repeating the learning rate\n        increase and decrease procedure.\n\n    \"\"\"\n\n    def train_loop_one_epoch(\n        net: nn.Module,\n        dataloader: DataLoader,\n        criterion: Callable[[Tensor, Tensor], Tensor],\n        optimizer: optim.SGD,\n        scheduler: lrs.ChainedScheduler,\n    ) -> None:\n        \"\"\"Performs training of ``net`` for one epoch over ``dataloader``, using\n        ``scheduler`` to update the learning rate throughout.\n\n        Args:\n            net: Network to be trained further.\n            dataloader: Dataloader of the dataset we wish to further train ``net``.\n            criterion: Criterion (loss function) used for training.\n            optimizer: Optimizer object for SGD.\n            scheduler: Scheduler object that linearly increases learning rate from 1e-4\n                to 1e-2 and decreases it from 1e-2 back to 1e-4 over 2+2 epochs.\n\n        \"\"\"\n        net = net.to(device)\n\n        for batch in tqdm(dataloader):\n            # Zero the parameter gradients.\n            optimizer.zero_grad()\n\n            # Forward + backward + optimize.\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n    optimizer = optim.SGD(net.parameters(), lr=1e-3)\n    models = [deepcopy(net).to(device)]\n\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    step_size_up, step_size_down = 2, 2\n    scheduler = lrs.CyclicLR(\n        optimizer, base_lr=1e-4, max_lr=1e-2, step_size_up=step_size_up, \n        step_size_down=step_size_down, mode='triangular'\n    )\n    net.train()\n    cycle_len = step_size_up + step_size_down\n    total_epochs = n_cycles * cycle_len\n    for epoch in range(1, total_epochs+1):\n        train_loop_one_epoch(net, dataloader, criterion, optimizer, scheduler)\n        if epoch % cycle_len == 0:\n            models.append(deepcopy(net).to(device))\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    print(\"Finished Training\")\n    return models\n\n\nnets_fge = fast_geometric_ensembling(net_baseline_5, trainloader_10)","metadata":{"_uuid":"6c375814-f59e-41f2-b79b-335d2b2d350c","_cell_guid":"e1fe92fc-e761-490c-8a83-0ddc1779adbd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:32:01.435823Z","iopub.execute_input":"2025-01-29T18:32:01.436484Z","iopub.status.idle":"2025-01-29T18:35:48.669254Z","shell.execute_reply.started":"2025-01-29T18:32:01.436438Z","shell.execute_reply":"2025-01-29T18:35:48.667813Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [00:13<00:00, 58.84it/s]\n100%|██████████| 782/782 [00:14<00:00, 54.87it/s]\n100%|██████████| 782/782 [00:13<00:00, 57.21it/s]\n100%|██████████| 782/782 [00:13<00:00, 58.29it/s]\n100%|██████████| 782/782 [00:14<00:00, 54.00it/s]\n100%|██████████| 782/782 [00:13<00:00, 56.96it/s]\n100%|██████████| 782/782 [00:14<00:00, 53.59it/s]\n100%|██████████| 782/782 [00:13<00:00, 56.72it/s]\n100%|██████████| 782/782 [00:15<00:00, 51.64it/s]\n100%|██████████| 782/782 [00:15<00:00, 49.23it/s]\n100%|██████████| 782/782 [00:15<00:00, 51.41it/s]\n100%|██████████| 782/782 [00:13<00:00, 57.06it/s]\n100%|██████████| 782/782 [00:13<00:00, 55.96it/s]\n100%|██████████| 782/782 [00:14<00:00, 54.04it/s]\n100%|██████████| 782/782 [00:13<00:00, 57.78it/s]\n100%|██████████| 782/782 [00:13<00:00, 55.92it/s]","output_type":"stream"},{"name":"stdout","text":"Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"We can now simply re-use our `AverageEnsemble` to wrap up and test our models.","metadata":{"_uuid":"0176b2bc-7241-4d4e-8885-c586c2f16e26","_cell_guid":"b6c9561f-d91a-4810-bb9a-c7916ed1ee87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(len(nets_fge))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:36:24.758138Z","iopub.execute_input":"2025-01-29T18:36:24.758543Z","iopub.status.idle":"2025-01-29T18:36:24.764412Z","shell.execute_reply.started":"2025-01-29T18:36:24.758506Z","shell.execute_reply":"2025-01-29T18:36:24.763320Z"}},"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"acc_fge, auroc_max_prob_fge, auroc_entropy_fge = evaluate(\n    AverageEnsemble(nets_fge), testloader_10, testloader_100\n)","metadata":{"_uuid":"cff1b3e5-4a52-466f-a825-7093244c8a8f","_cell_guid":"10cf5db7-f82c-45c7-8379-b767d3162cb8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-29T18:37:07.983039Z","iopub.execute_input":"2025-01-29T18:37:07.983430Z","iopub.status.idle":"2025-01-29T18:37:18.569140Z","shell.execute_reply.started":"2025-01-29T18:37:07.983397Z","shell.execute_reply":"2025-01-29T18:37:18.565643Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:05<00:00, 30.33it/s]\n100%|██████████| 157/157 [00:05<00:00, 29.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.650, AUROC max_prob: 0.666, AUROC entropy: 0.681\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## 2.5 Interpret the results (4 points)\n\nIn this section, we will compare the results of the above methods. Please **copy-paste the results of your models here**, in case I can't rerun your code:\n\nBaseline 5 epochs:\n\nBaseline 25 epochs:\n\nBagging:\n\nDropout:\n\nFast geometric ensembling:","metadata":{"_uuid":"7b21bdd7-621e-4df3-a722-ede0a9f8f65f","_cell_guid":"205d4ef2-dcfd-45b3-b176-55a4efca4366","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Analyze the numerical results. Which model would you select in which situation and why? Next, comment on their computational and time complexity during train and test. Which practical implications do they have? What models are fair to compare?","metadata":{"_uuid":"ca97fb90-0bf0-4d3e-8926-d5a2c098c561","_cell_guid":"e589e0d7-bb8a-400f-b62d-095b1ff27a37","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"# **3. Aleatoric Uncertainty and Multiple Choices (26 points)**\n**Recommended start: 23.01.2025**","metadata":{"_uuid":"2ab4e609-0bf9-4424-aaa9-a9bb3164a01d","_cell_guid":"d6550145-a642-4589-afae-0d133314a789","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 3.1 Can cross-entropy do it? (3 points)\n\nSuppose you have a classification problem and there is a particular datapoint $x$ with two reasonable classes. Indeed, your annotators provided you both with equal probability in your training data. If your model was trained with a cross-entropy loss on this data (and converged to the global minimum), what prediction will it make, and why? Argue mathematically, but you don't need to provide a complete formal proof.","metadata":{"_uuid":"caa389e5-1a8b-4697-a312-fbe26abff303","_cell_guid":"6a7b4670-4dbe-4210-82b0-31568b43ed64","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Comparing distributions (11 points)","metadata":{"_uuid":"07e59f50-5d10-44bf-ba2a-1193c1065e1f","_cell_guid":"566e7999-ea6f-4668-877d-d3ee954c3ea5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 3.2.1 Analytical solution of Expected Likelihood (7 points)\n\nSometimes, we need to compare distributions to distributions in uncertainty quantification. One way you probably know is the KL divergence. There are, however, other approaches to this. A prominent alternative is the Expected Likelihood Kernel (ELK). For two distributions $p$, $q$ over the same carrier $\\mathcal{X}$, it is defined as\n\n$ELK(p, q) = \\int_\\mathcal{X} p(x) q(x) dx$\n\nLet $p(x) = \\mathcal{N}(x \\mid \\mu_1, \\sigma_1^2)$ and $q(x) = \\mathcal{N}(x \\mid \\mu_2, \\sigma_2^2)$. \n\n**Show that**\n\n$ELK(p, q) = \\frac{1}{\\sqrt{2\\pi}} \\frac{\\tilde{\\sigma}}{\\sigma_1 \\sigma_2} \\exp\\left( -\\frac{1}{2} \\left[ \\mu_1^2 \\frac{1}{\\sigma_1^2} + \\mu_2^2 \\frac{1}{\\sigma_2^2} - \\tilde{\\mu}^2 \\frac{1}{\\tilde{\\sigma}^2} \\right] \\right)$\n\nwith $\\tilde{\\mu} := \\frac{\\mu_1 \\frac{1}{\\sigma_1^2} + \\mu_2\\frac{1}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}$ and $\\tilde{\\sigma}^2 := \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} \\right)^{-1}$.\n\n*Hint: Try to add zeros, multiply ones, and drag out terms until what you have in the integral is the density of a $\\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma}^2)$ distribution.*","metadata":{"_uuid":"a94ba44e-8c56-4c98-b5be-96fba42aed56","_cell_guid":"eb9d79ab-c43e-4647-8bd8-377a3ce207d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"### 3.2.2 Monte Carlo Approximation of Expected Likelihood (4 points)\n\nSometimes, we can't find an analytical solution and need to Monte Carlo estimate the ELK. Rethink why the ELK is called ELK and implement a Monte Carlo estimator of the ELK (between two 1D normal distributions) below.\n\nYou're allowed to use `torch.distributions.Normal`.","metadata":{"_uuid":"83f6e5b6-271b-4d5b-8d8a-f4d6519496fd","_cell_guid":"fcef8b9f-a9b7-4a3c-8ffb-769b92a6052c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def mc_elk(\n    mu1: Tensor, sigma1: Tensor, mu2: Tensor, sigma2: Tensor, n_mc_samples: int = 1000\n) -> Tensor:\n    \"\"\"Implements a Monte Carlo estimator for the Expected Likelihood Kernel.\n\n    Args:\n        mu1, sigma1, mu2, sigma2: Float tensors of shape (n,) where the ELK is approximated\n            for normals with the corresponding means and standard deviations, resulting in\n            n ELK approximations.\n\n    Returns:\n        A float tensor of shape (n,), containing the Monte Carlo approximation of the ELK\n        for normals with the corresponding means and standard deviations.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return estimate\n\n\n# Test:\nprint(mc_elk(torch.zeros(1), torch.ones(1), torch.ones(1), torch.ones(1) * 2))\n# Should return approx 0.16 +/- 0.01.","metadata":{"_uuid":"4c006270-798a-46c9-9e96-1af5eb4d99a6","_cell_guid":"d2d91cc3-cb23-4010-bbca-2cb496748feb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implement a unit test: Calculate the analytical solution and the Monte-Carlo approximation for some normal distributions. Confirm that they are approximately the same by plotting them in a 2D scatter plot.","metadata":{"_uuid":"6b7b5f7f-918a-4bd6-a6c8-4e692d399509","_cell_guid":"0ae16f83-1e0d-49df-a72f-e4c147fa257b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def exact_elk(mu1: Tensor, sigma1: Tensor, mu2: Tensor, sigma2: Tensor) -> Tensor:\n    \"\"\"Implements the exact, analytical ELK of two Gaussians as seen in 3.2.1\n\n    Args:\n        mu1, sigma1, mu2, sigma2: Float tensors of shape (n,) where the ELK is calculated\n            for normals with  the corresponding means and standard deviations, resulting in\n            n ELKs.\n\n    Returns:\n        A float tensor of shape (n,), containing the exact ELKs for normals with the\n        corresponding means and standard deviations.\n\n    \"\"\"\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n    return elk\n\n\ndef plot_comparison(n: int = 100) -> None:\n    \"\"\"Calculates the exact and approximate ELK for n pairs of normal distributions with\n    different parameters. Then plots the exact vs. approximate results in a scatterplot.\n\n    Args:\n        n: Number of pairs of distributions.\n\n    \"\"\"\n    # Generate some random parameters for our distributions\n    mu1s = torch.zeros(n).uniform_()\n    mu2s = torch.zeros(n).uniform_()\n    sigma1s = torch.zeros(n).uniform_() * 5\n    sigma2s = torch.zeros(n).uniform_() * 5\n\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    # Calculate exact and approx ELK for each entry in (mu1s, sigma1s) vs (mu2s, sigma2s)\n    exact = ...\n    approx = ...\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    # Plot\n    plt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"dashed\")\n    plt.scatter(exact, approx)\n    # You will probably run into numerical issues when using the non-logarithmed ELK for\n    # high values\n    plt.xlim(0, 0.3)\n    plt.ylim(0, 0.3)\n    plt.gca().set_aspect(\"equal\")\n\n\nplot_comparison()","metadata":{"_uuid":"876b3c9c-c4a6-416c-9fb2-f4922e61f5a5","_cell_guid":"8bb71b2b-7342-4074-97d6-c038db705158","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Large Language Models and Uncertainty (12 points)","metadata":{"_uuid":"3a8f51ae-fcf1-4540-b538-fd5b60add397","_cell_guid":"2b3522b4-7e1c-4eb9-aecb-3101d6c59ddb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this exercise, we will use the already familiar HuggingFace interface to inject uncertainty into LLMs and evaluate their calibration.","metadata":{"_uuid":"a3709696-ff27-4e59-b049-44893cfc0f86","_cell_guid":"1f36fd77-56ea-4ac1-b1c9-7b32fe1f1349","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 3.3.1 Text Generation (8 points)\n\nLet's first load `GPT-2`, a fairly small autoregressive model we will work with for the first part of the exercise.","metadata":{"_uuid":"0134745b-0541-4f7d-8604-29569f4bf89f","_cell_guid":"43d794fb-382e-4f62-babd-1bd239a985c1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nmodel = model.to(device)","metadata":{"_uuid":"c6ef4f99-ba47-4be2-8cfc-6bc9f1fb010f","_cell_guid":"af75479b-ac84-48b3-adb6-6f4db3c0c2e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The simplest and cheapest way of integrating uncertainty estimation in an LLM and approximating the model posterior is using MC dropout to obtain several models. We use this solely for simplicity: there are much more sophisticated model posteriors (such as [HET-XL](https://arxiv.org/abs/2301.12860) or [SNGP](https://arxiv.org/abs/2006.10108)) out there.\n\nComplete the implementation of the function `predict_with_uncertainty` that returns the epistemic, aleatoric, and predictive uncertainty per each token. Mathematically, for a token $x$ and models $f_1, \\dots, f_M$ sampled from the dropout posterior,\n$$\n\\begin{align*}\n\\text{EU}(x) &= \\text{JSD}_{f_1, \\dots, f_M}(x) = \\mathbb{H}\\left(\\frac{1}{M}\\sum_{i=1}^M f_i(x)\\right) - \\frac{1}{M}\\sum_{i=1}^M \\mathbb{H}\\left(f_i(x)\\right)\\\\\n\\text{AU}(x) &= \\frac{1}{M}\\sum_{i=1}^M \\mathbb{H}\\left(f_i(x)\\right)\\\\\n\\text{PU}(x) &= \\mathbb{H}\\left(\\frac{1}{M}\\sum_{i=1}^M f_i(x)\\right).\n\\end{align*}\n$$\nWith this setup, it trivially holds that $\\text{PU}(x) = \\text{EU}(x) + \\text{AU}(x)$. Take a moment to understand why (1) $\\text{EU}(x)$ measures parameter uncertainty (as model disagreement), (2) $\\text{AU}(x)$ approximates the stochasticity of the generative process, and $\\text{PU}(x)$ contains both previous sources of uncertainty. If you feel convinced and find the metrics plausible, let's proceed.\n\n*Hint: It is often useful to think about extreme scenarios: What is the highest possible $\\text{EU}(x)$? How do the models behave in that case? What happens to $\\text{PU}(x)$ when the aleatoric uncertainty is high but the epistemic uncertainty is low? What happens in the opposite case?*\n\n*Question to the curious reader: Is $\\text{PU}(x)$ connected to the notion of correctness that was introduced in the lecture? Your findings might point you to two separate interpretations of the predictive uncertainty.*","metadata":{"_uuid":"f851331d-a339-49db-9f4f-0fc1eaa9388d","_cell_guid":"a80abb75-c273-4303-ac25-dcd39a2c3805","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Function to get model predictions with uncertainties\n@torch.no_grad()\ndef predict_with_uncertainty(model, text, n_samples=50):\n    model.eval()\n    enable_dropout(model)\n    logits_list = []\n\n    # Get multiple predictions using dropout\n    for _ in range(n_samples):\n        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n        logits = model(**inputs).logits\n        logits_list.append(logits)\n\n    #### >>>> PUT YOUR SOLUTION HERE <<<<\n    # Implement the above formula for PU(x) and AU(x).\n    # You get EU(x) trivially from these.\n    # Don't aggregate the uncertainties over the tokens.\n    # You should construct normalized probabilities.\n\n    #### >>>> END OF YOUR SOLUTION <<<<\n\n    return eu_per_token, au_per_token, pu_per_token","metadata":{"_uuid":"ff773326-114b-443a-88f6-19ffcaba7130","_cell_guid":"1a629446-1e56-4d77-879e-448214ef9b80","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we have a tool to obtain token-wise uncertainties. Below, we provide a function to visualize the epistemic and aleatoric uncertainties. (Predictive uncertainty is left out, as it is just the sum of the previous two following the above definition.)","metadata":{"_uuid":"f2ccc7a9-a76d-4a4e-a7a2-9b85c182d501","_cell_guid":"fd9a1b7a-6c08-45b1-93ac-6126dbab0498","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def visualize_uncertainty(sentence, epistemic, aleatoric):\n    # Tokenize the sentence to obtain individual words\n    epistemic = epistemic.cpu()\n    aleatoric = aleatoric.cpu()\n    tokens = tokenizer.tokenize(sentence)\n\n    # Create a bar plot\n    x = np.arange(len(tokens))\n    width = 0.35\n\n    fig, ax = plt.subplots()\n\n    rects1 = ax.bar(\n        x - width / 2, epistemic, width, label=\"Epistemic Uncertainty\", alpha=0.8\n    )\n    rects2 = ax.bar(\n        x + width / 2, aleatoric, width, label=\"Aleatoric Uncertainty\", alpha=0.8\n    )\n\n    ax.set_xlabel(\"Tokens\")\n    ax.set_ylabel(\"Uncertainty\")\n    ax.set_title(\"Uncertainties per token in the input sentence\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(tokens, rotation=45, ha=\"right\")\n    ax.legend()\n\n    # Function to auto label the bars with their height values\n    def autolabel(rects):\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate(\n                f\"{height:.2f}\",\n                xy=(rect.get_x() + rect.get_width() / 2, height),\n                xytext=(0, 1),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"bottom\",\n            )\n\n    autolabel(rects1)\n    autolabel(rects2)\n\n    fig.tight_layout()\n    plt.show()","metadata":{"_uuid":"929b3d4e-4d2a-48d4-9ecd-ab265e4367f2","_cell_guid":"3cc61236-80e9-46e7-a5c2-ea8b11a14def","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it's your turn. Experiment with the aleatoric and epistemic uncertainties and write a report in which you address the following points. You can be brief.\n- How do these behave on \"strange\" sentences? Based on your intuition of what kind of texts GPT-2 was trained on, what sentences would you consider out-of-distribution?\n- Do you see a tendency of how the token index affects the associated uncertainties?\n- Are the magnitudes of the two uncertainty sources comparable?\n- How stable are the uncertainty values across different runs of the same cell?\n- Think of a simple, factually correct sentence. Try to modify a single word to make a completely nonsensical sentence. Do you see a change in the uncertainties between the two sentences? Are these changes stable w.r.t. stochasticity?","metadata":{"_uuid":"5c039237-24de-421c-9241-caa8eec72636","_cell_guid":"63451ea8-a845-46d1-8bdd-49b2d4adb2d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Test on ambiguous sentences, incorrect sentences, etc.\n#### >>>> PUT YOUR SOLUTION HERE <<<<\nsentence = ...\n#### >>>> END OF YOUR SOLUTION <<<<\nepistemic_uncertainty, aleatoric_uncertainty, _ = predict_with_uncertainty(model, sentence)\n\nvisualize_uncertainty(sentence, epistemic_uncertainty, aleatoric_uncertainty)\nprint(f\"Average Epistemic Uncertainty: {epistemic_uncertainty.mean().item():.4f}\")\nprint(f\"Average Aleatoric Uncertainty: {aleatoric_uncertainty.mean().item():.4f}\")","metadata":{"_uuid":"72eaaf3b-90fb-442d-b7ff-b3996f6d5d71","_cell_guid":"e7811fd1-dc84-4618-9b64-5982dc62b2da","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's spend a bit more time on trying to detect factual errors in the input using uncertainty. First, complete the code below using the function `roc_auroc_score`. Check where it's imported from and try to understand its inputs. Then, run the cell and observe the results. Which type of uncertainty is the most predictive of correctness? Now, run the cell multiple times. Are the results stable over stochasticity? Would you trust a system that tries to detect falsehood using some of these uncertainties?\n\n*Note: While the results might be interesting on their own, consider the fact that the MC dropout is a simple approximate posterior and the chosen model is also far from being state-of-the-art.*","metadata":{"_uuid":"691ac2a4-3299-48ea-aee8-fbad91d7631b","_cell_guid":"3fae0898-7bfa-42e4-826e-90938d374367","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"sentences = [\n    \"Donald Trump is a British songwriter.\",\n    \"Donald Trump is an American politician and businessman.\",\n    \"Albert Einstein is an Indian physicist.\",\n    \"Edward Teller is a Hungarian physicist.\",\n    \"The first World War was in the 14th century.\",\n    \"The Declaration of Independence was signed in 1776.\",\n    \"John von Neumann has been to Los Alamos.\",\n    \"The Sun has an extremely cold temperature.\",\n    \"Germany is a country in America.\",\n    \"Michael Jackson was an extremely popular black singer.\",\n    # Feel free to extend/change this list if you get more interesting results that way.\n]\ncorrectness = np.array([0, 1, 0, 1, 0, 1, 1, 0, 0, 1])\n\neus = []\naus = []\npus = []\nfor sentence in sentences:\n    (\n        epistemic_uncertainty,\n        aleatoric_uncertainty,\n        predictive_uncertainty,\n    ) = predict_with_uncertainty(model, sentence)\n    eu = epistemic_uncertainty.mean().item()\n    au = aleatoric_uncertainty.mean().item()\n    pu = predictive_uncertainty.mean().item()\n    eus.append(eu)\n    aus.append(au)\n    pus.append(pu)\n\neus = np.array(eus)\naus = np.array(aus)\npus = np.array(pus)\n#### >>>> PUT YOUR SOLUTION HERE <<<<\n# Print the AUROC scores of the negative epistemic, aleatoric, and predictive uncertainties\n# against the correctness of prediction.\n#### >>>> END OF YOUR SOLUTION <<<<","metadata":{"_uuid":"d30319f2-f243-42d0-875f-305a719ac963","_cell_guid":"8b2f406d-f38a-42a0-b022-8bb863d0f052","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3.2 Named Entity Recognition (4 points)\n\nLet's turn to Named Entity Recognition. Don't worry, this part doesn't need new ideas. We'll use our uncertainty estimators from above in a different setting: Named Entity Recognition, where we aim to detect different types of named entities in the input (such as people, geographical locations, institutions, or brands).","metadata":{"_uuid":"748c5fc2-5a65-4073-b16b-7a7e2028467f","_cell_guid":"8843f4c7-f969-4dfb-a9e2-f766d14acd3c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\nmodel = model.to(device)","metadata":{"_uuid":"07abd97d-41a9-455a-9899-7381e0f08e69","_cell_guid":"86ceca82-8cff-422d-8cf8-152e9118447f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What uncertainties do we obtain for the individual tokens in a toy example?","metadata":{"_uuid":"eb151e03-2412-4dbe-a3e4-7e3a8106d73c","_cell_guid":"f9f63f1b-732c-4732-b98b-56e0e6536481","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"text = \"HuggingFace is a company based in New York.\"\ntokens = tokenizer.tokenize(text)\n\neu, au, _ = predict_with_uncertainty(model, text)\n\nfor token, epistemic, aleatoric in zip(tokens, eu, au):\n    print(f\"{token}\\t\\tEpistemic: {epistemic:.4f}\\tAleatoric: {aleatoric:.4f}\")","metadata":{"_uuid":"e28ae206-788d-4093-899c-8d1902b26b35","_cell_guid":"05bdfe2f-f260-470c-8fc8-e78e6e4a1f2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we can check how well the model's correctness correlates with our uncertainty estimates on the `conll2003` dataset. First, we need some boilerplate code to align the dataset labels with the tokens. This is needed because the tokenizer for our model is a subword tokenizer whereas the dataset labels correspond to words. This means that our model predicts a named entity class for each subword token, but the labels for the sentences are for individual words.\n\nYou don't need to understand the boilerplate code, but feel free to explore it if you feel confused by the subsequent code cell.","metadata":{"_uuid":"fbd98cbb-3c47-4847-9ba1-f8e0ac576f02","_cell_guid":"a68e5c77-6f53-4782-8bd6-8aed2535c7f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load the dataset\nraw_datasets = load_dataset(\"conll2003\")\n\n\n# Function to align labels with tokens\ndef align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n\n\n# Function to tokenize and align labels\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs\n\n\n# Tokenize and align the dataset\ntokenized_datasets = raw_datasets.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)","metadata":{"_uuid":"5d627880-e1c7-460f-93fe-bd520d252fb9","_cell_guid":"b0d9d747-e62d-40b5-a8d9-766ceb82b733","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below, we evaluate our uncertainty estimators on how well they can predict the correctness of our model's per-token classification. Which uncertainty metric results in the highest AUROC? How stable are these AUROC results with the stochasticity in the model posterior? Would you trust these estimates compared to the GPT-2 ones?","metadata":{"_uuid":"7a5cf7b3-e604-4577-bf8f-77625817312a","_cell_guid":"83241e2b-2d48-4e14-9a87-fb6a7235337c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def evaluate_aurocs(tokenized_datasets, model, tokenizer):\n    # Initial lists to store results\n    correctness = []\n    eus = []\n    aus = []\n    pus = []\n\n    # Evaluate on 16 test examples\n    # These are 16 test samples that might actually appear in natural language\n    for idx in tqdm(range(300, 317)):\n        sample = tokenized_datasets[\"test\"][idx]\n\n        # Get the text sentence from token IDs\n        sentence = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n\n        # Get uncertainties\n        #### >>>> PUT YOUR SOLUTION HERE <<<<\n        eu, au, pu = ...\n        #### >>>> END OF YOUR SOLUTION <<<<\n\n        # Get model's predictions\n        inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n        logits = model(**inputs).logits\n        predictions = torch.argmax(logits, dim=-1)[0].tolist()\n\n        # Check correctness and store results into\n        # `correctness`, `eus`, `aus`, and `pus`\n        for true_label, pred, eu_val, au_val, pu_val in zip(\n            sample[\"labels\"], predictions, eu, au, pu\n        ):\n            if true_label != -100:  # Exclude special tokens\n                #### >>>> PUT YOUR SOLUTION HERE <<<<\n\n                #### >>>> END OF YOUR SOLUTION <<<<\n\n    # Compute AUROC\n    epistemic_auroc = roc_auc_score(correctness, -np.array(eus))\n    aleatoric_auroc = roc_auc_score(correctness, -np.array(aus))\n    predictive_auroc = roc_auc_score(correctness, -np.array(pus))\n\n    print(\"\\nAccuracy:\", sum(map(int, correctness)) / len(correctness))\n    print(\"Epistemic AUROC:\", epistemic_auroc)\n    print(\"Aleatoric AUROC:\", aleatoric_auroc)\n    print(\"Predictive AUROC:\", predictive_auroc)\n\n    return correctness, eus, aus, pus\n\ncorrectness, eus, aus, pus = evaluate_aurocs(tokenized_datasets, model, tokenizer)","metadata":{"_uuid":"a0b25b58-7ffe-4cb9-a955-9fa5f174a15d","_cell_guid":"dcfc2677-6f8d-41b5-9c5d-ffae532b87b5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's verify our results visually as well. Make a plot containing the epistemic uncertainties separately for correct and incorrect tokens, and see whether you find linear separability. Choose your plot type and limits carefully to convey your message. Interpret your results.","metadata":{"_uuid":"80e2a7de-1da0-4f6f-9c6e-5fd26f43f41f","_cell_guid":"73b22211-3460-4b3d-9923-e2ce6478f3a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n#### >>>> PUT YOUR SOLUTION HERE <<<<\n\n#### >>>> END OF YOUR SOLUTION <<<<\nplt.show()","metadata":{"_uuid":"1da00f67-8650-4e79-8077-2427b3eb73c6","_cell_guid":"9c4181c6-70f9-4041-97b0-2999189851ae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Relationships Between Predictive, Aleatoric, and Epistemic Uncertainty (17 points)**\n**Recommended start: 23.01.2025**\n\nIn the last exercise, we will relate different sources of uncertainty by exact formulas. We will gain an understanding for the relationship of predictive, aleatoric, and epistemic uncertainty through various uncertainty decompositions. Note that \"the\" uncertainty decomposition doesn't exist. There are many sensible formulations; what performs best for your specific use case should be given preference.","metadata":{"_uuid":"dfcf3f47-ff4f-421f-8de0-0e5520d71a78","_cell_guid":"d737ae39-5024-4c3a-a79a-fff3db6a3d27","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.1 A Risk-based Predictive Uncertainty Decomposition (6 points)\n\n#### 4.1.1 Regression (3 points)\n\nConsider a regression problem with label space $\\mathcal{Y} = \\mathbb{R}$ and pointwise generative process $P(Y \\mid X = x) = \\mathcal{N}\\left(Y; \\mu(x), \\sigma^2(x)\\right)$. We choose the squared loss $\\ell(y, a) = (y - a)^2$ where $a \\in \\mathcal{A} = \\mathbb{R}$ is an element of the action space.\n\n(a) Show that the pointwise risk, defined as $R(f, x) = \\mathbb{E}_{P(Y \\mid X = x)}\\left[\\ell(Y, f(x))\\right]$ is equal to $\\sigma^2(x) + (f(x) - \\mu(x))^2$. Does this value capture our desiderata from predictive uncertainty? Argue based on intuitive definitions from the lecture.\n\n**WRITE YOUR ANSWER HERE** <br><br>\n$R(f, x) = \\mathbb{E}_{P(Y \\mid X = x)}\\left[\\ell(Y, f(x))\\right] = \\int_{-\\infty}^{+\\infty} p(y | X = x). (y - f(x))^2 dy = \\int_{-\\infty}^{+\\infty} p(y | X = x). (y^2 - 2 y f(x) + (f(x))^2) dy$ <br>\n$= \\int_{-\\infty}^{+\\infty} y^2. p(y | X = x) dy - 2 f(x) \\int_{-\\infty}^{+\\infty} y. p(y | X = x) dy +  (f(x))^2 \\int_{-\\infty}^{+\\infty} p (y | X = x) dy$ <br>\n$= (\\sigma^2(x) + (\\mu(x))^2) - 2 f(x) \\mu(x) + (f(x))^2$ <br>\n$= \\sigma^2(x) + ((\\mu(x))^2 - 2 f(x) \\mu(x) + (f(x))^2)$ <br>\n$= \\sigma^2(x) + (f(x) - \\mu(x))^2$\n\nYes, $R(f, x)$ captures our desiderata of predictive uncertainty. If $f(x)$ is closer to $\\mu(x)$, the term $(f(x) - \\mu(x))^2$ would be lower and the prediction $f(x)$ would likely be closer to the actual value of $y$. A lower $\\sigma^2(x)$ indicates that $y$ is likely to be closer to $\\mu(x)$. Both terms are low if the model prediction is close to $\\mu(x)$ and the generating process has a low variance, which means that the prediction would likely be close to the true value of $y$. Thus, a lower value of $R(f, X)$ corresponds to a lower predictive uncertainty.\n\n\n(b) Argue whether $R(f^*, x)$ is a reasonable aleatoric uncertainty metric based on the informal definition from the lecture. <br>\n\n**WRITE YOUR ANSWER HERE** <br><br>\nFor an optimal $f^*$, we would have $f^*(x) = \\mu(x) \\implies R(f^*, x) = \\sigma^2(x)$. Thus, $R(f^*, x)$ is the variance of the distribution $P(Y | X = x)$, which corresponds to the informal definition of aleatoric uncertainty.\n\n#### 4.1.2 Classification (3 points)\n\nConsider a classification problem with label space $\\mathcal{Y} = \\{1, \\dots, K\\}$ and pointwise generative process $P(Y \\mid X = x) = \\operatorname{Cat}(\\mu(x))$. We choose the negative log-likelihood (NLL) loss $\\ell(y, a) = -\\log a_y$ where $a \\in \\mathcal{A} = \\Delta^K$ is an element of the action space, i.e., the $K-1$-dimensional probability simplex.\n\n(a) Show that the pointwise risk is equal to $\\mathbb{H}\\left(\\mu(x), f(x)\\right)$. Does this value capture our desiderata from predictive uncertainty?\n\n**WRITE YOUR ANSWER HERE** <br><br>\n$R(f, x) = \\mathbb{E}_{P(Y | X = x)} [\\ell(Y, f(X))] = \\sum_{j = 1}^{K} P(Y = j | X = x). \\ell(j, f(x))$ <br>\n$= \\sum_{j = 1}^{K} \\mu_j (x) . \\ell(j, f(x))$ <br>\n$= -\\sum_{j = 1}^{K} \\mu_j (x) . log(f_j(x))$ <br>\n$= \\mathbb{H} (\\mu(x), f(x))$ <br><br>\nIf $f(x) = \\mu(x)$, the cross-entropy $\\mathbb{H} (\\mu(x), f(x))$ is minimized and the model's prediction is likely to be equal to the true value of $Y$. The closer $f(x)$ is to $\\mu(x)$, the more likely the model's prediction is to be correct. Hence, the cross-entropy captures the desired properties of predictive uncertainty.\n\n\n(b) Argue whether $R(f^*, x)$ is a reasonable aleatoric uncertainty metric.\n\n**WRITE YOUR ANSWER HERE** <br><br>\nFor the optimal predictor, we have $f^*(x) = \\mu(x) \\implies R(f^*, x) = \\mathbb{H} (\\mu(x), f^*(x)) = \\mathbb{H} (\\mu(x), \\mu(x))$ which is the entropy of the generative process. The entropy is minimized when $\\mu(x)$ is a one-hot vector, i.e. when there is no uncertainty in the generative process. It is maximized when $\\mu(x)$ is a uniform distribution over the $K$ classes. Hence, $R(f^*, x)$ is a reasonable metric for aleatoric uncertainty because it quantifies the uncertainty in the generative process.","metadata":{"_uuid":"013db93f-e411-4f9f-b083-c192a36d835e","_cell_guid":"f04eac1b-1a3a-4837-abb9-e2a244f9320c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.2 An Information-Theoretical Predictive Uncertainty Decomposition (5 points)\n\nLet the distribution $P(Y \\mid x, \\omega)$ denote model $\\omega$s prediction for input $x$ (e.g., by considering the model's softmax output). Further, let $P(Y \\mid x, \\mathcal{D}) := \\int_\\Omega P(Y \\mid x, \\omega) dP(\\omega \\mid \\mathcal{D})$ be the so-called predictive distribution. Let $\\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) := D_\\text{KL}\\left(P(Y, \\omega \\mid x, \\mathcal{D})\\ \\Vert\\ P(Y \\mid x)P(\\omega \\mid \\mathcal{D})\\right)$ be the mutual information of the variables $Y \\mid x$ and $\\omega \\mid \\mathcal{D}$. *Note: the decomposition we discussed for the NLP models has close ties with this one.*\n\n(a) Try to gain an intuition for the mutual information. What is its minimizer and for what $P(Y, \\omega \\mid x, \\mathcal{D})$ does it happen?\n\n**WRITE YOUR ANSWER HERE** <br><br>\n$P(Y \\mid x)$ is the generative process of the data whereas $P(\\omega \\mid \\mathcal{D})$ is the posterior distribution of model parameters given a dataset. The mutual information is minimized (= 0) when $P(Y, \\omega \\mid x, \\mathcal{D}) = P(Y \\mid x). P(\\omega \\mid \\mathcal{D})$, i.e. when $P(Y \\mid x)$ and $P(\\omega \\mid \\mathcal{D})$ are independent of each other. Thus, the mutual information quantifies the dependence between $P(Y \\mid x)$ (generative process of the data) and $P(\\omega \\mid \\mathcal{D})$ (model training).\n\n(b) Show that $$\\mathbb{H}_{P(Y \\mid x, \\mathcal{D})}(Y) = \\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) + \\mathbb{E}_{P(\\omega \\mid \\mathcal{D})}\\left[\\mathbb{H}_{P(Y \\mid x, \\omega)}(Y)\\right].$$\n\n**WRITE YOUR ANSWER HERE**\nLet $D_Y$ denote the domain of $Y$. <br>\n$\\mathbb{H}_{P(Y \\mid x, \\mathcal{D})}(Y) = -\\int_{D_Y} p(y | x, \\mathcal{D}). log(p(y | x, \\mathcal{D})) dy$ <br>\n$= -\\int_{D_Y} \\int_{\\Omega} p(y, \\omega | x, \\mathcal{D}). p(\\omega | \\mathcal{D}). log(p(y | x, \\mathcal{D})) d\\omega dy$ <br>\n$= -\\int_{D_Y} \\int_{\\Omega} p(y, \\omega | x, \\mathcal{D}). p(\\omega | \\mathcal{D}). (log(\\frac{p(y, \\omega | x, \\mathcal{D})}{p(y | x) p(\\omega | \\mathcal{D})}) + log(p(y | x) p(\\omega | \\mathcal{D}))) d\\omega dy$ <br>\n$= -\\int_{D_Y} \\int_{\\Omega} p(y, \\omega | x, \\mathcal{D}). p(\\omega | \\mathcal{D}). (log(\\frac{p(y, \\omega | x, \\mathcal{D})}{p(y | x) p(\\omega | \\mathcal{D})})) d\\omega dy - \\int_{D_Y} \\int_{\\Omega} p(y, \\omega | x, \\mathcal{D}). p(\\omega | \\mathcal{D}). log(p(y | x) p(\\omega | \\mathcal{D}))) d\\omega dy$ <br>\n$= \\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) - \\int_{\\Omega} p(\\omega | \\mathcal{D}) (\\int_{D_Y} p(y, \\omega | x, \\mathcal{D}) log(p(y | x) p(\\omega | \\mathcal{D})) dy) d\\omega$ <br>\n$= \\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) + \\int_{\\Omega} p(\\omega | \\mathcal{D}). \\mathbb{H}_{P(Y \\mid x, \\omega)}(Y) d\\omega$ <br>\n$= \\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D}) + \\mathbb{E}_{P(\\omega \\mid \\mathcal{D})}\\left[\\mathbb{H}_{P(Y \\mid x, \\omega)}(Y)\\right]$\n\n(c) Based on your findings and the intuition from (a), what different sources of uncertainty do the individual components measure? Like in the previous sections, it suffices to argue based on informal desiderata from the lecture.\n\n**WRITE YOUR ANSWER HERE** <br>\n$\\mathbb{I}(Y; \\omega \\mid x, \\mathcal{D})$ measures the epistemic uncertainty. If the uncertainty in $P(Y \\mid x)$ does not affect the model parameter estimate, the distributions $P(Y \\mid x)$ and $P(\\omega \\mid \\mathcal{D})$ would be independent and the mutual information would be zero. <br>\n\n$\\mathbb{E}_{P(\\omega \\mid \\mathcal{D})}\\left[\\mathbb{H}_{P(Y \\mid x, \\omega)}(Y)\\right]$ measures the aleatoric uncertainty. This is because $\\left[\\mathbb{H}_{P(Y \\mid x, \\omega)}(Y)\\right]$ measures the stochasticity in the generative process and we then take the expectation of this quantity over the posterior distribution of models. For instance, $\\left[\\mathbb{H}_{P(Y \\mid x, \\omega)}(Y)\\right]$ would be zero if $Y$ is a deterministic function of $x$.","metadata":{"_uuid":"3c940915-98e2-4f2b-a165-025e4f81793c","_cell_guid":"ed138227-78a3-4a14-8edd-d2ae523faa29","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.3 Evidential methods as a straightforward Bayesian decomposition (6 points)\n\nEvidential Deep Learning allows the inference of epistemic uncertainty in a single forward pass (i.e., without Monte-Carlo samples) by fitting a prior distribution over likelihood functions. Conjugacy allows us to formulate a tractable posterior predictive given a likelihood and conjugate prior distribution. We use the negative log-likelihood of this posterior predictive as an objective function for training.","metadata":{}},{"cell_type":"markdown","source":"### 4.3.1 Fundamentals of Evidential Deep Learning (3 points)\n\nIn the following, we will employ EDL in a regression setting. For simplicity, we assume a Gaussian likelihood and a Normal-Inverse-Gamma (NIG) prior. The NLL loss-function is composed as follows.\n\n$$\n\\begin{split}\n\\mathcal{L}_{nll} &= - \\log \\int_{\\sigma^2=0}^{\\infty} \\int_{\\mu=-\\infty}^{\\infty} \\mathcal{N}(y \\vert \\mu, \\sigma^2) \\mathrm{NIG}(\\mu, \\sigma^2 \\vert \\mu_0, \\alpha, \\beta, \\nu) d\\mu d\\sigma^2 \\\\\n&= - \\log \\mathrm{St}_{2\\alpha}\\left(y; \\mu_0, \\frac{\\beta (1 + \\nu)}{\\nu \\alpha} \\right)\n\\end{split}\n$$\n\nPost-training, the relevant quantities can be computed from the predicted parameters.\n\n$$\n\\hat y = \\mu_0, \\quad \\underbrace{\\mathbb{E}[\\sigma^2] = \\frac{\\beta}{\\alpha - 1}}_{aleatoric}, \\quad \\underbrace{\\mathrm{Var}[\\mu] = \\frac{\\beta}{\\nu(\\alpha-1)}}_{epistemic}\n$$\n\n(a) Discuss the role of the evidential parameters $\\{\\mu_0, \\nu, \\alpha, \\beta\\}$ in the EDL regression model with regards to their impact on uncertainty estimates. Provide examples of how these parameters might change under varied data scenarios, such as high noise or OOD samples.\n\n**WRITE YOUR ANSWER HERE** <br>\n$\\mu_0$ is our prior belief of the mean $\\mu$ whereas $\\nu$ represents the confidence in $\\mu_0$. If $\\nu$ is higher, we have higher confidence in $\\mu_0$ and the epistemic uncertainty is lower. However, in case of OOD samples, $\\nu$ would be lower and the epistemic uncertainty ($Var[\\mu]$) would be higher since we have a lower confidence in $\\mu_0$. <br>\nThe parameter $\\beta$ corresponds to the expected value of $\\sigma^2$ while $\\alpha$ represents the confidence in $\\sigma^2$. In case of noisy data, $\\alpha$ would be lower, which implies that the aleatoric uncertainty would be higher.\n\n(b) Explain how EDL differs from alternative uncertainty estimation methods, such as Monte Carlo dropout, deep ensembles, or other Bayesian approximations. Focus on computational complexity and the ability to distinguish between aleatoric and epistemic uncertainty.\n\n**WRITE YOUR ANSWER HERE** <br>\nOther uncertainty quantification methods require an ensemble of multiple models or require running the same model multiple times. Thus, they consume a large amount of computational resources and memory. We may not get reproducible uncertainty estimates due to the randomness in these methods. Also, many of these methods only compute the overall predictive uncertainty and may not decompose it into epistemic and aleatoric components.\n\nOn the other hand, EDL optimizes the parameters of an NIG distribution, which results a closed form estimate of aleatoric and epistemic uncertainty. In case of EDL, we do not need to run multiple models unlike the other uncertainty quantification methods.","metadata":{}},{"cell_type":"markdown","source":"### 4.3.2 Limitations of EDL (3 points)\n\nWhile the formulation of EDL is analytically sound, the approach is plagued by a lack of proper scoring rules. A common, partial, remedy is the addition of heuristic regularization. In the following exercise, you will investigate the impact of regularization on a simple EDL toy experiment, there will be no coding required.\n\nFirst, we will define both loss functions.","metadata":{}},{"cell_type":"code","source":"# Define NLL loss functions\ndef NIG_NLL(y, mu, nu, alpha, beta):\n    two_b_lambda = 2 * beta * (1 + nu)\n    nll = 0.5 * torch.log(torch.pi / (nu + 1e-8)) \\\n        - alpha * torch.log(two_b_lambda) \\\n        + (alpha + 0.5) * torch.log(nu * torch.square(y - mu) + two_b_lambda) \\\n        + torch.lgamma(alpha) \\\n        - torch.lgamma(alpha + 0.5)\n\n    return nll.mean()\n\n\ndef reg_NIG_NLL(y, mu, nu, alpha, beta):\n    two_b_lambda = 2 * beta * (1 + nu)\n    nll = 0.5 * torch.log(torch.pi / (nu + 1e-8)) \\\n        - alpha * torch.log(two_b_lambda) \\\n        + (alpha + 0.5) * torch.log(nu * torch.square(y - mu) + two_b_lambda) \\\n        + torch.lgamma(alpha) \\\n        - torch.lgamma(alpha + 0.5)\n\n    reg = torch.square(y - mu).detach() * torch.reciprocal(beta / (alpha - 1.)) + 2 * alpha\n    loss = nll + 1e-2 * reg\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T17:24:31.567509Z","iopub.execute_input":"2025-01-30T17:24:31.568229Z","iopub.status.idle":"2025-01-30T17:24:31.579781Z","shell.execute_reply.started":"2025-01-30T17:24:31.568189Z","shell.execute_reply":"2025-01-30T17:24:31.578564Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Next, some simple code to run and visualize the experiments.","metadata":{}},{"cell_type":"code","source":"from unc_utils import edl_train, edl_visualize, UnivariateNonUniformData, DenseInverseGamma, UnivariateDerNet\n\nEPOCHS=120\nOOD_lower = -10.\nOOD_upper = 20.\n\nnet = UnivariateDerNet()\nnet.to(device)\n\n# Generate simple toy problem\ntrain_data = UnivariateNonUniformData(N=2000, X_intervals=[(-2., 10.)], X_distribution=\"gaussian\")\ntest_data = UnivariateNonUniformData(N=100, X_intervals=[(OOD_lower, OOD_upper)])\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n\n# Define optimizer\noptimizer_params = {\n    \"lr\": 1e-03,\n    \"betas\": (0.9, 0.999),\n    \"eps\": 1e-8,\n    \"weight_decay\": 1e-2,\n    \"amsgrad\": False}\n\noptimizer = torch.optim.AdamW(net.parameters(), **optimizer_params)\n\n# Initialize figures\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=optimizer_params[\"lr\"], steps_per_epoch=len(train_loader), epochs=EPOCHS)\n\n# Run training\nlosses = edl_train(net=net,\n    criterion=NIG_NLL,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    train_loader=train_loader,\n    test_data=test_data,\n    epochs=EPOCHS,\n    device=device)\n\n# Evaluate on larger subspace including unseen data\nline_x = np.linspace(OOD_lower, OOD_upper, num=200)\noutputs = net(torch.Tensor(np.expand_dims(line_x, axis=1)).to(device))\nmu0, nu, alpha, beta = (out.detach().cpu().numpy() for out in outputs)\n\n# Compute uncertainties according to Normal-Inverse-Gamma definition\naleatoric = np.sqrt(beta / (alpha - 1.))\nepistemic = np.sqrt(beta / (nu * (alpha - 1.)))\n\nedl_visualize(mu0, aleatoric, epistemic, train_data, test_data, axes[0])\n\n# Reset scheduler and network parameters\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=optimizer_params[\"lr\"], steps_per_epoch=len(train_loader), epochs=EPOCHS)\nnet.reset()\n\n# Run training\nlosses = edl_train(net=net,\n    criterion=reg_NIG_NLL,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    train_loader=train_loader,\n    test_data=test_data,\n    epochs=EPOCHS,\n    device=device)\n\n# Evaluate on larger subspace including unseen data\nline_x = np.linspace(OOD_lower, OOD_upper, num=200)\noutputs = net(torch.Tensor(np.expand_dims(line_x, axis=1)).to(device))\nmu0, nu, alpha, beta = (out.detach().cpu().numpy() for out in outputs)\n\n# Compute uncertainties according to Normal-Inverse-Gamma definition\naleatoric = np.sqrt(beta / (alpha - 1.))\nepistemic = np.sqrt(beta / (nu * (alpha - 1.)))\n\nedl_visualize(mu0, aleatoric, epistemic, train_data, test_data, axes[1])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T18:04:52.516585Z","iopub.execute_input":"2025-01-30T18:04:52.517564Z","iopub.status.idle":"2025-01-30T18:05:24.599669Z","shell.execute_reply.started":"2025-01-30T18:04:52.517464Z","shell.execute_reply":"2025-01-30T18:05:24.598415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"718.467187pt\" height=\"278.884525pt\" viewBox=\"0 0 718.467187 278.884525\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-01-30T18:05:24.473931</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.5, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 278.884525 \nL 718.467187 278.884525 \nL 718.467187 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 35.304688 255.0064 \nL 339.668324 255.0064 \nL 339.668324 33.2464 \nL 35.304688 33.2464 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 35.304688 255.0064 \nL 339.668324 255.0064 \nL 339.668324 33.2464 \nL 35.304688 33.2464 \nz\n\" clip-path=\"url(#p0613878dd6)\" style=\"fill: #404040; fill-opacity: 0.25098; stroke: #404040; stroke-opacity: 0.25098; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 116.468324 255.0064 \nL 238.213778 255.0064 \nL 238.213778 33.2464 \nL 116.468324 33.2464 \nz\n\" clip-path=\"url(#p0613878dd6)\" style=\"fill: #ffffff; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path id=\"m41a03e1542\" d=\"M -3 0 \nL 3 0 \nM 0 3 \nL 0 -3 \n\" style=\"stroke: #0000ff; stroke-width: 1.5\"/>\n    </defs>\n    <g clip-path=\"url(#p0613878dd6)\">\n     <use xlink:href=\"#m41a03e1542\" x=\"235.088309\" y=\"177.799544\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"205.603799\" y=\"135.882772\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"223.14145\" y=\"121.809961\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"213.028294\" y=\"88.664847\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"184.390594\" y=\"184.57282\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"200.968784\" y=\"127.796051\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"191.237376\" y=\"166.227107\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"164.796625\" y=\"136.952034\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"185.162344\" y=\"175.91581\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"178.166249\" y=\"153.866521\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"201.279447\" y=\"148.20986\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"216.574255\" y=\"86.387737\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"156.650685\" y=\"143.615647\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"144.879102\" y=\"140.793173\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"235.436674\" y=\"153.561591\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"154.392122\" y=\"132.090114\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"177.89899\" y=\"165.400406\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"142.390947\" y=\"142.779375\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"220.318303\" y=\"92.512261\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"175.746883\" y=\"163.859978\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"194.822527\" y=\"165.901343\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"195.353828\" y=\"162.445155\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"176.353853\" y=\"155.732049\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"190.882162\" y=\"153.770054\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"174.407513\" y=\"154.911158\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"128.747258\" y=\"137.416651\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"212.686264\" y=\"93.887042\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"183.101769\" y=\"168.966726\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"139.280166\" y=\"142.172599\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"152.671873\" y=\"137.027815\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"202.641932\" y=\"134.249615\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"162.019511\" y=\"139.685339\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"146.413059\" y=\"142.677954\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"178.67249\" y=\"161.094579\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"178.00408\" y=\"164.596222\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"230.449209\" y=\"150.36577\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"197.374026\" y=\"152.151454\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"135.845864\" y=\"143.529436\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"173.323114\" y=\"152.668935\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"135.00896\" y=\"142.051445\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"194.298314\" y=\"160.103907\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"233.715094\" y=\"154.601368\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"193.876593\" y=\"170.601995\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"200.818505\" y=\"141.932262\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"234.759216\" y=\"160.913714\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"185.182008\" y=\"160.453233\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"126.994249\" y=\"139.367766\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"136.281419\" y=\"145.120393\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"139.486776\" y=\"143.38549\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"158.369328\" y=\"137.772075\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"164.214559\" y=\"126.935728\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"184.476104\" y=\"170.414792\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"148.539\" y=\"136.4782\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"234.608979\" y=\"142.257889\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"151.976303\" y=\"137.850764\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"145.335419\" y=\"138.899296\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"197.850572\" y=\"150.165853\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"216.808322\" y=\"97.574317\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"180.89404\" y=\"164.791312\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"157.467119\" y=\"128.064825\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"175.84425\" y=\"153.67569\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"177.445394\" y=\"165.639311\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"215.062059\" y=\"107.806737\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"215.330874\" y=\"91.326311\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"141.845656\" y=\"141.626084\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"183.629099\" y=\"162.930343\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"135.314288\" y=\"143.127123\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"172.57372\" y=\"144.029376\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"162.319116\" y=\"136.755038\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"156.632444\" y=\"129.853644\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"190.756457\" y=\"153.320423\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"160.555641\" y=\"132.904354\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"143.240539\" y=\"143.58662\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"194.198764\" y=\"161.87052\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"216.32221\" y=\"103.917346\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"194.033743\" y=\"152.916656\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"168.649779\" y=\"135.904864\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"199.117114\" y=\"138.431976\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"237.602032\" y=\"177.839854\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"136.244935\" y=\"141.915328\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"196.955942\" y=\"170.198562\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"176.700702\" y=\"168.684177\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"207.663312\" y=\"137.448512\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"210.395746\" y=\"119.154698\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"140.297066\" y=\"143.151004\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"198.897235\" y=\"153.271731\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"190.212006\" y=\"170.479905\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"160.978937\" y=\"133.834452\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"160.944686\" y=\"135.160925\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"163.795207\" y=\"133.728128\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"188.721885\" y=\"176.106721\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"125.430368\" y=\"140.559968\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"153.244483\" y=\"130.487893\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"144.748113\" y=\"144.416367\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"210.320841\" y=\"111.645964\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"159.163853\" y=\"136.264952\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"161.094168\" y=\"140.677016\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"235.223583\" y=\"152.88063\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"130.768912\" y=\"139.966693\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"161.837764\" y=\"129.886301\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <defs>\n     <path id=\"mb66081e307\" d=\"M 35.304688 -139.922835 \nL 35.304688 -127.688415 \nL 36.834153 -128.462915 \nL 38.363619 -129.234729 \nL 39.893084 -130.003945 \nL 41.42255 -130.770719 \nL 42.952015 -131.535265 \nL 44.481481 -132.297801 \nL 46.010946 -133.058608 \nL 47.540412 -133.817923 \nL 49.069877 -134.576022 \nL 50.599343 -135.3331 \nL 52.128808 -136.089328 \nL 53.658274 -136.844724 \nL 55.187739 -137.599203 \nL 56.717205 -138.352464 \nL 58.24667 -139.103956 \nL 59.776136 -139.852892 \nL 61.305601 -140.598119 \nL 62.835067 -141.338102 \nL 64.364532 -142.070986 \nL 65.893998 -142.794454 \nL 67.423463 -143.505818 \nL 68.952929 -144.202052 \nL 70.482394 -144.879753 \nL 72.01186 -145.53528 \nL 73.541325 -146.164781 \nL 75.070791 -146.764255 \nL 76.600256 -147.329622 \nL 78.129722 -147.856824 \nL 79.659187 -148.341881 \nL 81.188653 -148.780939 \nL 82.718118 -149.170358 \nL 84.247584 -149.506786 \nL 85.777049 -149.787248 \nL 87.306515 -150.009153 \nL 88.83598 -150.170444 \nL 90.365446 -150.269565 \nL 91.894911 -150.305537 \nL 93.424377 -150.278 \nL 94.953842 -150.187205 \nL 96.483308 -150.033972 \nL 98.012773 -149.819716 \nL 99.542239 -149.546325 \nL 101.071704 -149.216115 \nL 102.60117 -148.831735 \nL 104.130635 -148.395978 \nL 105.660101 -147.911756 \nL 107.189566 -147.381899 \nL 108.719032 -146.809117 \nL 110.248497 -146.195865 \nL 111.777963 -145.544354 \nL 113.307428 -144.856517 \nL 114.836894 -144.134057 \nL 116.366359 -143.37853 \nL 117.895825 -142.591499 \nL 119.425291 -141.77478 \nL 120.954756 -140.930815 \nL 122.484222 -140.063224 \nL 124.013687 -139.177652 \nL 125.543153 -138.282834 \nL 127.072618 -137.392043 \nL 128.602084 -136.524603 \nL 130.131549 -135.707186 \nL 131.661015 -134.974177 \nL 133.19048 -134.366122 \nL 134.719946 -133.925728 \nL 136.249411 -133.691719 \nL 137.778877 -133.692232 \nL 139.308342 -133.939749 \nL 140.837808 -134.428847 \nL 142.367273 -135.136594 \nL 143.896739 -136.024635 \nL 145.426204 -137.042038 \nL 146.95567 -138.128574 \nL 148.485135 -139.218492 \nL 150.014601 -140.244797 \nL 151.544066 -141.143498 \nL 153.073532 -141.857169 \nL 154.602997 -142.337277 \nL 156.132463 -142.545194 \nL 157.661928 -142.452152 \nL 159.191394 -142.038508 \nL 160.720859 -141.292417 \nL 162.250325 -140.208189 \nL 163.77979 -138.78481 \nL 165.309256 -137.02562 \nL 166.838721 -134.940109 \nL 168.368187 -132.547736 \nL 169.897652 -129.882663 \nL 171.427118 -126.997525 \nL 172.956583 -123.964763 \nL 174.486049 -120.874975 \nL 176.015514 -117.831667 \nL 177.54498 -114.943357 \nL 179.074445 -112.314742 \nL 180.603911 -110.039527 \nL 182.133376 -108.196367 \nL 183.662842 -106.847837 \nL 185.192307 -106.041322 \nL 186.721773 -105.810864 \nL 188.251238 -106.178801 \nL 189.780704 -107.157155 \nL 191.310169 -108.74831 \nL 192.839635 -110.945296 \nL 194.3691 -113.731672 \nL 195.898566 -117.080978 \nL 197.428031 -120.956104 \nL 198.957497 -125.308509 \nL 200.486963 -130.077101 \nL 202.016428 -135.187291 \nL 203.545894 -140.549921 \nL 205.075359 -146.060503 \nL 206.604825 -151.598392 \nL 208.13429 -157.026299 \nL 209.663756 -162.188898 \nL 211.193221 -166.90981 \nL 212.722687 -170.986448 \nL 214.252152 -174.184885 \nL 215.781618 -176.245163 \nL 217.311083 -176.913314 \nL 218.840549 -176.009099 \nL 220.370014 -173.512212 \nL 221.89948 -169.617058 \nL 223.428945 -164.690967 \nL 224.958411 -159.142095 \nL 226.487876 -153.302524 \nL 228.017342 -147.393891 \nL 229.546807 -141.549485 \nL 231.076273 -135.848001 \nL 232.605738 -130.338544 \nL 234.135204 -125.055485 \nL 235.664669 -120.024907 \nL 237.194135 -115.266029 \nL 238.7236 -110.789647 \nL 240.253066 -106.597093 \nL 241.782531 -102.679498 \nL 243.311997 -99.019671 \nL 244.841462 -95.594862 \nL 246.370928 -92.379897 \nL 247.900393 -89.349775 \nL 249.429859 -86.481125 \nL 250.959324 -83.75313 \nL 252.48879 -81.147729 \nL 254.018255 -78.649414 \nL 255.547721 -76.245248 \nL 257.077186 -73.924238 \nL 258.606652 -71.67717 \nL 260.136117 -69.496186 \nL 261.665583 -67.374745 \nL 263.195048 -65.307203 \nL 264.724514 -63.288683 \nL 266.253979 -61.315003 \nL 267.783445 -59.382468 \nL 269.31291 -57.4878 \nL 270.842376 -55.628138 \nL 272.371841 -53.800914 \nL 273.901307 -52.00373 \nL 275.430772 -50.234447 \nL 276.960238 -48.491193 \nL 278.489703 -46.772102 \nL 280.019169 -45.075576 \nL 281.548635 -43.400092 \nL 283.0781 -41.744225 \nL 284.607566 -40.106661 \nL 286.137031 -38.486238 \nL 287.666497 -36.881836 \nL 289.195962 -35.292396 \nL 290.725428 -33.716978 \nL 292.254893 -32.15463 \nL 293.784359 -30.604611 \nL 295.313824 -29.066055 \nL 296.84329 -27.538338 \nL 298.372755 -26.020772 \nL 299.902221 -24.512691 \nL 301.431686 -23.013609 \nL 302.961152 -21.522944 \nL 304.490617 -20.040189 \nL 306.020083 -18.565005 \nL 307.549548 -17.096769 \nL 309.079014 -15.635299 \nL 310.608479 -14.180101 \nL 312.137945 -12.730888 \nL 313.66741 -11.287331 \nL 315.196876 -9.849179 \nL 316.726341 -8.416112 \nL 318.255807 -6.987889 \nL 319.785272 -5.564308 \nL 321.314738 -4.145157 \nL 322.844203 -2.730152 \nL 324.373669 -1.319144 \nL 325.903134 0.087973 \nL 327.4326 1.491463 \nL 328.962065 2.891443 \nL 330.491531 4.288059 \nL 332.020996 5.681408 \nL 333.550462 7.071681 \nL 335.079927 8.458981 \nL 336.609393 9.843406 \nL 338.138858 11.225102 \nL 339.668324 12.604102 \nL 339.668324 11.958855 \nL 339.668324 11.958855 \nL 338.138858 10.573088 \nL 336.609393 9.18454 \nL 335.079927 7.793178 \nL 333.550462 6.398856 \nL 332.020996 5.001457 \nL 330.491531 3.600896 \nL 328.962065 2.196983 \nL 327.4326 0.789602 \nL 325.903134 -0.621396 \nL 324.373669 -2.036105 \nL 322.844203 -3.454833 \nL 321.314738 -4.877642 \nL 319.785272 -6.304723 \nL 318.255807 -7.736362 \nL 316.726341 -9.172749 \nL 315.196876 -10.614105 \nL 313.66741 -12.060675 \nL 312.137945 -13.512775 \nL 310.608479 -14.970681 \nL 309.079014 -16.43472 \nL 307.549548 -17.905198 \nL 306.020083 -19.382592 \nL 304.490617 -20.867124 \nL 302.961152 -22.359396 \nL 301.431686 -23.859789 \nL 299.902221 -25.368811 \nL 298.372755 -26.887064 \nL 296.84329 -28.415035 \nL 295.313824 -29.953412 \nL 293.784359 -31.502923 \nL 292.254893 -33.064192 \nL 290.725428 -34.63813 \nL 289.195962 -36.225518 \nL 287.666497 -37.827309 \nL 286.137031 -39.444506 \nL 284.607566 -41.078189 \nL 283.0781 -42.729542 \nL 281.548635 -44.399769 \nL 280.019169 -46.090263 \nL 278.489703 -47.802497 \nL 276.960238 -49.538085 \nL 275.430772 -51.298702 \nL 273.901307 -53.086331 \nL 272.371841 -54.902961 \nL 270.842376 -56.750859 \nL 269.31291 -58.632589 \nL 267.783445 -60.550901 \nL 266.253979 -62.508899 \nL 264.724514 -64.510114 \nL 263.195048 -66.55856 \nL 261.665583 -68.658819 \nL 260.136117 -70.816223 \nL 258.606652 -73.036998 \nL 257.077186 -75.328394 \nL 255.547721 -77.699114 \nL 254.018255 -80.159441 \nL 252.48879 -82.721667 \nL 250.959324 -85.400317 \nL 249.429859 -88.212822 \nL 247.900393 -91.17959 \nL 246.370928 -94.324212 \nL 244.841462 -97.673232 \nL 243.311997 -101.255161 \nL 241.782531 -105.098738 \nL 240.253066 -109.229866 \nL 238.7236 -113.667814 \nL 237.194135 -118.421716 \nL 235.664669 -123.488162 \nL 234.135204 -128.851947 \nL 232.605738 -134.487666 \nL 231.076273 -140.362191 \nL 229.546807 -146.433963 \nL 228.017342 -152.646722 \nL 226.487876 -158.914176 \nL 224.958411 -165.094413 \nL 223.428945 -170.956639 \nL 221.89948 -176.161581 \nL 220.370014 -180.298369 \nL 218.840549 -183.000134 \nL 217.311083 -184.072875 \nL 215.781618 -183.537274 \nL 214.252152 -181.575993 \nL 212.722687 -178.447777 \nL 211.193221 -174.418258 \nL 209.663756 -169.726489 \nL 208.13429 -164.578995 \nL 206.604825 -159.154885 \nL 205.075359 -153.61128 \nL 203.545894 -148.086595 \nL 202.016428 -142.702128 \nL 200.486963 -137.562724 \nL 198.957497 -132.757706 \nL 197.428031 -128.361704 \nL 195.898566 -124.435769 \nL 194.3691 -121.028332 \nL 192.839635 -118.176345 \nL 191.310169 -115.906083 \nL 189.780704 -114.233826 \nL 188.251238 -113.166427 \nL 186.721773 -112.701496 \nL 185.192307 -112.82713 \nL 183.662842 -113.521261 \nL 182.133376 -114.750276 \nL 180.603911 -116.467341 \nL 179.074445 -118.610499 \nL 177.54498 -121.101713 \nL 176.015514 -123.847857 \nL 174.486049 -126.744835 \nL 172.956583 -129.68494 \nL 171.427118 -132.565973 \nL 169.897652 -135.299245 \nL 168.368187 -137.814506 \nL 166.838721 -140.060775 \nL 165.309256 -142.004093 \nL 163.77979 -143.623294 \nL 162.250325 -144.905524 \nL 160.720859 -145.843222 \nL 159.191394 -146.433265 \nL 157.661928 -146.678072 \nL 156.132463 -146.587659 \nL 154.602997 -146.181675 \nL 153.073532 -145.491004 \nL 151.544066 -144.558583 \nL 150.014601 -143.439242 \nL 148.485135 -142.19811 \nL 146.95567 -140.907407 \nL 145.426204 -139.641961 \nL 143.896739 -138.474244 \nL 142.367273 -137.469816 \nL 140.837808 -136.683619 \nL 139.308342 -136.156992 \nL 137.778877 -135.915093 \nL 136.249411 -135.964915 \nL 134.719946 -136.294771 \nL 133.19048 -136.876288 \nL 131.661015 -137.669244 \nL 130.131549 -138.628109 \nL 128.602084 -139.708338 \nL 127.072618 -140.870657 \nL 125.543153 -142.082929 \nL 124.013687 -143.320067 \nL 122.484222 -144.562969 \nL 120.954756 -145.797194 \nL 119.425291 -147.011675 \nL 117.895825 -148.197798 \nL 116.366359 -149.348663 \nL 114.836894 -150.458581 \nL 113.307428 -151.522756 \nL 111.777963 -152.537034 \nL 110.248497 -153.497733 \nL 108.719032 -154.401533 \nL 107.189566 -155.245378 \nL 105.660101 -156.026474 \nL 104.130635 -156.742213 \nL 102.60117 -157.390241 \nL 101.071704 -157.968454 \nL 99.542239 -158.475132 \nL 98.012773 -158.908886 \nL 96.483308 -159.268818 \nL 94.953842 -159.554558 \nL 93.424377 -159.766256 \nL 91.894911 -159.904687 \nL 90.365446 -159.971142 \nL 88.83598 -159.967496 \nL 87.306515 -159.896135 \nL 85.777049 -159.759925 \nL 84.247584 -159.562095 \nL 82.718118 -159.306256 \nL 81.188653 -158.996275 \nL 79.659187 -158.636195 \nL 78.129722 -158.230211 \nL 76.600256 -157.782563 \nL 75.070791 -157.297467 \nL 73.541325 -156.779091 \nL 72.01186 -156.231488 \nL 70.482394 -155.658552 \nL 68.952929 -155.063934 \nL 67.423463 -154.451028 \nL 65.893998 -153.822925 \nL 64.364532 -153.182324 \nL 62.835067 -152.531574 \nL 61.305601 -151.87264 \nL 59.776136 -151.207038 \nL 58.24667 -150.535967 \nL 56.717205 -149.860261 \nL 55.187739 -149.180416 \nL 53.658274 -148.496701 \nL 52.128808 -147.809172 \nL 50.599343 -147.117695 \nL 49.069877 -146.422059 \nL 47.540412 -145.721929 \nL 46.010946 -145.016994 \nL 44.481481 -144.306863 \nL 42.952015 -143.591254 \nL 41.42255 -142.869851 \nL 39.893084 -142.142436 \nL 38.363619 -141.408839 \nL 36.834153 -140.668969 \nL 35.304688 -139.922835 \nz\n\" style=\"stroke: #8a2be2; stroke-opacity: 0.501961\"/>\n    </defs>\n    <g clip-path=\"url(#p0613878dd6)\">\n     <use xlink:href=\"#mb66081e307\" x=\"0\" y=\"278.884525\" style=\"fill: #8a2be2; fill-opacity: 0.501961; stroke: #8a2be2; stroke-opacity: 0.501961\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_2\">\n    <defs>\n     <path id=\"m2dba570a02\" d=\"M 35.304688 -127.688415 \nL 35.304688 -119.530641 \nL 36.834153 -120.41968 \nL 38.363619 -121.306885 \nL 39.893084 -122.192295 \nL 41.42255 -123.076008 \nL 42.952015 -123.958153 \nL 44.481481 -124.83882 \nL 46.010946 -125.718142 \nL 47.540412 -126.596191 \nL 49.069877 -127.473045 \nL 50.599343 -128.348682 \nL 52.128808 -129.223039 \nL 53.658274 -130.095862 \nL 55.187739 -130.966798 \nL 56.717205 -131.835224 \nL 58.24667 -132.700273 \nL 59.776136 -133.560814 \nL 61.305601 -134.415352 \nL 62.835067 -135.262006 \nL 64.364532 -136.098585 \nL 65.893998 -136.922464 \nL 67.423463 -137.730702 \nL 68.952929 -138.520059 \nL 70.482394 -139.287004 \nL 72.01186 -140.027856 \nL 73.541325 -140.738806 \nL 75.070791 -141.416002 \nL 76.600256 -142.055602 \nL 78.129722 -142.653877 \nL 79.659187 -143.207247 \nL 81.188653 -143.712312 \nL 82.718118 -144.16594 \nL 84.247584 -144.565308 \nL 85.777049 -144.908003 \nL 87.306515 -145.191995 \nL 88.83598 -145.415809 \nL 90.365446 -145.578479 \nL 91.894911 -145.679639 \nL 93.424377 -145.719557 \nL 94.953842 -145.699137 \nL 96.483308 -145.619884 \nL 98.012773 -145.483909 \nL 99.542239 -145.29381 \nL 101.071704 -145.052602 \nL 102.60117 -144.763605 \nL 104.130635 -144.43021 \nL 105.660101 -144.055837 \nL 107.189566 -143.643684 \nL 108.719032 -143.196659 \nL 110.248497 -142.717206 \nL 111.777963 -142.20728 \nL 113.307428 -141.668293 \nL 114.836894 -141.101125 \nL 116.366359 -140.506203 \nL 117.895825 -139.883632 \nL 119.425291 -139.233449 \nL 120.954756 -138.555999 \nL 122.484222 -137.852521 \nL 124.013687 -137.126042 \nL 125.543153 -136.382504 \nL 127.072618 -135.632291 \nL 128.602084 -134.891828 \nL 130.131549 -134.184974 \nL 131.661015 -133.543477 \nL 133.19048 -133.005525 \nL 134.719946 -132.611839 \nL 136.249411 -132.399579 \nL 137.778877 -132.395749 \nL 139.308342 -132.61213 \nL 140.837808 -133.04305 \nL 142.367273 -133.665875 \nL 143.896739 -134.443273 \nL 145.426204 -135.326255 \nL 146.95567 -136.257573 \nL 148.485135 -137.175461 \nL 150.014601 -138.017625 \nL 151.544066 -138.725028 \nL 153.073532 -139.244888 \nL 154.602997 -139.532513 \nL 156.132463 -139.552009 \nL 157.661928 -139.276058 \nL 159.191394 -138.685121 \nL 160.720859 -137.766036 \nL 162.250325 -136.510396 \nL 163.77979 -134.913247 \nL 165.309256 -132.973373 \nL 166.838721 -130.696069 \nL 168.368187 -128.097991 \nL 169.897652 -125.212414 \nL 171.427118 -122.092671 \nL 172.956583 -118.8125 \nL 174.486049 -115.463567 \nL 176.015514 -112.149954 \nL 177.54498 -108.980642 \nL 179.074445 -106.061199 \nL 180.603911 -103.486949 \nL 182.133376 -101.338991 \nL 183.662842 -99.682947 \nL 185.192307 -98.569521 \nL 186.721773 -98.036014 \nL 188.251238 -98.107576 \nL 189.780704 -98.798374 \nL 191.310169 -100.112011 \nL 192.839635 -102.041738 \nL 194.3691 -104.570306 \nL 195.898566 -107.66963 \nL 197.428031 -111.300457 \nL 198.957497 -115.412181 \nL 200.486963 -119.942468 \nL 202.016428 -124.817136 \nL 203.545894 -129.949814 \nL 205.075359 -135.241395 \nL 206.604825 -140.57869 \nL 208.13429 -145.832485 \nL 209.663756 -150.853891 \nL 211.193221 -155.468675 \nL 212.722687 -159.469355 \nL 214.252152 -162.60732 \nL 215.781618 -164.595893 \nL 217.311083 -165.142267 \nL 218.840549 -164.020892 \nL 220.370014 -161.174933 \nL 221.89948 -156.793008 \nL 223.428945 -151.277815 \nL 224.958411 -145.097739 \nL 226.487876 -138.642883 \nL 228.017342 -132.176565 \nL 229.546807 -125.858056 \nL 231.076273 -119.781546 \nL 232.605738 -114.004682 \nL 234.135204 -108.564584 \nL 235.664669 -103.483841 \nL 237.194135 -98.771047 \nL 238.7236 -94.419447 \nL 240.253066 -90.407399 \nL 241.782531 -86.700897 \nL 243.311997 -83.259055 \nL 244.841462 -80.039475 \nL 246.370928 -77.00282 \nL 247.900393 -74.115073 \nL 249.429859 -71.348213 \nL 250.959324 -68.680074 \nL 252.48879 -66.093267 \nL 254.018255 -63.574253 \nL 255.547721 -61.1126 \nL 257.077186 -58.700064 \nL 258.606652 -56.330111 \nL 260.136117 -53.997401 \nL 261.665583 -51.697529 \nL 263.195048 -49.426811 \nL 264.724514 -47.181921 \nL 266.253979 -44.960135 \nL 267.783445 -42.75878 \nL 269.31291 -40.575517 \nL 270.842376 -38.408158 \nL 272.371841 -36.254735 \nL 273.901307 -34.113283 \nL 275.430772 -31.981951 \nL 276.960238 -29.859141 \nL 278.489703 -27.743057 \nL 280.019169 -25.63225 \nL 281.548635 -23.525143 \nL 283.0781 -21.420384 \nL 284.607566 -19.31646 \nL 286.137031 -17.212187 \nL 287.666497 -15.106244 \nL 289.195962 -12.997393 \nL 290.725428 -10.88446 \nL 292.254893 -8.766366 \nL 293.784359 -6.641939 \nL 295.313824 -4.510088 \nL 296.84329 -2.369916 \nL 298.372755 -0.220226 \nL 299.902221 1.939837 \nL 301.431686 4.111225 \nL 302.961152 6.295027 \nL 304.490617 8.492089 \nL 306.020083 10.70329 \nL 307.549548 12.929749 \nL 309.079014 15.172154 \nL 310.608479 17.431415 \nL 312.137945 19.708631 \nL 313.66741 22.004574 \nL 315.196876 24.320165 \nL 316.726341 26.656269 \nL 318.255807 29.013977 \nL 319.785272 31.394071 \nL 321.314738 33.797523 \nL 322.844203 36.225318 \nL 324.373669 38.678258 \nL 325.903134 41.157635 \nL 327.4326 43.664208 \nL 328.962065 46.198835 \nL 330.491531 48.762923 \nL 332.020996 51.357316 \nL 333.550462 53.983094 \nL 335.079927 56.641356 \nL 336.609393 59.333309 \nL 338.138858 62.059881 \nL 339.668324 64.822301 \nL 339.668324 12.604102 \nL 339.668324 12.604102 \nL 338.138858 11.225102 \nL 336.609393 9.843406 \nL 335.079927 8.458981 \nL 333.550462 7.071681 \nL 332.020996 5.681408 \nL 330.491531 4.288059 \nL 328.962065 2.891443 \nL 327.4326 1.491463 \nL 325.903134 0.087973 \nL 324.373669 -1.319144 \nL 322.844203 -2.730152 \nL 321.314738 -4.145157 \nL 319.785272 -5.564308 \nL 318.255807 -6.987889 \nL 316.726341 -8.416112 \nL 315.196876 -9.849179 \nL 313.66741 -11.287331 \nL 312.137945 -12.730888 \nL 310.608479 -14.180101 \nL 309.079014 -15.635299 \nL 307.549548 -17.096769 \nL 306.020083 -18.565005 \nL 304.490617 -20.040189 \nL 302.961152 -21.522944 \nL 301.431686 -23.013609 \nL 299.902221 -24.512691 \nL 298.372755 -26.020772 \nL 296.84329 -27.538338 \nL 295.313824 -29.066055 \nL 293.784359 -30.604611 \nL 292.254893 -32.15463 \nL 290.725428 -33.716978 \nL 289.195962 -35.292396 \nL 287.666497 -36.881836 \nL 286.137031 -38.486238 \nL 284.607566 -40.106661 \nL 283.0781 -41.744225 \nL 281.548635 -43.400092 \nL 280.019169 -45.075576 \nL 278.489703 -46.772102 \nL 276.960238 -48.491193 \nL 275.430772 -50.234447 \nL 273.901307 -52.00373 \nL 272.371841 -53.800914 \nL 270.842376 -55.628138 \nL 269.31291 -57.4878 \nL 267.783445 -59.382468 \nL 266.253979 -61.315003 \nL 264.724514 -63.288683 \nL 263.195048 -65.307203 \nL 261.665583 -67.374745 \nL 260.136117 -69.496186 \nL 258.606652 -71.67717 \nL 257.077186 -73.924238 \nL 255.547721 -76.245248 \nL 254.018255 -78.649414 \nL 252.48879 -81.147729 \nL 250.959324 -83.75313 \nL 249.429859 -86.481125 \nL 247.900393 -89.349775 \nL 246.370928 -92.379897 \nL 244.841462 -95.594862 \nL 243.311997 -99.019671 \nL 241.782531 -102.679498 \nL 240.253066 -106.597093 \nL 238.7236 -110.789647 \nL 237.194135 -115.266029 \nL 235.664669 -120.024907 \nL 234.135204 -125.055485 \nL 232.605738 -130.338544 \nL 231.076273 -135.848001 \nL 229.546807 -141.549485 \nL 228.017342 -147.393891 \nL 226.487876 -153.302524 \nL 224.958411 -159.142095 \nL 223.428945 -164.690967 \nL 221.89948 -169.617058 \nL 220.370014 -173.512212 \nL 218.840549 -176.009099 \nL 217.311083 -176.913314 \nL 215.781618 -176.245163 \nL 214.252152 -174.184885 \nL 212.722687 -170.986448 \nL 211.193221 -166.90981 \nL 209.663756 -162.188898 \nL 208.13429 -157.026299 \nL 206.604825 -151.598392 \nL 205.075359 -146.060503 \nL 203.545894 -140.549921 \nL 202.016428 -135.187291 \nL 200.486963 -130.077101 \nL 198.957497 -125.308509 \nL 197.428031 -120.956104 \nL 195.898566 -117.080978 \nL 194.3691 -113.731672 \nL 192.839635 -110.945296 \nL 191.310169 -108.74831 \nL 189.780704 -107.157155 \nL 188.251238 -106.178801 \nL 186.721773 -105.810864 \nL 185.192307 -106.041322 \nL 183.662842 -106.847837 \nL 182.133376 -108.196367 \nL 180.603911 -110.039527 \nL 179.074445 -112.314742 \nL 177.54498 -114.943357 \nL 176.015514 -117.831667 \nL 174.486049 -120.874975 \nL 172.956583 -123.964763 \nL 171.427118 -126.997525 \nL 169.897652 -129.882663 \nL 168.368187 -132.547736 \nL 166.838721 -134.940109 \nL 165.309256 -137.02562 \nL 163.77979 -138.78481 \nL 162.250325 -140.208189 \nL 160.720859 -141.292417 \nL 159.191394 -142.038508 \nL 157.661928 -142.452152 \nL 156.132463 -142.545194 \nL 154.602997 -142.337277 \nL 153.073532 -141.857169 \nL 151.544066 -141.143498 \nL 150.014601 -140.244797 \nL 148.485135 -139.218492 \nL 146.95567 -138.128574 \nL 145.426204 -137.042038 \nL 143.896739 -136.024635 \nL 142.367273 -135.136594 \nL 140.837808 -134.428847 \nL 139.308342 -133.939749 \nL 137.778877 -133.692232 \nL 136.249411 -133.691719 \nL 134.719946 -133.925728 \nL 133.19048 -134.366122 \nL 131.661015 -134.974177 \nL 130.131549 -135.707186 \nL 128.602084 -136.524603 \nL 127.072618 -137.392043 \nL 125.543153 -138.282834 \nL 124.013687 -139.177652 \nL 122.484222 -140.063224 \nL 120.954756 -140.930815 \nL 119.425291 -141.77478 \nL 117.895825 -142.591499 \nL 116.366359 -143.37853 \nL 114.836894 -144.134057 \nL 113.307428 -144.856517 \nL 111.777963 -145.544354 \nL 110.248497 -146.195865 \nL 108.719032 -146.809117 \nL 107.189566 -147.381899 \nL 105.660101 -147.911756 \nL 104.130635 -148.395978 \nL 102.60117 -148.831735 \nL 101.071704 -149.216115 \nL 99.542239 -149.546325 \nL 98.012773 -149.819716 \nL 96.483308 -150.033972 \nL 94.953842 -150.187205 \nL 93.424377 -150.278 \nL 91.894911 -150.305537 \nL 90.365446 -150.269565 \nL 88.83598 -150.170444 \nL 87.306515 -150.009153 \nL 85.777049 -149.787248 \nL 84.247584 -149.506786 \nL 82.718118 -149.170358 \nL 81.188653 -148.780939 \nL 79.659187 -148.341881 \nL 78.129722 -147.856824 \nL 76.600256 -147.329622 \nL 75.070791 -146.764255 \nL 73.541325 -146.164781 \nL 72.01186 -145.53528 \nL 70.482394 -144.879753 \nL 68.952929 -144.202052 \nL 67.423463 -143.505818 \nL 65.893998 -142.794454 \nL 64.364532 -142.070986 \nL 62.835067 -141.338102 \nL 61.305601 -140.598119 \nL 59.776136 -139.852892 \nL 58.24667 -139.103956 \nL 56.717205 -138.352464 \nL 55.187739 -137.599203 \nL 53.658274 -136.844724 \nL 52.128808 -136.089328 \nL 50.599343 -135.3331 \nL 49.069877 -134.576022 \nL 47.540412 -133.817923 \nL 46.010946 -133.058608 \nL 44.481481 -132.297801 \nL 42.952015 -131.535265 \nL 41.42255 -130.770719 \nL 39.893084 -130.003945 \nL 38.363619 -129.234729 \nL 36.834153 -128.462915 \nL 35.304688 -127.688415 \nz\n\" style=\"stroke: #3cb371; stroke-opacity: 0.501961\"/>\n    </defs>\n    <g clip-path=\"url(#p0613878dd6)\">\n     <use xlink:href=\"#m2dba570a02\" x=\"0\" y=\"278.884525\" style=\"fill: #3cb371; fill-opacity: 0.501961; stroke: #3cb371; stroke-opacity: 0.501961\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_3\">\n    <defs>\n     <path id=\"mfe3291ab18\" d=\"M 35.304688 -139.922835 \nL 35.304688 -148.08061 \nL 36.834153 -148.712206 \nL 38.363619 -149.336685 \nL 39.893084 -149.954087 \nL 41.42255 -150.564562 \nL 42.952015 -151.168367 \nL 44.481481 -151.765845 \nL 46.010946 -152.35746 \nL 47.540412 -152.943662 \nL 49.069877 -153.525037 \nL 50.599343 -154.102113 \nL 52.128808 -154.675462 \nL 53.658274 -155.245563 \nL 55.187739 -155.812819 \nL 56.717205 -156.377502 \nL 58.24667 -156.93965 \nL 59.776136 -157.499117 \nL 61.305601 -158.055407 \nL 62.835067 -158.60767 \nL 64.364532 -159.154726 \nL 65.893998 -159.694916 \nL 67.423463 -160.226145 \nL 68.952929 -160.745926 \nL 70.482394 -161.251301 \nL 72.01186 -161.738912 \nL 73.541325 -162.205066 \nL 75.070791 -162.64572 \nL 76.600256 -163.056584 \nL 78.129722 -163.43316 \nL 79.659187 -163.770828 \nL 81.188653 -164.0649 \nL 82.718118 -164.310674 \nL 84.247584 -164.503571 \nL 85.777049 -164.639171 \nL 87.306515 -164.713292 \nL 88.83598 -164.722132 \nL 90.365446 -164.662229 \nL 91.894911 -164.530589 \nL 93.424377 -164.324698 \nL 94.953842 -164.042625 \nL 96.483308 -163.682905 \nL 98.012773 -163.244693 \nL 99.542239 -162.727648 \nL 101.071704 -162.131966 \nL 102.60117 -161.458373 \nL 104.130635 -160.70798 \nL 105.660101 -159.882394 \nL 107.189566 -158.983591 \nL 108.719032 -158.013993 \nL 110.248497 -156.976391 \nL 111.777963 -155.874108 \nL 113.307428 -154.710981 \nL 114.836894 -153.491514 \nL 116.366359 -152.220991 \nL 117.895825 -150.905664 \nL 119.425291 -149.553006 \nL 120.954756 -148.172009 \nL 122.484222 -146.773672 \nL 124.013687 -145.371677 \nL 125.543153 -143.983259 \nL 127.072618 -142.630409 \nL 128.602084 -141.341112 \nL 130.131549 -140.15032 \nL 131.661015 -139.099945 \nL 133.19048 -138.236885 \nL 134.719946 -137.60866 \nL 136.249411 -137.257056 \nL 137.778877 -137.211576 \nL 139.308342 -137.484611 \nL 140.837808 -138.069417 \nL 142.367273 -138.940535 \nL 143.896739 -140.055606 \nL 145.426204 -141.357744 \nL 146.95567 -142.778407 \nL 148.485135 -144.241142 \nL 150.014601 -145.666413 \nL 151.544066 -146.977052 \nL 153.073532 -148.103285 \nL 154.602997 -148.986437 \nL 156.132463 -149.580843 \nL 157.661928 -149.854166 \nL 159.191394 -149.786653 \nL 160.720859 -149.369604 \nL 162.250325 -148.603318 \nL 163.77979 -147.494858 \nL 165.309256 -146.056339 \nL 166.838721 -144.304814 \nL 168.368187 -142.264251 \nL 169.897652 -139.969493 \nL 171.427118 -137.470827 \nL 172.956583 -134.837203 \nL 174.486049 -132.156243 \nL 176.015514 -129.529568 \nL 177.54498 -127.064426 \nL 179.074445 -124.864042 \nL 180.603911 -123.019919 \nL 182.133376 -121.60765 \nL 183.662842 -120.686152 \nL 185.192307 -120.298929 \nL 186.721773 -120.476348 \nL 188.251238 -121.237653 \nL 189.780704 -122.592608 \nL 191.310169 -124.542381 \nL 192.839635 -127.079904 \nL 194.3691 -130.189698 \nL 195.898566 -133.847118 \nL 197.428031 -138.017352 \nL 198.957497 -142.654034 \nL 200.486963 -147.697358 \nL 202.016428 -153.072283 \nL 203.545894 -158.686703 \nL 205.075359 -164.430386 \nL 206.604825 -170.174585 \nL 208.13429 -175.77281 \nL 209.663756 -181.061498 \nL 211.193221 -185.859394 \nL 212.722687 -189.964871 \nL 214.252152 -193.153558 \nL 215.781618 -195.186549 \nL 217.311083 -195.843924 \nL 218.840549 -194.988344 \nL 220.370014 -192.635648 \nL 221.89948 -188.985629 \nL 223.428945 -184.369792 \nL 224.958411 -179.138771 \nL 226.487876 -173.573818 \nL 228.017342 -167.864049 \nL 229.546807 -162.125394 \nL 231.076273 -156.428645 \nL 232.605738 -150.821528 \nL 234.135204 -145.342847 \nL 235.664669 -140.029226 \nL 237.194135 -134.916696 \nL 238.7236 -130.038013 \nL 240.253066 -125.41956 \nL 241.782531 -121.077336 \nL 243.311997 -117.015777 \nL 244.841462 -113.228619 \nL 246.370928 -109.701289 \nL 247.900393 -106.414292 \nL 249.429859 -103.345732 \nL 250.959324 -100.473376 \nL 252.48879 -97.776129 \nL 254.018255 -95.234599 \nL 255.547721 -92.831762 \nL 257.077186 -90.552569 \nL 258.606652 -88.384057 \nL 260.136117 -86.315008 \nL 261.665583 -84.336036 \nL 263.195048 -82.438951 \nL 264.724514 -80.616876 \nL 266.253979 -78.863772 \nL 267.783445 -77.17459 \nL 269.31291 -75.544872 \nL 270.842376 -73.970845 \nL 272.371841 -72.449134 \nL 273.901307 -70.976778 \nL 275.430772 -69.551193 \nL 276.960238 -68.170142 \nL 278.489703 -66.831542 \nL 280.019169 -65.533594 \nL 281.548635 -64.274719 \nL 283.0781 -63.053382 \nL 284.607566 -61.86839 \nL 286.137031 -60.718557 \nL 287.666497 -59.602896 \nL 289.195962 -58.520522 \nL 290.725428 -57.470649 \nL 292.254893 -56.452456 \nL 293.784359 -55.465595 \nL 295.313824 -54.509378 \nL 296.84329 -53.583457 \nL 298.372755 -52.68761 \nL 299.902221 -51.821339 \nL 301.431686 -50.984628 \nL 302.961152 -50.177367 \nL 304.490617 -49.399402 \nL 306.020083 -48.650887 \nL 307.549548 -47.931716 \nL 309.079014 -47.242173 \nL 310.608479 -46.582197 \nL 312.137945 -45.952294 \nL 313.66741 -45.352581 \nL 315.196876 -44.783449 \nL 316.726341 -44.24513 \nL 318.255807 -43.738228 \nL 319.785272 -43.263102 \nL 321.314738 -42.820322 \nL 322.844203 -42.410302 \nL 324.373669 -42.033507 \nL 325.903134 -41.691057 \nL 327.4326 -41.383143 \nL 328.962065 -41.11042 \nL 330.491531 -40.873967 \nL 332.020996 -40.67445 \nL 333.550462 -40.512557 \nL 335.079927 -40.389197 \nL 336.609393 -40.305353 \nL 338.138858 -40.261681 \nL 339.668324 -40.259344 \nL 339.668324 11.958855 \nL 339.668324 11.958855 \nL 338.138858 10.573088 \nL 336.609393 9.18454 \nL 335.079927 7.793178 \nL 333.550462 6.398856 \nL 332.020996 5.001457 \nL 330.491531 3.600896 \nL 328.962065 2.196983 \nL 327.4326 0.789602 \nL 325.903134 -0.621396 \nL 324.373669 -2.036105 \nL 322.844203 -3.454833 \nL 321.314738 -4.877642 \nL 319.785272 -6.304723 \nL 318.255807 -7.736362 \nL 316.726341 -9.172749 \nL 315.196876 -10.614105 \nL 313.66741 -12.060675 \nL 312.137945 -13.512775 \nL 310.608479 -14.970681 \nL 309.079014 -16.43472 \nL 307.549548 -17.905198 \nL 306.020083 -19.382592 \nL 304.490617 -20.867124 \nL 302.961152 -22.359396 \nL 301.431686 -23.859789 \nL 299.902221 -25.368811 \nL 298.372755 -26.887064 \nL 296.84329 -28.415035 \nL 295.313824 -29.953412 \nL 293.784359 -31.502923 \nL 292.254893 -33.064192 \nL 290.725428 -34.63813 \nL 289.195962 -36.225518 \nL 287.666497 -37.827309 \nL 286.137031 -39.444506 \nL 284.607566 -41.078189 \nL 283.0781 -42.729542 \nL 281.548635 -44.399769 \nL 280.019169 -46.090263 \nL 278.489703 -47.802497 \nL 276.960238 -49.538085 \nL 275.430772 -51.298702 \nL 273.901307 -53.086331 \nL 272.371841 -54.902961 \nL 270.842376 -56.750859 \nL 269.31291 -58.632589 \nL 267.783445 -60.550901 \nL 266.253979 -62.508899 \nL 264.724514 -64.510114 \nL 263.195048 -66.55856 \nL 261.665583 -68.658819 \nL 260.136117 -70.816223 \nL 258.606652 -73.036998 \nL 257.077186 -75.328394 \nL 255.547721 -77.699114 \nL 254.018255 -80.159441 \nL 252.48879 -82.721667 \nL 250.959324 -85.400317 \nL 249.429859 -88.212822 \nL 247.900393 -91.17959 \nL 246.370928 -94.324212 \nL 244.841462 -97.673232 \nL 243.311997 -101.255161 \nL 241.782531 -105.098738 \nL 240.253066 -109.229866 \nL 238.7236 -113.667814 \nL 237.194135 -118.421716 \nL 235.664669 -123.488162 \nL 234.135204 -128.851947 \nL 232.605738 -134.487666 \nL 231.076273 -140.362191 \nL 229.546807 -146.433963 \nL 228.017342 -152.646722 \nL 226.487876 -158.914176 \nL 224.958411 -165.094413 \nL 223.428945 -170.956639 \nL 221.89948 -176.161581 \nL 220.370014 -180.298369 \nL 218.840549 -183.000134 \nL 217.311083 -184.072875 \nL 215.781618 -183.537274 \nL 214.252152 -181.575993 \nL 212.722687 -178.447777 \nL 211.193221 -174.418258 \nL 209.663756 -169.726489 \nL 208.13429 -164.578995 \nL 206.604825 -159.154885 \nL 205.075359 -153.61128 \nL 203.545894 -148.086595 \nL 202.016428 -142.702128 \nL 200.486963 -137.562724 \nL 198.957497 -132.757706 \nL 197.428031 -128.361704 \nL 195.898566 -124.435769 \nL 194.3691 -121.028332 \nL 192.839635 -118.176345 \nL 191.310169 -115.906083 \nL 189.780704 -114.233826 \nL 188.251238 -113.166427 \nL 186.721773 -112.701496 \nL 185.192307 -112.82713 \nL 183.662842 -113.521261 \nL 182.133376 -114.750276 \nL 180.603911 -116.467341 \nL 179.074445 -118.610499 \nL 177.54498 -121.101713 \nL 176.015514 -123.847857 \nL 174.486049 -126.744835 \nL 172.956583 -129.68494 \nL 171.427118 -132.565973 \nL 169.897652 -135.299245 \nL 168.368187 -137.814506 \nL 166.838721 -140.060775 \nL 165.309256 -142.004093 \nL 163.77979 -143.623294 \nL 162.250325 -144.905524 \nL 160.720859 -145.843222 \nL 159.191394 -146.433265 \nL 157.661928 -146.678072 \nL 156.132463 -146.587659 \nL 154.602997 -146.181675 \nL 153.073532 -145.491004 \nL 151.544066 -144.558583 \nL 150.014601 -143.439242 \nL 148.485135 -142.19811 \nL 146.95567 -140.907407 \nL 145.426204 -139.641961 \nL 143.896739 -138.474244 \nL 142.367273 -137.469816 \nL 140.837808 -136.683619 \nL 139.308342 -136.156992 \nL 137.778877 -135.915093 \nL 136.249411 -135.964915 \nL 134.719946 -136.294771 \nL 133.19048 -136.876288 \nL 131.661015 -137.669244 \nL 130.131549 -138.628109 \nL 128.602084 -139.708338 \nL 127.072618 -140.870657 \nL 125.543153 -142.082929 \nL 124.013687 -143.320067 \nL 122.484222 -144.562969 \nL 120.954756 -145.797194 \nL 119.425291 -147.011675 \nL 117.895825 -148.197798 \nL 116.366359 -149.348663 \nL 114.836894 -150.458581 \nL 113.307428 -151.522756 \nL 111.777963 -152.537034 \nL 110.248497 -153.497733 \nL 108.719032 -154.401533 \nL 107.189566 -155.245378 \nL 105.660101 -156.026474 \nL 104.130635 -156.742213 \nL 102.60117 -157.390241 \nL 101.071704 -157.968454 \nL 99.542239 -158.475132 \nL 98.012773 -158.908886 \nL 96.483308 -159.268818 \nL 94.953842 -159.554558 \nL 93.424377 -159.766256 \nL 91.894911 -159.904687 \nL 90.365446 -159.971142 \nL 88.83598 -159.967496 \nL 87.306515 -159.896135 \nL 85.777049 -159.759925 \nL 84.247584 -159.562095 \nL 82.718118 -159.306256 \nL 81.188653 -158.996275 \nL 79.659187 -158.636195 \nL 78.129722 -158.230211 \nL 76.600256 -157.782563 \nL 75.070791 -157.297467 \nL 73.541325 -156.779091 \nL 72.01186 -156.231488 \nL 70.482394 -155.658552 \nL 68.952929 -155.063934 \nL 67.423463 -154.451028 \nL 65.893998 -153.822925 \nL 64.364532 -153.182324 \nL 62.835067 -152.531574 \nL 61.305601 -151.87264 \nL 59.776136 -151.207038 \nL 58.24667 -150.535967 \nL 56.717205 -149.860261 \nL 55.187739 -149.180416 \nL 53.658274 -148.496701 \nL 52.128808 -147.809172 \nL 50.599343 -147.117695 \nL 49.069877 -146.422059 \nL 47.540412 -145.721929 \nL 46.010946 -145.016994 \nL 44.481481 -144.306863 \nL 42.952015 -143.591254 \nL 41.42255 -142.869851 \nL 39.893084 -142.142436 \nL 38.363619 -141.408839 \nL 36.834153 -140.668969 \nL 35.304688 -139.922835 \nz\n\" style=\"stroke: #3cb371; stroke-opacity: 0.501961\"/>\n    </defs>\n    <g clip-path=\"url(#p0613878dd6)\">\n     <use xlink:href=\"#mfe3291ab18\" x=\"0\" y=\"278.884525\" style=\"fill: #3cb371; fill-opacity: 0.501961; stroke: #3cb371; stroke-opacity: 0.501961\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m26db2d5074\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"35.304688\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −10 -->\n      <g transform=\"translate(24.752344 269.604837) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"86.03196\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −5 -->\n      <g transform=\"translate(78.660866 269.604837) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"136.759233\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <g transform=\"translate(133.577983 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"187.486506\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 5 -->\n      <g transform=\"translate(184.305256 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"238.213778\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <g transform=\"translate(231.851278 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"288.941051\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 15 -->\n      <g transform=\"translate(282.578551 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"339.668324\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 20 -->\n      <g transform=\"translate(333.305824 269.604837) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"m69b353a02d\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"35.304688\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −20 -->\n      <g transform=\"translate(7.2 258.805619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"35.304688\" y=\"199.5664\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- −10 -->\n      <g transform=\"translate(7.2 203.365619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"35.304688\" y=\"144.1264\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(21.942188 147.925619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"35.304688\" y=\"88.6864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(15.579688 92.485619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"35.304688\" y=\"33.2464\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(15.579688 37.045619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 35.304688 145.078899 \nL 47.540412 139.114599 \nL 61.305601 132.649145 \nL 68.952929 129.251532 \nL 73.541325 127.412589 \nL 78.129722 125.841007 \nL 81.188653 124.995919 \nL 84.247584 124.350085 \nL 87.306515 123.931881 \nL 90.365446 123.764172 \nL 93.424377 123.862397 \nL 96.483308 124.233131 \nL 99.542239 124.873797 \nL 102.60117 125.773537 \nL 105.660101 126.91541 \nL 108.719032 128.2792 \nL 111.777963 129.843831 \nL 116.366359 132.520929 \nL 120.954756 135.520521 \nL 130.131549 141.716878 \nL 131.661015 142.562814 \nL 133.19048 143.26332 \nL 134.719946 143.774276 \nL 136.249411 144.056208 \nL 137.778877 144.080862 \nL 139.308342 143.836154 \nL 140.837808 143.328292 \nL 142.367273 142.58132 \nL 143.896739 141.635085 \nL 146.95567 139.366535 \nL 150.014601 137.042506 \nL 151.544066 136.033485 \nL 153.073532 135.210438 \nL 154.602997 134.625049 \nL 156.132463 134.318099 \nL 157.661928 134.319413 \nL 159.191394 134.648638 \nL 160.720859 135.316705 \nL 162.250325 136.327668 \nL 163.77979 137.680473 \nL 165.309256 139.369669 \nL 166.838721 141.384083 \nL 168.368187 143.703404 \nL 169.897652 146.293571 \nL 172.956583 152.059674 \nL 177.54498 160.86199 \nL 179.074445 163.421905 \nL 180.603911 165.631091 \nL 182.133376 167.411204 \nL 183.662842 168.699976 \nL 185.192307 169.450299 \nL 186.721773 169.628345 \nL 188.251238 169.211911 \nL 189.780704 168.189034 \nL 191.310169 166.557329 \nL 192.839635 164.323704 \nL 194.3691 161.504523 \nL 195.898566 158.126152 \nL 197.428031 154.225621 \nL 198.957497 149.851418 \nL 202.016428 139.939816 \nL 209.663756 112.926831 \nL 211.193221 108.220491 \nL 212.722687 104.167412 \nL 214.252152 101.004085 \nL 215.781618 98.993305 \nL 217.311083 98.39143 \nL 218.840549 99.379908 \nL 220.370014 101.979235 \nL 221.89948 105.995205 \nL 223.428945 111.060722 \nL 226.487876 122.776175 \nL 231.076273 140.779429 \nL 234.135204 151.930809 \nL 237.194135 162.040652 \nL 240.253066 170.971045 \nL 243.311997 178.747109 \nL 246.370928 185.532471 \nL 249.429859 191.537552 \nL 252.48879 196.949827 \nL 255.547721 201.912344 \nL 258.606652 206.527441 \nL 263.195048 212.951644 \nL 267.783445 218.91784 \nL 272.371841 224.532588 \nL 278.489703 231.597226 \nL 284.607566 238.2921 \nL 292.254893 246.275114 \nL 299.902221 253.943774 \nL 309.079014 262.849515 \nL 321.314738 274.373125 \nL 327.279862 279.884525 \nL 327.279862 279.884525 \n\" clip-path=\"url(#p0613878dd6)\" style=\"fill: none; stroke: #000000; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 35.304688 174.28693 \nL 38.363619 158.660522 \nL 42.952015 135.022445 \nL 44.481481 127.819794 \nL 46.010946 121.224589 \nL 47.540412 115.371547 \nL 49.069877 110.373499 \nL 50.599343 106.319283 \nL 52.128808 103.272285 \nL 53.658274 101.269653 \nL 55.187739 100.322171 \nL 56.717205 100.414809 \nL 58.24667 101.507889 \nL 59.776136 103.538837 \nL 61.305601 106.424458 \nL 62.835067 110.063668 \nL 64.364532 114.340596 \nL 65.893998 119.127965 \nL 68.952929 129.689425 \nL 73.541325 145.922977 \nL 75.070791 150.915421 \nL 76.600256 155.507969 \nL 78.129722 159.606827 \nL 79.659187 163.134741 \nL 81.188653 166.032396 \nL 82.718118 168.259288 \nL 84.247584 169.794073 \nL 85.777049 170.634386 \nL 87.306515 170.796164 \nL 88.83598 170.312488 \nL 90.365446 169.231995 \nL 91.894911 167.616927 \nL 93.424377 165.540861 \nL 94.953842 163.086197 \nL 98.012773 157.398722 \nL 104.130635 145.453285 \nL 105.660101 142.831578 \nL 107.189566 140.489553 \nL 108.719032 138.474625 \nL 110.248497 136.821238 \nL 111.777963 135.550345 \nL 113.307428 134.669304 \nL 114.836894 134.172199 \nL 116.366359 134.040564 \nL 117.895825 134.24447 \nL 119.425291 134.74396 \nL 120.954756 135.490749 \nL 122.484222 136.430161 \nL 125.543153 138.648866 \nL 128.602084 140.916335 \nL 130.131549 141.925185 \nL 131.661015 142.784615 \nL 133.19048 143.454477 \nL 134.719946 143.903911 \nL 136.249411 144.112406 \nL 137.778877 144.070496 \nL 139.308342 143.78008 \nL 140.837808 143.254362 \nL 142.367273 142.517399 \nL 143.896739 141.603279 \nL 146.95567 139.422773 \nL 150.014601 137.134437 \nL 151.544066 136.099196 \nL 153.073532 135.217607 \nL 154.602997 134.54745 \nL 156.132463 134.141497 \nL 157.661928 134.04546 \nL 159.191394 134.296134 \nL 160.720859 134.919766 \nL 162.250325 135.930744 \nL 163.77979 137.330621 \nL 165.309256 139.107523 \nL 166.838721 141.235966 \nL 168.368187 143.677104 \nL 171.427118 149.279656 \nL 177.54498 161.282986 \nL 179.074445 163.941114 \nL 180.603911 166.279171 \nL 182.133376 168.210345 \nL 183.662842 169.654886 \nL 185.192307 170.542728 \nL 186.721773 170.815926 \nL 188.251238 170.430827 \nL 189.780704 169.35991 \nL 191.310169 167.593232 \nL 192.839635 165.139434 \nL 194.3691 162.026251 \nL 195.898566 158.300522 \nL 197.428031 154.027667 \nL 198.957497 149.290631 \nL 202.016428 138.833577 \nL 206.604825 122.536614 \nL 208.13429 117.483679 \nL 209.663756 112.851757 \nL 211.193221 108.77375 \nL 212.722687 105.373525 \nL 214.252152 102.762386 \nL 215.781618 101.035784 \nL 217.311083 100.27032 \nL 218.840549 100.521166 \nL 220.370014 101.819951 \nL 221.89948 104.173201 \nL 223.428945 107.56137 \nL 224.958411 111.938519 \nL 226.487876 117.232658 \nL 228.017342 123.346754 \nL 229.546807 130.160418 \nL 232.605738 145.302561 \nL 238.7236 176.748579 \nL 240.253066 183.747951 \nL 241.782531 190.027693 \nL 243.311997 195.416911 \nL 244.841462 199.761495 \nL 246.370928 202.928334 \nL 247.900393 204.809123 \nL 249.429859 205.323643 \nL 250.959324 204.422437 \nL 252.48879 202.088785 \nL 254.018255 198.33993 \nL 255.547721 193.227491 \nL 257.077186 186.837058 \nL 258.606652 179.286933 \nL 260.136117 170.726052 \nL 263.195048 151.302982 \nL 270.842376 99.801352 \nL 272.371841 90.942276 \nL 273.901307 83.114795 \nL 275.430772 76.525505 \nL 276.960238 71.356223 \nL 278.489703 67.759127 \nL 280.019169 65.852516 \nL 281.548635 65.717287 \nL 283.0781 67.394252 \nL 284.607566 70.882353 \nL 286.137031 76.13785 \nL 287.666497 83.074505 \nL 289.195962 91.564797 \nL 290.725428 101.442134 \nL 292.254893 112.504041 \nL 295.313824 137.217723 \nL 301.431686 189.0875 \nL 302.961152 200.805645 \nL 304.490617 211.43425 \nL 306.020083 220.706466 \nL 307.549548 228.382344 \nL 309.079014 234.254955 \nL 310.608479 238.155826 \nL 312.137945 239.959559 \nL 313.66741 239.587517 \nL 315.196876 237.010458 \nL 316.726341 232.250061 \nL 318.255807 225.37926 \nL 319.785272 216.521391 \nL 321.314738 205.848126 \nL 322.844203 193.576251 \nL 324.373669 179.963312 \nL 327.4326 149.91514 \nL 332.020996 102.902013 \nL 335.079927 74.461555 \nL 336.609393 62.156933 \nL 338.138858 51.545799 \nL 339.668324 42.899031 \nL 339.668324 42.899031 \n\" clip-path=\"url(#p0613878dd6)\" style=\"fill: none; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 35.304688 155.99173 \nL 38.363619 140.866789 \nL 42.952015 117.980913 \nL 44.481481 111.028996 \nL 46.010946 104.684525 \nL 47.540412 99.082216 \nL 49.069877 94.334902 \nL 50.599343 90.531419 \nL 52.128808 87.735156 \nL 53.658274 85.983257 \nL 55.187739 85.286508 \nL 56.717205 85.629881 \nL 58.24667 86.973695 \nL 59.776136 89.255375 \nL 61.305601 92.39173 \nL 62.835067 96.281674 \nL 64.364532 100.809336 \nL 67.423463 111.260873 \nL 75.070791 139.139297 \nL 76.600256 143.982578 \nL 78.129722 148.33217 \nL 79.659187 152.110818 \nL 81.188653 155.259206 \nL 82.718118 157.736832 \nL 84.247584 159.52235 \nL 85.777049 160.613397 \nL 87.306515 161.025909 \nL 88.83598 160.792966 \nL 90.365446 159.963207 \nL 91.894911 158.598873 \nL 93.424377 156.77354 \nL 94.953842 154.56961 \nL 98.012773 149.383602 \nL 102.60117 141.032319 \nL 104.130635 138.4411 \nL 105.660101 136.070127 \nL 107.189566 133.978836 \nL 108.719032 132.214641 \nL 110.248497 130.811987 \nL 111.777963 129.791828 \nL 113.307428 129.161521 \nL 114.836894 128.91515 \nL 116.366359 129.034248 \nL 117.895825 129.488888 \nL 119.425291 130.239112 \nL 120.954756 131.236634 \nL 124.013687 133.750577 \nL 128.602084 137.915889 \nL 130.131549 139.175473 \nL 131.661015 140.285637 \nL 133.19048 141.206231 \nL 134.719946 141.906399 \nL 136.249411 142.365628 \nL 137.778877 142.24014 \nL 139.308342 141.69899 \nL 140.837808 140.922539 \nL 142.367273 139.934842 \nL 145.426204 137.470933 \nL 151.544066 132.012238 \nL 153.073532 130.879915 \nL 154.602997 129.959024 \nL 156.132463 129.302337 \nL 157.661928 128.955567 \nL 159.191394 128.955506 \nL 160.720859 129.328405 \nL 162.250325 130.08865 \nL 163.77979 131.237793 \nL 165.309256 132.763961 \nL 166.838721 134.641671 \nL 168.368187 136.832075 \nL 171.427118 141.93316 \nL 177.54498 152.933555 \nL 179.074445 155.34095 \nL 180.603911 157.428273 \nL 182.133376 159.108713 \nL 183.662842 160.30252 \nL 185.192307 160.939629 \nL 186.721773 160.962093 \nL 188.251238 160.32626 \nL 189.780704 159.004609 \nL 191.310169 156.987198 \nL 192.839635 154.282666 \nL 194.3691 150.918749 \nL 195.898566 146.942287 \nL 197.428031 142.418698 \nL 200.486963 132.077891 \nL 208.13429 104.119574 \nL 209.663756 99.236919 \nL 211.193221 94.908178 \nL 212.722687 91.257219 \nL 214.252152 88.395347 \nL 215.781618 86.418011 \nL 217.311083 85.401813 \nL 218.840549 85.401926 \nL 220.370014 86.449978 \nL 221.89948 88.552493 \nL 223.428945 91.689929 \nL 224.958411 95.816344 \nL 226.487876 100.859749 \nL 228.017342 106.723112 \nL 229.546807 113.286042 \nL 232.605738 127.926718 \nL 238.7236 158.369801 \nL 240.253066 165.11844 \nL 241.782531 171.147448 \nL 243.311997 176.285932 \nL 244.841462 180.379782 \nL 246.370928 183.295888 \nL 247.900393 184.925943 \nL 249.429859 185.18973 \nL 250.959324 184.03779 \nL 252.48879 181.453405 \nL 254.018255 177.453815 \nL 255.547721 172.090643 \nL 257.077186 165.449476 \nL 258.606652 157.648617 \nL 260.136117 148.837003 \nL 263.195048 128.912465 \nL 270.842376 76.157167 \nL 272.371841 67.047358 \nL 273.901307 58.969143 \nL 275.430772 52.129119 \nL 276.960238 46.709103 \nL 278.489703 42.861274 \nL 280.019169 40.703929 \nL 281.548635 40.317966 \nL 283.0781 41.744198 \nL 284.607566 44.981565 \nL 286.137031 49.986328 \nL 287.666497 56.67225 \nL 289.195962 64.911808 \nL 290.725428 74.538411 \nL 292.254893 85.349585 \nL 295.313824 109.561799 \nL 301.431686 160.428642 \nL 302.961152 171.896053 \nL 304.490617 182.273925 \nL 306.020083 191.295406 \nL 307.549548 198.720551 \nL 309.079014 204.342428 \nL 310.608479 207.992566 \nL 312.137945 209.545565 \nL 313.66741 208.922789 \nL 315.196876 206.094997 \nL 316.726341 201.083866 \nL 318.255807 193.962332 \nL 319.785272 184.853728 \nL 321.314738 173.92973 \nL 322.844203 161.407121 \nL 324.373669 147.543449 \nL 327.4326 116.99381 \nL 332.020996 69.228482 \nL 335.079927 40.286556 \nL 336.609393 27.731201 \nL 338.138858 16.869332 \nL 339.668324 7.971831 \nL 339.668324 7.971831 \n\" clip-path=\"url(#p0613878dd6)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 35.304688 192.58213 \nL 38.363619 176.454254 \nL 42.952015 152.063976 \nL 44.481481 144.610592 \nL 46.010946 137.764654 \nL 47.540412 131.660878 \nL 49.069877 126.412096 \nL 50.599343 122.107146 \nL 52.128808 118.809415 \nL 53.658274 116.556049 \nL 55.187739 115.357833 \nL 56.717205 115.199738 \nL 58.24667 116.042084 \nL 59.776136 117.822298 \nL 61.305601 120.457185 \nL 62.835067 123.845662 \nL 64.364532 127.871857 \nL 65.893998 132.408492 \nL 68.952929 142.468485 \nL 73.541325 157.949835 \nL 75.070791 162.691546 \nL 76.600256 167.03336 \nL 78.129722 170.881484 \nL 79.659187 174.158665 \nL 81.188653 176.805586 \nL 82.718118 178.781745 \nL 84.247584 180.065795 \nL 85.777049 180.655375 \nL 87.306515 180.56642 \nL 88.83598 179.832009 \nL 90.365446 178.500783 \nL 91.894911 176.634982 \nL 93.424377 174.308181 \nL 94.953842 171.602784 \nL 98.012773 165.413841 \nL 104.130635 152.46547 \nL 105.660101 149.59303 \nL 107.189566 147.000271 \nL 108.719032 144.734608 \nL 110.248497 142.830488 \nL 111.777963 141.308861 \nL 113.307428 140.177087 \nL 114.836894 139.429249 \nL 116.366359 139.046879 \nL 117.895825 139.000052 \nL 119.425291 139.248808 \nL 120.954756 139.744863 \nL 122.484222 140.433542 \nL 125.543153 142.150779 \nL 128.602084 143.916782 \nL 130.131549 144.674898 \nL 131.661015 145.283594 \nL 133.19048 145.702722 \nL 134.719946 145.901423 \nL 139.308342 145.861169 \nL 140.837808 145.586185 \nL 142.367273 145.099955 \nL 143.896739 144.436569 \nL 146.95567 142.757531 \nL 150.014601 140.970663 \nL 151.544066 140.186155 \nL 153.073532 139.5553 \nL 154.602997 139.135876 \nL 156.132463 138.980656 \nL 157.661928 139.135354 \nL 159.191394 139.636761 \nL 160.720859 140.511126 \nL 162.250325 141.772839 \nL 163.77979 143.42345 \nL 165.309256 145.451085 \nL 166.838721 147.830262 \nL 168.368187 150.522133 \nL 171.427118 156.626153 \nL 177.54498 169.632417 \nL 179.074445 172.541279 \nL 180.603911 175.13007 \nL 182.133376 177.311977 \nL 183.662842 179.007252 \nL 185.192307 180.145828 \nL 186.721773 180.669759 \nL 188.251238 180.535394 \nL 189.780704 179.71521 \nL 191.310169 178.199267 \nL 192.839635 175.996201 \nL 194.3691 173.133752 \nL 195.898566 169.658757 \nL 197.428031 165.636636 \nL 198.957497 161.150334 \nL 202.016428 151.194747 \nL 206.604825 135.649984 \nL 208.13429 130.847783 \nL 209.663756 126.466595 \nL 211.193221 122.639322 \nL 212.722687 119.48983 \nL 214.252152 117.129425 \nL 215.781618 115.653557 \nL 217.311083 115.138826 \nL 218.840549 115.640406 \nL 220.370014 117.189925 \nL 221.89948 119.793909 \nL 223.428945 123.432811 \nL 224.958411 128.060694 \nL 226.487876 133.605566 \nL 228.017342 139.970397 \nL 229.546807 147.034794 \nL 232.605738 162.678404 \nL 238.7236 195.127357 \nL 240.253066 202.377463 \nL 241.782531 208.907938 \nL 243.311997 214.54789 \nL 244.841462 219.143207 \nL 246.370928 222.56078 \nL 247.900393 224.692303 \nL 249.429859 225.457557 \nL 250.959324 224.807084 \nL 252.48879 222.724166 \nL 254.018255 219.226044 \nL 255.547721 214.36434 \nL 257.077186 208.22464 \nL 258.606652 200.925248 \nL 260.136117 192.615101 \nL 263.195048 173.693499 \nL 270.842376 123.445537 \nL 272.371841 114.837195 \nL 273.901307 107.260447 \nL 275.430772 100.921891 \nL 276.960238 96.003342 \nL 278.489703 92.656981 \nL 280.019169 91.001103 \nL 281.548635 91.116607 \nL 283.0781 93.044306 \nL 284.607566 96.783141 \nL 286.137031 102.289371 \nL 287.666497 109.47676 \nL 289.195962 118.217786 \nL 290.725428 128.345856 \nL 292.254893 139.658497 \nL 295.313824 164.873646 \nL 301.431686 217.746358 \nL 302.961152 229.715237 \nL 304.490617 240.594576 \nL 306.020083 250.117525 \nL 307.549548 258.044137 \nL 309.079014 264.167482 \nL 310.608479 268.319086 \nL 312.137945 270.373553 \nL 313.66741 270.252244 \nL 315.196876 267.92592 \nL 316.726341 263.416256 \nL 318.255807 256.796189 \nL 319.785272 248.189053 \nL 321.314738 237.766522 \nL 322.844203 225.74538 \nL 324.373669 212.383176 \nL 327.4326 182.836471 \nL 332.020996 136.575545 \nL 335.079927 108.636554 \nL 336.609393 96.582666 \nL 338.138858 86.222265 \nL 339.668324 77.826231 \nL 339.668324 77.826231 \n\" clip-path=\"url(#p0613878dd6)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 35.304688 255.0064 \nL 35.304688 33.2464 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 339.668324 255.0064 \nL 339.668324 33.2464 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 35.304688 255.0064 \nL 339.668324 255.0064 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 35.304688 33.2464 \nL 339.668324 33.2464 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_9\">\n     <path d=\"M 63.759943 59.557812 \nL 311.213068 59.557812 \nQ 313.213068 59.557812 313.213068 57.557812 \nL 313.213068 9.2 \nQ 313.213068 7.2 311.213068 7.2 \nL 63.759943 7.2 \nQ 61.759943 7.2 61.759943 9.2 \nL 61.759943 57.557812 \nQ 61.759943 59.557812 63.759943 59.557812 \nz\n\" style=\"fill: #ffffff; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"patch_10\">\n     <path d=\"M 65.759943 20.4 \nL 85.759943 20.4 \nL 85.759943 13.4 \nL 65.759943 13.4 \nz\n\" style=\"fill: #404040; fill-opacity: 0.25098; stroke: #404040; stroke-opacity: 0.25098; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- ood -->\n     <g transform=\"translate(93.759943 20.4) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6f\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"61.181641\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"122.363281\"/>\n     </g>\n    </g>\n    <g id=\"patch_11\">\n     <path d=\"M 65.759943 35.078125 \nL 85.759943 35.078125 \nL 85.759943 28.078125 \nL 65.759943 28.078125 \nz\n\" style=\"fill: #8a2be2; fill-opacity: 0.501961; stroke: #8a2be2; stroke-opacity: 0.501961; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- $\\mathbb{E}[\\sigma]$ (aleatoric) -->\n     <g transform=\"translate(93.759943 35.078125) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"STIXNonUnicode-Italic-e156\" d=\"M 4250 4179 \nL 4192 3942 \nL 2150 3942 \nL 1709 2266 \nL 3514 2266 \nL 3443 2029 \nL 1645 2029 \nL 1165 237 \nL 3443 237 \nL 3392 0 \nL 109 0 \nL 1229 4179 \nL 4250 4179 \nz\nM 1914 3942 \nL 1414 3942 \nL 416 237 \nL 928 237 \nL 1914 3942 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5b\" d=\"M 550 4863 \nL 1875 4863 \nL 1875 4416 \nL 1125 4416 \nL 1125 -397 \nL 1875 -397 \nL 1875 -844 \nL 550 -844 \nL 550 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Oblique-3c3\" d=\"M 2219 3044 \nQ 1744 3044 1422 2700 \nQ 1081 2341 969 1747 \nQ 844 1119 1044 756 \nQ 1241 397 1706 397 \nQ 2166 397 2503 759 \nQ 2844 1122 2966 1747 \nQ 3075 2319 2881 2700 \nQ 2700 3044 2219 3044 \nz\nM 2309 3503 \nL 4219 3500 \nL 4106 2925 \nL 3463 2925 \nQ 3706 2438 3575 1747 \nQ 3406 888 2884 400 \nQ 2359 -91 1609 -91 \nQ 856 -91 525 400 \nQ 194 888 363 1747 \nQ 528 2609 1050 3097 \nQ 1484 3503 2309 3503 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5d\" d=\"M 1947 4863 \nL 1947 -844 \nL 622 -844 \nL 622 -397 \nL 1369 -397 \nL 1369 4416 \nL 622 4416 \nL 622 4863 \nL 1947 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#STIXNonUnicode-Italic-e156\" transform=\"translate(0 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-5b\" transform=\"translate(63.899994 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3c3\" transform=\"translate(102.913666 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-5d\" transform=\"translate(166.292572 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(205.306244 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(237.093353 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(276.107025 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(337.386322 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(365.169525 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(426.692963 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(487.97226 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(527.181244 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(588.362885 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(629.476166 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(657.259369 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(712.239838 0.015625)\"/>\n     </g>\n    </g>\n    <g id=\"patch_12\">\n     <path d=\"M 65.759943 53.457812 \nL 85.759943 53.457812 \nL 85.759943 46.457812 \nL 65.759943 46.457812 \nz\n\" style=\"fill: #3cb371; fill-opacity: 0.501961; stroke: #3cb371; stroke-opacity: 0.501961; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- $\\sqrt{\\mathrm{Var}[\\mu]}$ (epistemic) -->\n     <g transform=\"translate(93.759943 53.457812) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"STIXSizeOneSym-Regular-221a\" d=\"M 6970 9933 \nL 3373 -1888 \nL 3104 -1888 \nL 1626 2918 \nQ 1555 3149 1465 3251 \nQ 1376 3354 1229 3354 \nQ 1011 3354 794 3181 \nL 717 3309 \nL 1766 4115 \nL 1926 4115 \nL 3379 -602 \nL 3405 -602 \nL 6605 9933 \nL 6970 9933 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-56\" d=\"M 1831 0 \nL 50 4666 \nL 709 4666 \nL 2188 738 \nL 3669 4666 \nL 4325 4666 \nL 2547 0 \nL 1831 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Oblique-3bc\" d=\"M -84 -1331 \nL 856 3500 \nL 1434 3500 \nL 1009 1322 \nQ 997 1256 987 1175 \nQ 978 1094 978 1013 \nQ 978 722 1161 565 \nQ 1344 409 1684 409 \nQ 2147 409 2431 671 \nQ 2716 934 2816 1459 \nL 3213 3500 \nL 3788 3500 \nL 3266 809 \nQ 3253 750 3248 706 \nQ 3244 663 3244 628 \nQ 3244 531 3283 486 \nQ 3322 441 3406 441 \nQ 3438 441 3492 456 \nQ 3547 472 3647 513 \nL 3559 50 \nQ 3422 -19 3297 -55 \nQ 3172 -91 3053 -91 \nQ 2847 -91 2730 40 \nQ 2613 172 2613 403 \nQ 2438 153 2195 31 \nQ 1953 -91 1625 -91 \nQ 1334 -91 1117 43 \nQ 900 178 831 397 \nL 494 -1331 \nL -84 -1331 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#STIXSizeOneSym-Regular-221a\" transform=\"translate(0 -0.09375) scale(0.693173)\"/>\n      <use xlink:href=\"#DejaVuSans-56\" transform=\"translate(85.829376 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(154.237579 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(215.516876 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-5b\" transform=\"translate(256.630157 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(295.643829 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-5d\" transform=\"translate(359.266876 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(410.780548 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(442.567657 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(481.581329 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(543.104767 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(606.581329 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(634.364532 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(686.464142 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(725.673126 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" transform=\"translate(787.196564 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(884.608673 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(912.391876 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(967.372345 0.34375)\"/>\n      <path d=\"M 73.329376 100.5 \nL 73.329376 106.75 \nL 410.780548 106.75 \nL 410.780548 100.5 \nL 73.329376 100.5 \nz\n\"/>\n     </g>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 214.459943 16.9 \nL 224.459943 16.9 \nL 234.459943 16.9 \n\" style=\"fill: none; stroke: #000000; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- $\\hat \\mu$ -->\n     <g transform=\"translate(242.459943 20.4) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-302\" d=\"M -1831 5119 \nL -1369 5119 \nL -603 3944 \nL -1038 3944 \nL -1600 4709 \nL -2163 3944 \nL -2597 3944 \nL -1831 5119 \nz\nM -1600 3584 \nL -1600 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-302\" transform=\"translate(62.699219 12.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(0 0.828125)\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 214.459943 31.598437 \nL 224.459943 31.598437 \nL 234.459943 31.598437 \n\" style=\"fill: none; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- true mean -->\n     <g transform=\"translate(242.459943 35.098437) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"143.701172\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"205.224609\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"237.011719\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"334.423828\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"395.947266\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"457.226562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 214.459943 46.276562 \nL 224.459943 46.276562 \nL 234.459943 46.276562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- true variance -->\n     <g transform=\"translate(242.459943 49.776562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"143.701172\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"205.224609\"/>\n      <use xlink:href=\"#DejaVuSans-76\" x=\"237.011719\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"296.191406\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"357.470703\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"398.583984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"426.367188\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"487.646484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"551.025391\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"606.005859\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_13\">\n    <path d=\"M 400.541051 255.0064 \nL 704.904687 255.0064 \nL 704.904687 33.2464 \nL 400.541051 33.2464 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 400.541051 255.0064 \nL 704.904687 255.0064 \nL 704.904687 33.2464 \nL 400.541051 33.2464 \nz\n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: #404040; fill-opacity: 0.25098; stroke: #404040; stroke-opacity: 0.25098; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 481.704687 255.0064 \nL 603.450142 255.0064 \nL 603.450142 33.2464 \nL 481.704687 33.2464 \nz\n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: #ffffff; stroke: #ffffff; stroke-linejoin: miter\"/>\n   </g>\n   <g id=\"PathCollection_2\">\n    <g clip-path=\"url(#p4130a5dda9)\">\n     <use xlink:href=\"#m41a03e1542\" x=\"600.324672\" y=\"177.799544\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"570.840162\" y=\"135.882772\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"588.377814\" y=\"121.809961\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"578.264658\" y=\"88.664847\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"549.626958\" y=\"184.57282\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"566.205147\" y=\"127.796051\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"556.473739\" y=\"166.227107\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"530.032989\" y=\"136.952034\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"550.398708\" y=\"175.91581\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"543.402613\" y=\"153.866521\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"566.51581\" y=\"148.20986\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"581.810618\" y=\"86.387737\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"521.887049\" y=\"143.615647\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"510.115466\" y=\"140.793173\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"600.673037\" y=\"153.561591\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"519.628486\" y=\"132.090114\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"543.135353\" y=\"165.400406\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"507.627311\" y=\"142.779375\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"585.554666\" y=\"92.512261\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"540.983247\" y=\"163.859978\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"560.058891\" y=\"165.901343\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"560.590192\" y=\"162.445155\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"541.590216\" y=\"155.732049\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"556.118526\" y=\"153.770054\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"539.643877\" y=\"154.911158\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"493.983622\" y=\"137.416651\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"577.922627\" y=\"93.887042\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"548.338132\" y=\"168.966726\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"504.51653\" y=\"142.172599\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"517.908237\" y=\"137.027815\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"567.878296\" y=\"134.249615\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"527.255874\" y=\"139.685339\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"511.649422\" y=\"142.677954\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"543.908853\" y=\"161.094579\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"543.240444\" y=\"164.596222\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"595.685572\" y=\"150.36577\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"562.610389\" y=\"152.151454\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"501.082228\" y=\"143.529436\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"538.559477\" y=\"152.668935\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"500.245323\" y=\"142.051445\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"559.534678\" y=\"160.103907\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"598.951457\" y=\"154.601368\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"559.112957\" y=\"170.601995\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"566.054869\" y=\"141.932262\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"599.99558\" y=\"160.913714\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"550.418372\" y=\"160.453233\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"492.230613\" y=\"139.367766\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"501.517783\" y=\"145.120393\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"504.723139\" y=\"143.38549\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"523.605692\" y=\"137.772075\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"529.450922\" y=\"126.935728\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"549.712468\" y=\"170.414792\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"513.775364\" y=\"136.4782\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"599.845343\" y=\"142.257889\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"517.212667\" y=\"137.850764\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"510.571782\" y=\"138.899296\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"563.086936\" y=\"150.165853\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"582.044685\" y=\"97.574317\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"546.130403\" y=\"164.791312\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"522.703483\" y=\"128.064825\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"541.080614\" y=\"153.67569\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"542.681758\" y=\"165.639311\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"580.298422\" y=\"107.806737\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"580.567238\" y=\"91.326311\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"507.082019\" y=\"141.626084\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"548.865463\" y=\"162.930343\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"500.550651\" y=\"143.127123\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"537.810084\" y=\"144.029376\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"527.55548\" y=\"136.755038\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"521.868808\" y=\"129.853644\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"555.992821\" y=\"153.320423\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"525.792004\" y=\"132.904354\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"508.476903\" y=\"143.58662\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"559.435127\" y=\"161.87052\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"581.558573\" y=\"103.917346\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"559.270106\" y=\"152.916656\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"533.886142\" y=\"135.904864\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"564.353477\" y=\"138.431976\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"602.838395\" y=\"177.839854\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"501.481298\" y=\"141.915328\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"562.192306\" y=\"170.198562\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"541.937065\" y=\"168.684177\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"572.899676\" y=\"137.448512\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"575.63211\" y=\"119.154698\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"505.533429\" y=\"143.151004\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"564.133599\" y=\"153.271731\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"555.44837\" y=\"170.479905\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"526.2153\" y=\"133.834452\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"526.18105\" y=\"135.160925\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"529.031571\" y=\"133.728128\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"553.958249\" y=\"176.106721\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"490.666731\" y=\"140.559968\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"518.480846\" y=\"130.487893\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"509.984477\" y=\"144.416367\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"575.557205\" y=\"111.645964\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"524.400216\" y=\"136.264952\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"526.330532\" y=\"140.677016\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"600.459946\" y=\"152.88063\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"496.005276\" y=\"139.966693\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n     <use xlink:href=\"#m41a03e1542\" x=\"527.074127\" y=\"129.886301\" style=\"fill: #0000ff; stroke: #0000ff; stroke-width: 1.5\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_4\">\n    <defs>\n     <path id=\"m0e4850d9fd\" d=\"M 400.541051 -179.195814 \nL 400.541051 -157.614579 \nL 402.070517 -157.183505 \nL 403.599982 -156.756257 \nL 405.129448 -156.332873 \nL 406.658913 -155.913233 \nL 408.188379 -155.497252 \nL 409.717844 -155.084876 \nL 411.24731 -154.675943 \nL 412.776775 -154.270298 \nL 414.306241 -153.867786 \nL 415.835706 -153.468197 \nL 417.365172 -153.071371 \nL 418.894637 -152.677104 \nL 420.424103 -152.285235 \nL 421.953568 -151.895658 \nL 423.483034 -151.508306 \nL 425.012499 -151.12335 \nL 426.541965 -150.740921 \nL 428.07143 -150.361551 \nL 429.600896 -149.985945 \nL 431.130361 -149.615169 \nL 432.659827 -149.250679 \nL 434.189292 -148.894426 \nL 435.718758 -148.548763 \nL 437.248223 -148.216575 \nL 438.777689 -147.901084 \nL 440.307154 -147.60579 \nL 441.83662 -147.334096 \nL 443.366085 -147.089057 \nL 444.895551 -146.87287 \nL 446.425016 -146.686447 \nL 447.954482 -146.52904 \nL 449.483947 -146.39801 \nL 451.013413 -146.288939 \nL 452.542878 -146.195945 \nL 454.072344 -146.112323 \nL 455.601809 -146.031391 \nL 457.131275 -145.947225 \nL 458.66074 -145.855231 \nL 460.190206 -145.75237 \nL 461.719672 -145.637075 \nL 463.249137 -145.508968 \nL 464.778603 -145.368349 \nL 466.308068 -145.215737 \nL 467.837534 -145.051391 \nL 469.366999 -144.874927 \nL 470.896465 -144.684888 \nL 472.42593 -144.478534 \nL 473.955396 -144.251619 \nL 475.484861 -143.998346 \nL 477.014327 -143.711415 \nL 478.543792 -143.382268 \nL 480.073258 -143.001352 \nL 481.602723 -142.558594 \nL 483.132189 -142.044037 \nL 484.661654 -141.448796 \nL 486.19112 -140.76653 \nL 487.720585 -139.995503 \nL 489.250051 -139.141184 \nL 490.779516 -138.218871 \nL 492.308982 -137.255519 \nL 493.838447 -136.289637 \nL 495.367913 -135.368728 \nL 496.897378 -134.544435 \nL 498.426844 -133.866733 \nL 499.956309 -133.37866 \nL 501.485775 -133.112632 \nL 503.01524 -133.088524 \nL 504.544706 -133.312938 \nL 506.074171 -133.778821 \nL 507.603637 -134.464879 \nL 509.133102 -135.334835 \nL 510.662568 -136.337357 \nL 512.192033 -137.407753 \nL 513.721499 -138.472126 \nL 515.250964 -139.453607 \nL 516.78043 -140.27937 \nL 518.309895 -140.886731 \nL 519.839361 -141.226891 \nL 521.368826 -141.265595 \nL 522.898292 -140.980965 \nL 524.427757 -140.360164 \nL 525.957223 -139.396543 \nL 527.486688 -138.088403 \nL 529.016154 -136.439385 \nL 530.545619 -134.460142 \nL 532.075085 -132.17023 \nL 533.60455 -129.599484 \nL 535.134016 -126.788263 \nL 536.663481 -123.78651 \nL 538.192947 -120.652076 \nL 539.722412 -117.44924 \nL 541.251878 -114.248411 \nL 542.781344 -111.127323 \nL 544.310809 -108.173053 \nL 545.840275 -105.482357 \nL 547.36974 -103.158693 \nL 548.899206 -101.30477 \nL 550.428671 -100.013268 \nL 551.958137 -99.359118 \nL 553.487602 -99.395861 \nL 555.017068 -100.155908 \nL 556.546533 -101.652272 \nL 558.075999 -103.879035 \nL 559.605464 -106.809217 \nL 561.13493 -110.389697 \nL 562.664395 -114.535883 \nL 564.193861 -119.12986 \nL 565.723326 -124.02526 \nL 567.252792 -129.059434 \nL 568.782257 -134.066559 \nL 570.311723 -138.88671 \nL 571.841188 -143.369847 \nL 573.370654 -147.379556 \nL 574.900119 -150.801113 \nL 576.429585 -153.553003 \nL 577.95905 -155.599054 \nL 579.488516 -156.957904 \nL 581.017981 -157.705618 \nL 582.547447 -157.964045 \nL 584.076912 -157.873666 \nL 585.606378 -157.562936 \nL 587.135843 -157.129412 \nL 588.665309 -156.637298 \nL 590.194774 -156.12476 \nL 591.72424 -155.613169 \nL 593.253705 -155.114335 \nL 594.783171 -154.634764 \nL 596.312636 -154.178201 \nL 597.842102 -153.74669 \nL 599.371567 -153.341322 \nL 600.901033 -152.962552 \nL 602.430498 -152.610367 \nL 603.959964 -152.284483 \nL 605.489429 -151.984362 \nL 607.018895 -151.709382 \nL 608.54836 -151.458791 \nL 610.077826 -151.231791 \nL 611.607291 -151.02759 \nL 613.136757 -150.845289 \nL 614.666222 -150.684162 \nL 616.195688 -150.54333 \nL 617.725153 -150.421987 \nL 619.254619 -150.319379 \nL 620.784084 -150.234758 \nL 622.31355 -150.167357 \nL 623.843016 -150.116499 \nL 625.372481 -150.081443 \nL 626.901947 -150.061567 \nL 628.431412 -150.05619 \nL 629.960878 -150.064711 \nL 631.490343 -150.08649 \nL 633.019809 -150.12099 \nL 634.549274 -150.167631 \nL 636.07874 -150.225872 \nL 637.608205 -150.2952 \nL 639.137671 -150.375131 \nL 640.667136 -150.465209 \nL 642.196602 -150.564951 \nL 643.726067 -150.673906 \nL 645.255533 -150.791734 \nL 646.784998 -150.918002 \nL 648.314464 -151.052293 \nL 649.843929 -151.19432 \nL 651.373395 -151.343675 \nL 652.90286 -151.500098 \nL 654.432326 -151.663244 \nL 655.961791 -151.832841 \nL 657.491257 -152.008602 \nL 659.020722 -152.190219 \nL 660.550188 -152.377491 \nL 662.079653 -152.570161 \nL 663.609119 -152.768033 \nL 665.138584 -152.970796 \nL 666.66805 -153.178346 \nL 668.197515 -153.390444 \nL 669.726981 -153.606914 \nL 671.256446 -153.827537 \nL 672.785912 -154.052205 \nL 674.315377 -154.280727 \nL 675.844843 -154.512947 \nL 677.374308 -154.74876 \nL 678.903774 -154.987939 \nL 680.433239 -155.230411 \nL 681.962705 -155.476029 \nL 683.49217 -155.724711 \nL 685.021636 -155.976322 \nL 686.551101 -156.230744 \nL 688.080567 -156.487875 \nL 689.610032 -156.747636 \nL 691.139498 -157.00989 \nL 692.668963 -157.274592 \nL 694.198429 -157.541594 \nL 695.727894 -157.810904 \nL 697.25736 -158.082369 \nL 698.786825 -158.355919 \nL 700.316291 -158.631563 \nL 701.845756 -158.909089 \nL 703.375222 -159.188569 \nL 704.904687 -159.469863 \nL 704.904687 -216.185107 \nL 704.904687 -216.185107 \nL 703.375222 -215.677047 \nL 701.845756 -215.169605 \nL 700.316291 -214.662899 \nL 698.786825 -214.156864 \nL 697.25736 -213.651653 \nL 695.727894 -213.147257 \nL 694.198429 -212.643723 \nL 692.668963 -212.141156 \nL 691.139498 -211.639526 \nL 689.610032 -211.138979 \nL 688.080567 -210.6395 \nL 686.551101 -210.141215 \nL 685.021636 -209.644169 \nL 683.49217 -209.148428 \nL 681.962705 -208.654072 \nL 680.433239 -208.161223 \nL 678.903774 -207.669897 \nL 677.374308 -207.180231 \nL 675.844843 -206.692246 \nL 674.315377 -206.206133 \nL 672.785912 -205.721955 \nL 671.256446 -205.239823 \nL 669.726981 -204.759896 \nL 668.197515 -204.282216 \nL 666.66805 -203.806974 \nL 665.138584 -203.33428 \nL 663.609119 -202.864324 \nL 662.079653 -202.39716 \nL 660.550188 -201.933042 \nL 659.020722 -201.472085 \nL 657.491257 -201.014523 \nL 655.961791 -200.560471 \nL 654.432326 -200.110163 \nL 652.90286 -199.66382 \nL 651.373395 -199.221644 \nL 649.843929 -198.783913 \nL 648.314464 -198.350773 \nL 646.784998 -197.92257 \nL 645.255533 -197.499506 \nL 643.726067 -197.081894 \nL 642.196602 -196.670076 \nL 640.667136 -196.26427 \nL 639.137671 -195.864851 \nL 637.608205 -195.472152 \nL 636.07874 -195.086537 \nL 634.549274 -194.708356 \nL 633.019809 -194.337979 \nL 631.490343 -193.975824 \nL 629.960878 -193.622319 \nL 628.431412 -193.277838 \nL 626.901947 -192.942864 \nL 625.372481 -192.617825 \nL 623.843016 -192.303222 \nL 622.31355 -191.999485 \nL 620.784084 -191.707147 \nL 619.254619 -191.426678 \nL 617.725153 -191.158603 \nL 616.195688 -190.903434 \nL 614.666222 -190.661678 \nL 613.136757 -190.433848 \nL 611.607291 -190.220543 \nL 610.077826 -190.022179 \nL 608.54836 -189.839316 \nL 607.018895 -189.67239 \nL 605.489429 -189.521827 \nL 603.959964 -189.387961 \nL 602.430498 -189.271004 \nL 600.901033 -189.171045 \nL 599.371567 -189.087899 \nL 597.842102 -189.021111 \nL 596.312636 -188.969767 \nL 594.783171 -188.932345 \nL 593.253705 -188.906411 \nL 591.72424 -188.887911 \nL 590.194774 -188.870168 \nL 588.665309 -188.841342 \nL 587.135843 -188.780265 \nL 585.606378 -188.649248 \nL 584.076912 -188.385086 \nL 582.547447 -187.892067 \nL 581.017981 -187.044899 \nL 579.488516 -185.707742 \nL 577.95905 -183.764262 \nL 576.429585 -181.14301 \nL 574.900119 -177.826633 \nL 573.370654 -173.847318 \nL 571.841188 -169.277989 \nL 570.311723 -164.223737 \nL 568.782257 -158.815475 \nL 567.252792 -153.205388 \nL 565.723326 -147.563005 \nL 564.193861 -142.067345 \nL 562.664395 -136.892051 \nL 561.13493 -132.187526 \nL 559.605464 -128.067213 \nL 558.075999 -124.60455 \nL 556.546533 -121.83853 \nL 555.017068 -119.782709 \nL 553.487602 -118.432858 \nL 551.958137 -117.770466 \nL 550.428671 -117.762965 \nL 548.899206 -118.36213 \nL 547.36974 -119.503377 \nL 545.840275 -121.107782 \nL 544.310809 -123.087098 \nL 542.781344 -125.349999 \nL 541.251878 -127.807444 \nL 539.722412 -130.375413 \nL 538.192947 -132.975669 \nL 536.663481 -135.535421 \nL 535.134016 -137.987441 \nL 533.60455 -140.271097 \nL 532.075085 -142.333872 \nL 530.545619 -144.132693 \nL 529.016154 -145.634396 \nL 527.486688 -146.815278 \nL 525.957223 -147.660046 \nL 524.427757 -148.16081 \nL 522.898292 -148.316673 \nL 521.368826 -148.134486 \nL 519.839361 -147.630669 \nL 518.309895 -146.83373 \nL 516.78043 -145.786498 \nL 515.250964 -144.546825 \nL 513.721499 -143.185734 \nL 512.192033 -141.782864 \nL 510.662568 -140.419973 \nL 509.133102 -139.173762 \nL 507.603637 -138.109783 \nL 506.074171 -137.27871 \nL 504.544706 -136.715286 \nL 503.01524 -136.43903 \nL 501.485775 -136.455442 \nL 499.956309 -136.756799 \nL 498.426844 -137.32247 \nL 496.897378 -138.119338 \nL 495.367913 -139.10324 \nL 493.838447 -140.222118 \nL 492.308982 -141.42095 \nL 490.779516 -142.647588 \nL 489.250051 -143.858076 \nL 487.720585 -145.019936 \nL 486.19112 -146.11298 \nL 484.661654 -147.127974 \nL 483.132189 -148.064182 \nL 481.602723 -148.926666 \nL 480.073258 -149.724003 \nL 478.543792 -150.46646 \nL 477.014327 -151.164709 \nL 475.484861 -151.828868 \nL 473.955396 -152.467771 \nL 472.42593 -153.08851 \nL 470.896465 -153.696085 \nL 469.366999 -154.293358 \nL 467.837534 -154.881193 \nL 466.308068 -155.458839 \nL 464.778603 -156.024354 \nL 463.249137 -156.575272 \nL 461.719672 -157.109146 \nL 460.190206 -157.624175 \nL 458.66074 -158.119646 \nL 457.131275 -158.596385 \nL 455.601809 -159.056897 \nL 454.072344 -159.5053 \nL 452.542878 -159.947016 \nL 451.013413 -160.388186 \nL 449.483947 -160.835013 \nL 447.954482 -161.293053 \nL 446.425016 -161.766636 \nL 444.895551 -162.258623 \nL 443.366085 -162.770281 \nL 441.83662 -163.301433 \nL 440.307154 -163.850735 \nL 438.777689 -164.416018 \nL 437.248223 -164.994689 \nL 435.718758 -165.583962 \nL 434.189292 -166.181159 \nL 432.659827 -166.783809 \nL 431.130361 -167.389816 \nL 429.600896 -167.9974 \nL 428.07143 -168.605154 \nL 426.541965 -169.212005 \nL 425.012499 -169.81722 \nL 423.483034 -170.420187 \nL 421.953568 -171.020707 \nL 420.424103 -171.618524 \nL 418.894637 -172.213643 \nL 417.365172 -172.806105 \nL 415.835706 -173.396034 \nL 414.306241 -173.983613 \nL 412.776775 -174.568988 \nL 411.24731 -175.152387 \nL 409.717844 -175.733989 \nL 408.188379 -176.313978 \nL 406.658913 -176.892577 \nL 405.129448 -177.469907 \nL 403.599982 -178.046106 \nL 402.070517 -178.6214 \nL 400.541051 -179.195814 \nz\n\" style=\"stroke: #8a2be2; stroke-opacity: 0.501961\"/>\n    </defs>\n    <g clip-path=\"url(#p4130a5dda9)\">\n     <use xlink:href=\"#m0e4850d9fd\" x=\"0\" y=\"278.884525\" style=\"fill: #8a2be2; fill-opacity: 0.501961; stroke: #8a2be2; stroke-opacity: 0.501961\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_5\">\n    <path d=\"M 400.541051 121.269946 \nL 400.541051 11113.390595 \nL 402.070517 9734.145315 \nL 403.599982 8524.118588 \nL 405.129448 7463.530815 \nL 406.658913 6534.604244 \nL 408.188379 5721.503419 \nL 409.717844 5010.197458 \nL 411.24731 4388.202888 \nL 412.776775 3844.556908 \nL 414.306241 3369.558872 \nL 415.835706 2954.684846 \nL 417.365172 2592.452764 \nL 418.894637 2276.267115 \nL 420.424103 2000.35397 \nL 421.953568 1759.651858 \nL 423.483034 1549.717185 \nL 425.012499 1366.665279 \nL 426.541965 1207.081641 \nL 428.07143 1067.986424 \nL 429.600896 946.768864 \nL 431.130361 841.143296 \nL 432.659827 749.112904 \nL 434.189292 668.92624 \nL 435.718758 599.058525 \nL 437.248223 538.169525 \nL 438.777689 485.092847 \nL 440.307154 438.805764 \nL 441.83662 398.418381 \nL 443.366085 363.156549 \nL 444.895551 332.349878 \nL 446.425016 305.421939 \nL 447.954482 281.879829 \nL 449.483947 261.305048 \nL 451.013413 243.342395 \nL 452.542878 227.68805 \nL 454.072344 214.079069 \nL 455.601809 202.283969 \nL 457.131275 192.095184 \nL 458.66074 183.325296 \nL 460.190206 175.804904 \nL 461.719672 169.381162 \nL 463.249137 163.917211 \nL 464.778603 159.290971 \nL 466.308068 155.393981 \nL 467.837534 152.129963 \nL 469.366999 149.413929 \nL 470.896465 147.171354 \nL 472.42593 145.337548 \nL 473.955396 143.8573 \nL 475.484861 142.684417 \nL 477.014327 141.781174 \nL 478.543792 141.117564 \nL 480.073258 140.670438 \nL 481.602723 140.422389 \nL 483.132189 140.360454 \nL 484.661654 140.474515 \nL 486.19112 140.755266 \nL 487.720585 141.191667 \nL 489.250051 141.767893 \nL 490.779516 142.460322 \nL 492.308982 143.235302 \nL 493.838447 144.048868 \nL 495.367913 144.848999 \nL 496.897378 145.580253 \nL 498.426844 146.189499 \nL 499.956309 146.631188 \nL 501.485775 146.871053 \nL 503.01524 146.888044 \nL 504.544706 146.67501 \nL 506.074171 146.238975 \nL 507.603637 145.601536 \nL 509.133102 144.799304 \nL 510.662568 143.883588 \nL 512.192033 142.918251 \nL 513.721499 141.975161 \nL 515.250964 141.127714 \nL 516.78043 140.44382 \nL 518.309895 139.980133 \nL 519.839361 139.779004 \nL 521.368826 139.868771 \nL 522.898292 140.266883 \nL 524.427757 140.983847 \nL 525.957223 142.026189 \nL 527.486688 143.397334 \nL 529.016154 145.096572 \nL 530.545619 147.116744 \nL 532.075085 149.441851 \nL 533.60455 152.045402 \nL 535.134016 154.890057 \nL 536.663481 157.928569 \nL 538.192947 161.105443 \nL 539.722412 164.358358 \nL 541.251878 167.618311 \nL 542.781344 170.808206 \nL 544.310809 173.840594 \nL 545.840275 176.61726 \nL 547.36974 179.032415 \nL 548.899206 180.980727 \nL 550.428671 182.367295 \nL 551.958137 183.115993 \nL 553.487602 183.173396 \nL 555.017068 182.508566 \nL 556.546533 181.111225 \nL 558.075999 178.991064 \nL 559.605464 176.179522 \nL 561.13493 172.734275 \nL 562.664395 168.743661 \nL 564.193861 164.327556 \nL 565.723326 159.631857 \nL 567.252792 154.816623 \nL 568.782257 150.044404 \nL 570.311723 145.473464 \nL 571.841188 141.256002 \nL 573.370654 137.535776 \nL 574.900119 134.440193 \nL 576.429585 132.068208 \nL 577.95905 130.477776 \nL 579.488516 129.676587 \nL 581.017981 129.620472 \nL 582.547447 130.226799 \nL 584.076912 131.404799 \nL 585.606378 133.090809 \nL 587.135843 135.272319 \nL 588.665309 137.994223 \nL 590.194774 141.352885 \nL 591.72424 145.485679 \nL 593.253705 150.562079 \nL 594.783171 156.779989 \nL 596.312636 164.366709 \nL 597.842102 173.584322 \nL 599.371567 184.736915 \nL 600.901033 198.178676 \nL 602.430498 214.321455 \nL 603.959964 233.645058 \nL 605.489429 256.704597 \nL 607.018895 284.141458 \nL 608.54836 316.692885 \nL 610.077826 355.204899 \nL 611.607291 400.644893 \nL 613.136757 454.116521 \nL 614.666222 516.871914 \nL 616.195688 590.334059 \nL 617.725153 676.116242 \nL 619.254619 776.036361 \nL 620.784084 892.148333 \nL 622.31355 1026.765782 \nL 623.843016 1182.485386 \nL 625.372481 1362.233784 \nL 626.901947 1569.278531 \nL 628.431412 1807.291039 \nL 629.960878 2080.380074 \nL 631.490343 2393.121702 \nL 633.019809 2750.656746 \nL 634.549274 3158.691289 \nL 636.07874 3623.592775 \nL 637.608205 4152.460733 \nL 639.137671 4753.188527 \nL 640.667136 5434.54897 \nL 642.196602 6206.294817 \nL 643.726067 7079.244719 \nL 645.255533 8065.361386 \nL 646.784998 9177.893029 \nL 648.314464 10431.576048 \nL 649.843929 11842.538937 \nL 651.373395 13428.568244 \nL 652.90286 15209.295359 \nL 654.432326 17206.201929 \nL 655.961791 19442.770097 \nL 657.491257 21944.510925 \nL 659.020722 24739.045607 \nL 660.550188 27856.211041 \nL 662.079653 31327.856802 \nL 663.609119 35187.498646 \nL 665.138584 39470.365877 \nL 666.66805 44212.280638 \nL 668.197515 49449.798681 \nL 669.726981 55217.843416 \nL 671.256446 61549.275494 \nL 672.785912 68472.361736 \nL 674.315377 76008.763806 \nL 675.844843 84170.078627 \nL 677.374308 92954.584525 \nL 678.903774 102345.037713 \nL 680.433239 112302.992931 \nL 681.962705 122768.159181 \nL 683.49217 133655.741416 \nL 685.021636 144856.722072 \nL 686.551101 156240.708869 \nL 688.080567 167661.868619 \nL 689.610032 178964.503713 \nL 691.139498 189994.291713 \nL 692.668963 200608.149775 \nL 694.198429 210682.680588 \nL 695.727894 220120.950775 \nL 697.25736 228857.082025 \nL 698.786825 236855.731338 \nL 700.316291 244111.094838 \nL 701.845756 250641.753588 \nL 703.375222 256484.523213 \nL 704.904687 261689.841119 \nL 704.904687 119.414662 \nL 704.904687 119.414662 \nL 703.375222 119.695956 \nL 701.845756 119.975436 \nL 700.316291 120.252962 \nL 698.786825 120.528606 \nL 697.25736 120.802156 \nL 695.727894 121.073621 \nL 694.198429 121.342931 \nL 692.668963 121.609933 \nL 691.139498 121.874635 \nL 689.610032 122.136889 \nL 688.080567 122.39665 \nL 686.551101 122.653781 \nL 685.021636 122.908203 \nL 683.49217 123.159814 \nL 681.962705 123.408496 \nL 680.433239 123.654114 \nL 678.903774 123.896586 \nL 677.374308 124.135765 \nL 675.844843 124.371578 \nL 674.315377 124.603798 \nL 672.785912 124.83232 \nL 671.256446 125.056988 \nL 669.726981 125.277611 \nL 668.197515 125.494081 \nL 666.66805 125.706179 \nL 665.138584 125.913729 \nL 663.609119 126.116492 \nL 662.079653 126.314364 \nL 660.550188 126.507034 \nL 659.020722 126.694306 \nL 657.491257 126.875923 \nL 655.961791 127.051684 \nL 654.432326 127.221281 \nL 652.90286 127.384427 \nL 651.373395 127.54085 \nL 649.843929 127.690205 \nL 648.314464 127.832232 \nL 646.784998 127.966523 \nL 645.255533 128.092791 \nL 643.726067 128.210619 \nL 642.196602 128.319574 \nL 640.667136 128.419316 \nL 639.137671 128.509394 \nL 637.608205 128.589325 \nL 636.07874 128.658653 \nL 634.549274 128.716894 \nL 633.019809 128.763535 \nL 631.490343 128.798035 \nL 629.960878 128.819814 \nL 628.431412 128.828335 \nL 626.901947 128.822958 \nL 625.372481 128.803082 \nL 623.843016 128.768026 \nL 622.31355 128.717168 \nL 620.784084 128.649767 \nL 619.254619 128.565146 \nL 617.725153 128.462538 \nL 616.195688 128.341195 \nL 614.666222 128.200363 \nL 613.136757 128.039236 \nL 611.607291 127.856935 \nL 610.077826 127.652734 \nL 608.54836 127.425734 \nL 607.018895 127.175143 \nL 605.489429 126.900163 \nL 603.959964 126.600042 \nL 602.430498 126.274158 \nL 600.901033 125.921973 \nL 599.371567 125.543203 \nL 597.842102 125.137835 \nL 596.312636 124.706324 \nL 594.783171 124.249761 \nL 593.253705 123.77019 \nL 591.72424 123.271356 \nL 590.194774 122.759765 \nL 588.665309 122.247227 \nL 587.135843 121.755113 \nL 585.606378 121.321589 \nL 584.076912 121.010859 \nL 582.547447 120.92048 \nL 581.017981 121.178907 \nL 579.488516 121.926621 \nL 577.95905 123.285471 \nL 576.429585 125.331522 \nL 574.900119 128.083412 \nL 573.370654 131.504969 \nL 571.841188 135.514678 \nL 570.311723 139.997815 \nL 568.782257 144.817966 \nL 567.252792 149.825091 \nL 565.723326 154.859265 \nL 564.193861 159.754665 \nL 562.664395 164.348642 \nL 561.13493 168.494828 \nL 559.605464 172.075308 \nL 558.075999 175.00549 \nL 556.546533 177.232253 \nL 555.017068 178.728617 \nL 553.487602 179.488664 \nL 551.958137 179.525407 \nL 550.428671 178.871257 \nL 548.899206 177.579755 \nL 547.36974 175.725832 \nL 545.840275 173.402168 \nL 544.310809 170.711472 \nL 542.781344 167.757202 \nL 541.251878 164.636114 \nL 539.722412 161.435285 \nL 538.192947 158.232449 \nL 536.663481 155.098015 \nL 535.134016 152.096262 \nL 533.60455 149.285041 \nL 532.075085 146.714295 \nL 530.545619 144.424383 \nL 529.016154 142.44514 \nL 527.486688 140.796122 \nL 525.957223 139.487982 \nL 524.427757 138.524361 \nL 522.898292 137.90356 \nL 521.368826 137.61893 \nL 519.839361 137.657634 \nL 518.309895 137.997794 \nL 516.78043 138.605155 \nL 515.250964 139.430918 \nL 513.721499 140.412399 \nL 512.192033 141.476772 \nL 510.662568 142.547168 \nL 509.133102 143.54969 \nL 507.603637 144.419646 \nL 506.074171 145.105704 \nL 504.544706 145.571587 \nL 503.01524 145.796001 \nL 501.485775 145.771893 \nL 499.956309 145.505865 \nL 498.426844 145.017792 \nL 496.897378 144.34009 \nL 495.367913 143.515797 \nL 493.838447 142.594888 \nL 492.308982 141.629006 \nL 490.779516 140.665654 \nL 489.250051 139.743341 \nL 487.720585 138.889022 \nL 486.19112 138.117995 \nL 484.661654 137.435729 \nL 483.132189 136.840488 \nL 481.602723 136.325931 \nL 480.073258 135.883173 \nL 478.543792 135.502257 \nL 477.014327 135.17311 \nL 475.484861 134.886179 \nL 473.955396 134.632906 \nL 472.42593 134.405991 \nL 470.896465 134.199637 \nL 469.366999 134.009598 \nL 467.837534 133.833134 \nL 466.308068 133.668788 \nL 464.778603 133.516176 \nL 463.249137 133.375557 \nL 461.719672 133.24745 \nL 460.190206 133.132155 \nL 458.66074 133.029294 \nL 457.131275 132.9373 \nL 455.601809 132.853134 \nL 454.072344 132.772202 \nL 452.542878 132.68858 \nL 451.013413 132.595586 \nL 449.483947 132.486515 \nL 447.954482 132.355485 \nL 446.425016 132.198078 \nL 444.895551 132.011655 \nL 443.366085 131.795468 \nL 441.83662 131.550429 \nL 440.307154 131.278735 \nL 438.777689 130.983441 \nL 437.248223 130.66795 \nL 435.718758 130.335762 \nL 434.189292 129.990099 \nL 432.659827 129.633846 \nL 431.130361 129.269356 \nL 429.600896 128.89858 \nL 428.07143 128.522974 \nL 426.541965 128.143604 \nL 425.012499 127.761175 \nL 423.483034 127.376219 \nL 421.953568 126.988867 \nL 420.424103 126.59929 \nL 418.894637 126.207421 \nL 417.365172 125.813154 \nL 415.835706 125.416328 \nL 414.306241 125.016739 \nL 412.776775 124.614227 \nL 411.24731 124.208582 \nL 409.717844 123.799649 \nL 408.188379 123.387273 \nL 406.658913 122.971292 \nL 405.129448 122.551652 \nL 403.599982 122.128268 \nL 402.070517 121.70102 \nL 400.541051 121.269946 \nz\n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: #3cb371; fill-opacity: 0.501961; stroke: #3cb371; stroke-opacity: 0.501961\"/>\n   </g>\n   <g id=\"PolyCollection_6\">\n    <path d=\"M 400.541051 99.688711 \nL 400.541051 -10892.431885 \nL 402.070517 -9512.180943 \nL 403.599982 -8301.151261 \nL 405.129448 -7239.564593 \nL 406.658913 -6309.640481 \nL 408.188379 -5495.545498 \nL 409.717844 -4783.247411 \nL 411.24731 -4160.262067 \nL 412.776775 -3615.627344 \nL 414.306241 -3139.641242 \nL 415.835706 -2723.779995 \nL 417.365172 -2360.561032 \nL 418.894637 -2043.38867 \nL 420.424103 -1766.488812 \nL 421.953568 -1524.799309 \nL 423.483034 -1313.876571 \nL 425.012499 -1129.836767 \nL 426.541965 -969.265571 \nL 428.07143 -829.184148 \nL 429.600896 -706.98309 \nL 431.130361 -600.379268 \nL 432.659827 -507.378322 \nL 434.189292 -426.232776 \nL 435.718758 -355.422168 \nL 437.248223 -293.611762 \nL 438.777689 -239.640919 \nL 440.307154 -192.493253 \nL 441.83662 -151.284878 \nL 443.366085 -115.246848 \nL 444.895551 -83.712323 \nL 446.425016 -56.105976 \nL 447.954482 -31.932875 \nL 449.483947 -10.769019 \nL 451.013413 7.749528 \nL 452.542878 23.93804 \nL 454.072344 38.07236 \nL 455.601809 50.396796 \nL 457.131275 61.13026 \nL 458.66074 70.468878 \nL 460.190206 78.587598 \nL 461.719672 85.641667 \nL 463.249137 91.767599 \nL 464.778603 97.085377 \nL 466.308068 101.700492 \nL 467.837534 105.706502 \nL 469.366999 109.186838 \nL 470.896465 112.216725 \nL 472.42593 114.864458 \nL 473.955396 117.192359 \nL 475.484861 119.257419 \nL 477.014327 121.111754 \nL 478.543792 122.802758 \nL 480.073258 124.373256 \nL 481.602723 125.861401 \nL 483.132189 127.300377 \nL 484.661654 128.717764 \nL 486.19112 130.134274 \nL 487.720585 131.561944 \nL 489.250051 133.001897 \nL 490.779516 134.442268 \nL 492.308982 135.857279 \nL 493.838447 137.208426 \nL 495.367913 138.448083 \nL 496.897378 139.525024 \nL 498.426844 140.390347 \nL 499.956309 141.002403 \nL 501.485775 141.329924 \nL 503.01524 141.353451 \nL 504.544706 141.065816 \nL 506.074171 140.472544 \nL 507.603637 139.592853 \nL 509.133102 138.461149 \nL 510.662568 137.128132 \nL 512.192033 135.660182 \nL 513.721499 134.136028 \nL 515.250964 132.640904 \nL 516.78043 131.259362 \nL 518.309895 130.068456 \nL 519.839361 129.132486 \nL 521.368826 128.500197 \nL 522.898292 128.204529 \nL 524.427757 128.264231 \nL 525.957223 128.686271 \nL 527.486688 129.468035 \nL 529.016154 130.598696 \nL 530.545619 132.059471 \nL 532.075085 133.823097 \nL 533.60455 135.853067 \nL 535.134016 138.103289 \nL 536.663481 140.518551 \nL 538.192947 143.035863 \nL 539.722412 145.58604 \nL 541.251878 148.094885 \nL 542.781344 150.483523 \nL 544.310809 152.668306 \nL 545.840275 154.561651 \nL 547.36974 156.074563 \nL 548.899206 157.121426 \nL 550.428671 157.625522 \nL 551.958137 157.523473 \nL 553.487602 156.766937 \nL 555.017068 155.321866 \nL 556.546533 153.167023 \nL 558.075999 150.294401 \nL 559.605464 146.713097 \nL 561.13493 142.457551 \nL 562.664395 137.597454 \nL 564.193861 132.244289 \nL 565.723326 126.548927 \nL 567.252792 120.687604 \nL 568.782257 114.842611 \nL 570.311723 109.18514 \nL 571.841188 103.865211 \nL 573.370654 99.006396 \nL 574.900119 94.701111 \nL 576.429585 91.004824 \nL 577.95905 87.927961 \nL 579.488516 85.426817 \nL 581.017981 83.398061 \nL 582.547447 81.686139 \nL 584.076912 80.105502 \nL 585.606378 78.466056 \nL 587.135843 76.587054 \nL 588.665309 74.296187 \nL 590.194774 71.42124 \nL 591.72424 67.782292 \nL 593.253705 63.186229 \nL 594.783171 57.42195 \nL 596.312636 50.254381 \nL 597.842102 41.416929 \nL 599.371567 30.60291 \nL 600.901033 17.456784 \nL 602.430498 1.566225 \nL 603.959964 -17.548451 \nL 605.489429 -40.44174 \nL 607.018895 -67.754184 \nL 608.54836 -100.221936 \nL 610.077826 -138.689823 \nL 611.607291 -184.12398 \nL 613.136757 -237.626591 \nL 614.666222 -300.448665 \nL 616.195688 -374.011817 \nL 617.725153 -459.927744 \nL 619.254619 -560.01333 \nL 620.784084 -676.321139 \nL 622.31355 -811.16361 \nL 623.843016 -967.135983 \nL 625.372481 -1147.164051 \nL 626.901947 -1354.513847 \nL 628.431412 -1592.855936 \nL 629.960878 -1866.2979 \nL 631.490343 -2179.415129 \nL 633.019809 -2537.346753 \nL 634.549274 -2945.798178 \nL 636.07874 -3411.136173 \nL 637.608205 -3940.458913 \nL 639.137671 -4541.659761 \nL 640.667136 -5223.509499 \nL 642.196602 -5995.760884 \nL 643.726067 -6869.23189 \nL 645.255533 -7855.883196 \nL 646.784998 -8968.964366 \nL 648.314464 -10223.210448 \nL 649.843929 -11634.747227 \nL 651.373395 -13221.363959 \nL 652.90286 -15002.689329 \nL 654.432326 -17000.204981 \nL 655.961791 -19237.393059 \nL 657.491257 -21739.764625 \nL 659.020722 -24534.938166 \nL 660.550188 -27652.753287 \nL 662.079653 -31125.054151 \nL 663.609119 -34985.361924 \nL 665.138584 -39268.900498 \nL 666.66805 -44011.497432 \nL 668.197515 -49249.697647 \nL 669.726981 -55018.446209 \nL 671.256446 -61350.571288 \nL 672.785912 -68274.372186 \nL 674.315377 -75811.478084 \nL 675.844843 -83973.518389 \nL 677.374308 -92758.738944 \nL 678.903774 -102149.928444 \nL 680.433239 -112108.619975 \nL 681.962705 -122574.522538 \nL 683.49217 -133462.841084 \nL 685.021636 -144664.579709 \nL 686.551101 -156049.302819 \nL 688.080567 -167471.220538 \nL 689.610032 -178774.6136 \nL 691.139498 -189805.181225 \nL 692.668963 -200419.7756 \nL 694.198429 -210495.086038 \nL 695.727894 -219934.13585 \nL 697.25736 -228671.046725 \nL 698.786825 -236670.475663 \nL 700.316291 -243926.618788 \nL 701.845756 -250458.057163 \nL 703.375222 -256301.606413 \nL 704.904687 -261507.747256 \nL 704.904687 62.699418 \nL 704.904687 62.699418 \nL 703.375222 63.207478 \nL 701.845756 63.71492 \nL 700.316291 64.221626 \nL 698.786825 64.727661 \nL 697.25736 65.232872 \nL 695.727894 65.737268 \nL 694.198429 66.240802 \nL 692.668963 66.743369 \nL 691.139498 67.244999 \nL 689.610032 67.745546 \nL 688.080567 68.245025 \nL 686.551101 68.74331 \nL 685.021636 69.240356 \nL 683.49217 69.736097 \nL 681.962705 70.230453 \nL 680.433239 70.723302 \nL 678.903774 71.214628 \nL 677.374308 71.704294 \nL 675.844843 72.192279 \nL 674.315377 72.678392 \nL 672.785912 73.16257 \nL 671.256446 73.644702 \nL 669.726981 74.124629 \nL 668.197515 74.602309 \nL 666.66805 75.077551 \nL 665.138584 75.550245 \nL 663.609119 76.020201 \nL 662.079653 76.487365 \nL 660.550188 76.951483 \nL 659.020722 77.41244 \nL 657.491257 77.870002 \nL 655.961791 78.324054 \nL 654.432326 78.774362 \nL 652.90286 79.220705 \nL 651.373395 79.662881 \nL 649.843929 80.100612 \nL 648.314464 80.533752 \nL 646.784998 80.961955 \nL 645.255533 81.385019 \nL 643.726067 81.802631 \nL 642.196602 82.214449 \nL 640.667136 82.620255 \nL 639.137671 83.019674 \nL 637.608205 83.412373 \nL 636.07874 83.797988 \nL 634.549274 84.176169 \nL 633.019809 84.546546 \nL 631.490343 84.908701 \nL 629.960878 85.262206 \nL 628.431412 85.606687 \nL 626.901947 85.941661 \nL 625.372481 86.2667 \nL 623.843016 86.581303 \nL 622.31355 86.88504 \nL 620.784084 87.177378 \nL 619.254619 87.457847 \nL 617.725153 87.725922 \nL 616.195688 87.981091 \nL 614.666222 88.222847 \nL 613.136757 88.450677 \nL 611.607291 88.663982 \nL 610.077826 88.862346 \nL 608.54836 89.045209 \nL 607.018895 89.212135 \nL 605.489429 89.362698 \nL 603.959964 89.496564 \nL 602.430498 89.613521 \nL 600.901033 89.71348 \nL 599.371567 89.796626 \nL 597.842102 89.863414 \nL 596.312636 89.914758 \nL 594.783171 89.95218 \nL 593.253705 89.978114 \nL 591.72424 89.996614 \nL 590.194774 90.014357 \nL 588.665309 90.043183 \nL 587.135843 90.10426 \nL 585.606378 90.235277 \nL 584.076912 90.499439 \nL 582.547447 90.992458 \nL 581.017981 91.839626 \nL 579.488516 93.176783 \nL 577.95905 95.120263 \nL 576.429585 97.741515 \nL 574.900119 101.057892 \nL 573.370654 105.037207 \nL 571.841188 109.606536 \nL 570.311723 114.660788 \nL 568.782257 120.06905 \nL 567.252792 125.679137 \nL 565.723326 131.32152 \nL 564.193861 136.81718 \nL 562.664395 141.992474 \nL 561.13493 146.696999 \nL 559.605464 150.817312 \nL 558.075999 154.279975 \nL 556.546533 157.045995 \nL 555.017068 159.101816 \nL 553.487602 160.451667 \nL 551.958137 161.114059 \nL 550.428671 161.12156 \nL 548.899206 160.522395 \nL 547.36974 159.381148 \nL 545.840275 157.776743 \nL 544.310809 155.797427 \nL 542.781344 153.534526 \nL 541.251878 151.077081 \nL 539.722412 148.509112 \nL 538.192947 145.908856 \nL 536.663481 143.349104 \nL 535.134016 140.897084 \nL 533.60455 138.613428 \nL 532.075085 136.550653 \nL 530.545619 134.751832 \nL 529.016154 133.250129 \nL 527.486688 132.069247 \nL 525.957223 131.224479 \nL 524.427757 130.723715 \nL 522.898292 130.567852 \nL 521.368826 130.750039 \nL 519.839361 131.253856 \nL 518.309895 132.050795 \nL 516.78043 133.098027 \nL 515.250964 134.3377 \nL 513.721499 135.698791 \nL 512.192033 137.101661 \nL 510.662568 138.464552 \nL 509.133102 139.710763 \nL 507.603637 140.774742 \nL 506.074171 141.605815 \nL 504.544706 142.169239 \nL 503.01524 142.445495 \nL 501.485775 142.429083 \nL 499.956309 142.127726 \nL 498.426844 141.562055 \nL 496.897378 140.765187 \nL 495.367913 139.781285 \nL 493.838447 138.662407 \nL 492.308982 137.463575 \nL 490.779516 136.236937 \nL 489.250051 135.026449 \nL 487.720585 133.864589 \nL 486.19112 132.771545 \nL 484.661654 131.756551 \nL 483.132189 130.820343 \nL 481.602723 129.957859 \nL 480.073258 129.160522 \nL 478.543792 128.418065 \nL 477.014327 127.719816 \nL 475.484861 127.055657 \nL 473.955396 126.416754 \nL 472.42593 125.796015 \nL 470.896465 125.18844 \nL 469.366999 124.591167 \nL 467.837534 124.003332 \nL 466.308068 123.425686 \nL 464.778603 122.860171 \nL 463.249137 122.309253 \nL 461.719672 121.775379 \nL 460.190206 121.26035 \nL 458.66074 120.764879 \nL 457.131275 120.28814 \nL 455.601809 119.827628 \nL 454.072344 119.379225 \nL 452.542878 118.937509 \nL 451.013413 118.496339 \nL 449.483947 118.049512 \nL 447.954482 117.591472 \nL 446.425016 117.117889 \nL 444.895551 116.625902 \nL 443.366085 116.114244 \nL 441.83662 115.583092 \nL 440.307154 115.03379 \nL 438.777689 114.468507 \nL 437.248223 113.889836 \nL 435.718758 113.300563 \nL 434.189292 112.703366 \nL 432.659827 112.100716 \nL 431.130361 111.494709 \nL 429.600896 110.887125 \nL 428.07143 110.279371 \nL 426.541965 109.67252 \nL 425.012499 109.067305 \nL 423.483034 108.464338 \nL 421.953568 107.863818 \nL 420.424103 107.266001 \nL 418.894637 106.670882 \nL 417.365172 106.07842 \nL 415.835706 105.488491 \nL 414.306241 104.900912 \nL 412.776775 104.315537 \nL 411.24731 103.732138 \nL 409.717844 103.150536 \nL 408.188379 102.570547 \nL 406.658913 101.991948 \nL 405.129448 101.414618 \nL 403.599982 100.838419 \nL 402.070517 100.263125 \nL 400.541051 99.688711 \nz\n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: #3cb371; fill-opacity: 0.501961; stroke: #3cb371; stroke-opacity: 0.501961\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_8\">\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"400.541051\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- −10 -->\n      <g transform=\"translate(389.988707 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_21\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"451.268324\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- −5 -->\n      <g transform=\"translate(443.89723 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"501.995597\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 0 -->\n      <g transform=\"translate(498.814347 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"552.722869\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 5 -->\n      <g transform=\"translate(549.541619 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"603.450142\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 10 -->\n      <g transform=\"translate(597.087642 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"654.177415\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 15 -->\n      <g transform=\"translate(647.814915 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m26db2d5074\" x=\"704.904687\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 20 -->\n      <g transform=\"translate(698.542187 269.604837) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"400.541051\" y=\"255.0064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- −20 -->\n      <g transform=\"translate(372.436364 258.805619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"400.541051\" y=\"199.5664\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- −10 -->\n      <g transform=\"translate(372.436364 203.365619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"400.541051\" y=\"144.1264\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0 -->\n      <g transform=\"translate(387.178551 147.925619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"400.541051\" y=\"88.6864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 10 -->\n      <g transform=\"translate(380.816051 92.485619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_31\">\n      <g>\n       <use xlink:href=\"#m69b353a02d\" x=\"400.541051\" y=\"33.2464\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 20 -->\n      <g transform=\"translate(380.816051 37.045619) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 400.541051 110.479329 \nL 425.012499 118.41424 \nL 437.248223 122.278893 \nL 443.366085 123.954856 \nL 447.954482 124.973478 \nL 455.601809 126.340381 \nL 461.719672 127.511414 \nL 467.837534 128.918232 \nL 473.955396 130.52483 \nL 478.543792 131.960161 \nL 481.602723 133.141895 \nL 484.661654 134.59614 \nL 487.720585 136.376805 \nL 490.779516 138.451295 \nL 495.367913 141.648541 \nL 496.897378 142.552638 \nL 498.426844 143.289923 \nL 499.956309 143.816796 \nL 501.485775 144.100488 \nL 503.01524 144.120748 \nL 504.544706 143.870413 \nL 506.074171 143.35576 \nL 507.603637 142.597194 \nL 509.133102 141.630227 \nL 512.192033 139.289217 \nL 515.250964 136.884309 \nL 516.78043 135.851591 \nL 518.309895 135.024294 \nL 519.839361 134.455745 \nL 521.368826 134.184484 \nL 522.898292 134.235706 \nL 524.427757 134.624038 \nL 525.957223 135.35623 \nL 527.486688 136.432685 \nL 529.016154 137.847634 \nL 530.545619 139.588107 \nL 532.075085 141.632474 \nL 533.60455 143.949234 \nL 536.663481 149.22356 \nL 544.310809 163.254449 \nL 545.840275 165.589455 \nL 547.36974 167.55349 \nL 548.899206 169.051076 \nL 550.428671 169.996408 \nL 551.958137 170.319732 \nL 553.487602 169.970166 \nL 555.017068 168.915217 \nL 556.546533 167.139124 \nL 558.075999 164.642732 \nL 559.605464 161.44631 \nL 561.13493 157.595913 \nL 562.664395 153.170558 \nL 565.723326 143.090392 \nL 570.311723 127.329301 \nL 571.841188 122.560607 \nL 573.370654 118.271087 \nL 574.900119 114.570652 \nL 576.429585 111.536517 \nL 577.95905 109.202868 \nL 579.488516 107.551703 \nL 581.017981 106.509266 \nL 582.547447 105.956469 \nL 584.076912 105.755149 \nL 585.606378 105.778434 \nL 588.665309 106.145205 \nL 597.842102 107.500624 \nL 602.430498 107.94384 \nL 607.018895 108.19364 \nL 611.607291 108.260459 \nL 617.725153 108.094231 \nL 623.843016 107.674665 \nL 629.960878 107.04101 \nL 637.608205 106.000849 \nL 645.255533 104.738905 \nL 654.432326 102.997821 \nL 665.138584 100.731989 \nL 677.374308 97.920031 \nL 692.668963 94.176651 \nL 704.904687 91.05704 \nL 704.904687 91.05704 \n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: none; stroke: #000000; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 400.541051 174.28693 \nL 403.599982 158.660522 \nL 408.188379 135.022445 \nL 409.717844 127.819794 \nL 411.24731 121.224589 \nL 412.776775 115.371547 \nL 414.306241 110.373499 \nL 415.835706 106.319283 \nL 417.365172 103.272285 \nL 418.894637 101.269653 \nL 420.424103 100.322171 \nL 421.953568 100.414809 \nL 423.483034 101.507889 \nL 425.012499 103.538837 \nL 426.541965 106.424458 \nL 428.07143 110.063668 \nL 429.600896 114.340596 \nL 431.130361 119.127965 \nL 434.189292 129.689425 \nL 438.777689 145.922977 \nL 440.307154 150.915421 \nL 441.83662 155.507969 \nL 443.366085 159.606827 \nL 444.895551 163.134741 \nL 446.425016 166.032396 \nL 447.954482 168.259288 \nL 449.483947 169.794073 \nL 451.013413 170.634386 \nL 452.542878 170.796164 \nL 454.072344 170.312488 \nL 455.601809 169.231995 \nL 457.131275 167.616927 \nL 458.66074 165.540861 \nL 460.190206 163.086197 \nL 463.249137 157.398722 \nL 469.366999 145.453285 \nL 470.896465 142.831578 \nL 472.42593 140.489553 \nL 473.955396 138.474625 \nL 475.484861 136.821238 \nL 477.014327 135.550345 \nL 478.543792 134.669304 \nL 480.073258 134.172199 \nL 481.602723 134.040564 \nL 483.132189 134.24447 \nL 484.661654 134.74396 \nL 486.19112 135.490749 \nL 487.720585 136.430161 \nL 490.779516 138.648866 \nL 493.838447 140.916335 \nL 495.367913 141.925185 \nL 496.897378 142.784615 \nL 498.426844 143.454477 \nL 499.956309 143.903911 \nL 501.485775 144.112406 \nL 503.01524 144.070496 \nL 504.544706 143.78008 \nL 506.074171 143.254362 \nL 507.603637 142.517399 \nL 509.133102 141.603279 \nL 512.192033 139.422773 \nL 515.250964 137.134437 \nL 516.78043 136.099196 \nL 518.309895 135.217607 \nL 519.839361 134.54745 \nL 521.368826 134.141497 \nL 522.898292 134.04546 \nL 524.427757 134.296134 \nL 525.957223 134.919766 \nL 527.486688 135.930744 \nL 529.016154 137.330621 \nL 530.545619 139.107523 \nL 532.075085 141.235966 \nL 533.60455 143.677104 \nL 536.663481 149.279656 \nL 542.781344 161.282986 \nL 544.310809 163.941114 \nL 545.840275 166.279171 \nL 547.36974 168.210345 \nL 548.899206 169.654886 \nL 550.428671 170.542728 \nL 551.958137 170.815926 \nL 553.487602 170.430827 \nL 555.017068 169.35991 \nL 556.546533 167.593232 \nL 558.075999 165.139434 \nL 559.605464 162.026251 \nL 561.13493 158.300522 \nL 562.664395 154.027667 \nL 564.193861 149.290631 \nL 567.252792 138.833577 \nL 571.841188 122.536614 \nL 573.370654 117.483679 \nL 574.900119 112.851757 \nL 576.429585 108.77375 \nL 577.95905 105.373525 \nL 579.488516 102.762386 \nL 581.017981 101.035784 \nL 582.547447 100.27032 \nL 584.076912 100.521166 \nL 585.606378 101.819951 \nL 587.135843 104.173201 \nL 588.665309 107.56137 \nL 590.194774 111.938519 \nL 591.72424 117.232658 \nL 593.253705 123.346754 \nL 594.783171 130.160418 \nL 597.842102 145.302561 \nL 603.959964 176.748579 \nL 605.489429 183.747951 \nL 607.018895 190.027693 \nL 608.54836 195.416911 \nL 610.077826 199.761495 \nL 611.607291 202.928334 \nL 613.136757 204.809123 \nL 614.666222 205.323643 \nL 616.195688 204.422437 \nL 617.725153 202.088785 \nL 619.254619 198.33993 \nL 620.784084 193.227491 \nL 622.31355 186.837058 \nL 623.843016 179.286933 \nL 625.372481 170.726052 \nL 628.431412 151.302982 \nL 636.07874 99.801352 \nL 637.608205 90.942276 \nL 639.137671 83.114795 \nL 640.667136 76.525505 \nL 642.196602 71.356223 \nL 643.726067 67.759127 \nL 645.255533 65.852516 \nL 646.784998 65.717287 \nL 648.314464 67.394252 \nL 649.843929 70.882353 \nL 651.373395 76.13785 \nL 652.90286 83.074505 \nL 654.432326 91.564797 \nL 655.961791 101.442134 \nL 657.491257 112.504041 \nL 660.550188 137.217723 \nL 666.66805 189.0875 \nL 668.197515 200.805645 \nL 669.726981 211.43425 \nL 671.256446 220.706466 \nL 672.785912 228.382344 \nL 674.315377 234.254955 \nL 675.844843 238.155826 \nL 677.374308 239.959559 \nL 678.903774 239.587517 \nL 680.433239 237.010458 \nL 681.962705 232.250061 \nL 683.49217 225.37926 \nL 685.021636 216.521391 \nL 686.551101 205.848126 \nL 688.080567 193.576251 \nL 689.610032 179.963312 \nL 692.668963 149.91514 \nL 697.25736 102.902013 \nL 700.316291 74.461555 \nL 701.845756 62.156933 \nL 703.375222 51.545799 \nL 704.904687 42.899031 \nL 704.904687 42.899031 \n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: none; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 400.541051 155.99173 \nL 403.599982 140.866789 \nL 408.188379 117.980913 \nL 409.717844 111.028996 \nL 411.24731 104.684525 \nL 412.776775 99.082216 \nL 414.306241 94.334902 \nL 415.835706 90.531419 \nL 417.365172 87.735156 \nL 418.894637 85.983257 \nL 420.424103 85.286508 \nL 421.953568 85.629881 \nL 423.483034 86.973695 \nL 425.012499 89.255375 \nL 426.541965 92.39173 \nL 428.07143 96.281674 \nL 429.600896 100.809336 \nL 432.659827 111.260873 \nL 440.307154 139.139297 \nL 441.83662 143.982578 \nL 443.366085 148.33217 \nL 444.895551 152.110818 \nL 446.425016 155.259206 \nL 447.954482 157.736832 \nL 449.483947 159.52235 \nL 451.013413 160.613397 \nL 452.542878 161.025909 \nL 454.072344 160.792966 \nL 455.601809 159.963207 \nL 457.131275 158.598873 \nL 458.66074 156.77354 \nL 460.190206 154.56961 \nL 463.249137 149.383602 \nL 467.837534 141.032319 \nL 469.366999 138.4411 \nL 470.896465 136.070127 \nL 472.42593 133.978836 \nL 473.955396 132.214641 \nL 475.484861 130.811987 \nL 477.014327 129.791828 \nL 478.543792 129.161521 \nL 480.073258 128.91515 \nL 481.602723 129.034248 \nL 483.132189 129.488888 \nL 484.661654 130.239112 \nL 486.19112 131.236634 \nL 489.250051 133.750577 \nL 493.838447 137.915889 \nL 495.367913 139.175473 \nL 496.897378 140.285637 \nL 498.426844 141.206231 \nL 499.956309 141.906399 \nL 501.485775 142.365628 \nL 503.01524 142.24014 \nL 504.544706 141.69899 \nL 506.074171 140.922539 \nL 507.603637 139.934842 \nL 510.662568 137.470933 \nL 516.78043 132.012238 \nL 518.309895 130.879915 \nL 519.839361 129.959024 \nL 521.368826 129.302337 \nL 522.898292 128.955567 \nL 524.427757 128.955506 \nL 525.957223 129.328405 \nL 527.486688 130.08865 \nL 529.016154 131.237793 \nL 530.545619 132.763961 \nL 532.075085 134.641671 \nL 533.60455 136.832075 \nL 536.663481 141.93316 \nL 542.781344 152.933555 \nL 544.310809 155.34095 \nL 545.840275 157.428273 \nL 547.36974 159.108713 \nL 548.899206 160.30252 \nL 550.428671 160.939629 \nL 551.958137 160.962093 \nL 553.487602 160.32626 \nL 555.017068 159.004609 \nL 556.546533 156.987198 \nL 558.075999 154.282666 \nL 559.605464 150.918749 \nL 561.13493 146.942287 \nL 562.664395 142.418698 \nL 565.723326 132.077891 \nL 573.370654 104.119574 \nL 574.900119 99.236919 \nL 576.429585 94.908178 \nL 577.95905 91.257219 \nL 579.488516 88.395347 \nL 581.017981 86.418011 \nL 582.547447 85.401813 \nL 584.076912 85.401926 \nL 585.606378 86.449978 \nL 587.135843 88.552493 \nL 588.665309 91.689929 \nL 590.194774 95.816344 \nL 591.72424 100.859749 \nL 593.253705 106.723112 \nL 594.783171 113.286042 \nL 597.842102 127.926718 \nL 603.959964 158.369801 \nL 605.489429 165.11844 \nL 607.018895 171.147448 \nL 608.54836 176.285932 \nL 610.077826 180.379782 \nL 611.607291 183.295888 \nL 613.136757 184.925943 \nL 614.666222 185.18973 \nL 616.195688 184.03779 \nL 617.725153 181.453405 \nL 619.254619 177.453815 \nL 620.784084 172.090643 \nL 622.31355 165.449476 \nL 623.843016 157.648617 \nL 625.372481 148.837003 \nL 628.431412 128.912465 \nL 636.07874 76.157167 \nL 637.608205 67.047358 \nL 639.137671 58.969143 \nL 640.667136 52.129119 \nL 642.196602 46.709103 \nL 643.726067 42.861274 \nL 645.255533 40.703929 \nL 646.784998 40.317966 \nL 648.314464 41.744198 \nL 649.843929 44.981565 \nL 651.373395 49.986328 \nL 652.90286 56.67225 \nL 654.432326 64.911808 \nL 655.961791 74.538411 \nL 657.491257 85.349585 \nL 660.550188 109.561799 \nL 666.66805 160.428642 \nL 668.197515 171.896053 \nL 669.726981 182.273925 \nL 671.256446 191.295406 \nL 672.785912 198.720551 \nL 674.315377 204.342428 \nL 675.844843 207.992566 \nL 677.374308 209.545565 \nL 678.903774 208.922789 \nL 680.433239 206.094997 \nL 681.962705 201.083866 \nL 683.49217 193.962332 \nL 685.021636 184.853728 \nL 686.551101 173.92973 \nL 688.080567 161.407121 \nL 689.610032 147.543449 \nL 692.668963 116.99381 \nL 697.25736 69.228482 \nL 700.316291 40.286556 \nL 701.845756 27.731201 \nL 703.375222 16.869332 \nL 704.904687 7.971831 \nL 704.904687 7.971831 \n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 400.541051 192.58213 \nL 403.599982 176.454254 \nL 408.188379 152.063976 \nL 409.717844 144.610592 \nL 411.24731 137.764654 \nL 412.776775 131.660878 \nL 414.306241 126.412096 \nL 415.835706 122.107146 \nL 417.365172 118.809415 \nL 418.894637 116.556049 \nL 420.424103 115.357833 \nL 421.953568 115.199738 \nL 423.483034 116.042084 \nL 425.012499 117.822298 \nL 426.541965 120.457185 \nL 428.07143 123.845662 \nL 429.600896 127.871857 \nL 431.130361 132.408492 \nL 434.189292 142.468485 \nL 438.777689 157.949835 \nL 440.307154 162.691546 \nL 441.83662 167.03336 \nL 443.366085 170.881484 \nL 444.895551 174.158665 \nL 446.425016 176.805586 \nL 447.954482 178.781745 \nL 449.483947 180.065795 \nL 451.013413 180.655375 \nL 452.542878 180.56642 \nL 454.072344 179.832009 \nL 455.601809 178.500783 \nL 457.131275 176.634982 \nL 458.66074 174.308181 \nL 460.190206 171.602784 \nL 463.249137 165.413841 \nL 469.366999 152.46547 \nL 470.896465 149.59303 \nL 472.42593 147.000271 \nL 473.955396 144.734608 \nL 475.484861 142.830488 \nL 477.014327 141.308861 \nL 478.543792 140.177087 \nL 480.073258 139.429249 \nL 481.602723 139.046879 \nL 483.132189 139.000052 \nL 484.661654 139.248808 \nL 486.19112 139.744863 \nL 487.720585 140.433542 \nL 490.779516 142.150779 \nL 493.838447 143.916782 \nL 495.367913 144.674898 \nL 496.897378 145.283594 \nL 498.426844 145.702722 \nL 499.956309 145.901423 \nL 504.544706 145.861169 \nL 506.074171 145.586185 \nL 507.603637 145.099955 \nL 509.133102 144.436569 \nL 512.192033 142.757531 \nL 515.250964 140.970663 \nL 516.78043 140.186155 \nL 518.309895 139.5553 \nL 519.839361 139.135876 \nL 521.368826 138.980656 \nL 522.898292 139.135354 \nL 524.427757 139.636761 \nL 525.957223 140.511126 \nL 527.486688 141.772839 \nL 529.016154 143.42345 \nL 530.545619 145.451085 \nL 532.075085 147.830262 \nL 533.60455 150.522133 \nL 536.663481 156.626153 \nL 542.781344 169.632417 \nL 544.310809 172.541279 \nL 545.840275 175.13007 \nL 547.36974 177.311977 \nL 548.899206 179.007252 \nL 550.428671 180.145828 \nL 551.958137 180.669759 \nL 553.487602 180.535394 \nL 555.017068 179.71521 \nL 556.546533 178.199267 \nL 558.075999 175.996201 \nL 559.605464 173.133752 \nL 561.13493 169.658757 \nL 562.664395 165.636636 \nL 564.193861 161.150334 \nL 567.252792 151.194747 \nL 571.841188 135.649984 \nL 573.370654 130.847783 \nL 574.900119 126.466595 \nL 576.429585 122.639322 \nL 577.95905 119.48983 \nL 579.488516 117.129425 \nL 581.017981 115.653557 \nL 582.547447 115.138826 \nL 584.076912 115.640406 \nL 585.606378 117.189925 \nL 587.135843 119.793909 \nL 588.665309 123.432811 \nL 590.194774 128.060694 \nL 591.72424 133.605566 \nL 593.253705 139.970397 \nL 594.783171 147.034794 \nL 597.842102 162.678404 \nL 603.959964 195.127357 \nL 605.489429 202.377463 \nL 607.018895 208.907938 \nL 608.54836 214.54789 \nL 610.077826 219.143207 \nL 611.607291 222.56078 \nL 613.136757 224.692303 \nL 614.666222 225.457557 \nL 616.195688 224.807084 \nL 617.725153 222.724166 \nL 619.254619 219.226044 \nL 620.784084 214.36434 \nL 622.31355 208.22464 \nL 623.843016 200.925248 \nL 625.372481 192.615101 \nL 628.431412 173.693499 \nL 636.07874 123.445537 \nL 637.608205 114.837195 \nL 639.137671 107.260447 \nL 640.667136 100.921891 \nL 642.196602 96.003342 \nL 643.726067 92.656981 \nL 645.255533 91.001103 \nL 646.784998 91.116607 \nL 648.314464 93.044306 \nL 649.843929 96.783141 \nL 651.373395 102.289371 \nL 652.90286 109.47676 \nL 654.432326 118.217786 \nL 655.961791 128.345856 \nL 657.491257 139.658497 \nL 660.550188 164.873646 \nL 666.66805 217.746358 \nL 668.197515 229.715237 \nL 669.726981 240.594576 \nL 671.256446 250.117525 \nL 672.785912 258.044137 \nL 674.315377 264.167482 \nL 675.844843 268.319086 \nL 677.374308 270.373553 \nL 678.903774 270.252244 \nL 680.433239 267.92592 \nL 681.962705 263.416256 \nL 683.49217 256.796189 \nL 685.021636 248.189053 \nL 686.551101 237.766522 \nL 688.080567 225.74538 \nL 689.610032 212.383176 \nL 692.668963 182.836471 \nL 697.25736 136.575545 \nL 700.316291 108.636554 \nL 701.845756 96.582666 \nL 703.375222 86.222265 \nL 704.904687 77.826231 \nL 704.904687 77.826231 \n\" clip-path=\"url(#p4130a5dda9)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 400.541051 255.0064 \nL 400.541051 33.2464 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 704.904687 255.0064 \nL 704.904687 33.2464 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 400.541051 255.0064 \nL 704.904687 255.0064 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 400.541051 33.2464 \nL 704.904687 33.2464 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_20\">\n     <path d=\"M 428.996307 59.557812 \nL 676.449432 59.557812 \nQ 678.449432 59.557812 678.449432 57.557812 \nL 678.449432 9.2 \nQ 678.449432 7.2 676.449432 7.2 \nL 428.996307 7.2 \nQ 426.996307 7.2 426.996307 9.2 \nL 426.996307 57.557812 \nQ 426.996307 59.557812 428.996307 59.557812 \nz\n\" style=\"fill: #ffffff; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"patch_21\">\n     <path d=\"M 430.996307 20.4 \nL 450.996307 20.4 \nL 450.996307 13.4 \nL 430.996307 13.4 \nz\n\" style=\"fill: #404040; fill-opacity: 0.25098; stroke: #404040; stroke-opacity: 0.25098; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"text_31\">\n     <!-- ood -->\n     <g transform=\"translate(458.996307 20.4) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-6f\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"61.181641\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"122.363281\"/>\n     </g>\n    </g>\n    <g id=\"patch_22\">\n     <path d=\"M 430.996307 35.078125 \nL 450.996307 35.078125 \nL 450.996307 28.078125 \nL 430.996307 28.078125 \nz\n\" style=\"fill: #8a2be2; fill-opacity: 0.501961; stroke: #8a2be2; stroke-opacity: 0.501961; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"text_32\">\n     <!-- $\\mathbb{E}[\\sigma]$ (aleatoric) -->\n     <g transform=\"translate(458.996307 35.078125) scale(0.1 -0.1)\">\n      <use xlink:href=\"#STIXNonUnicode-Italic-e156\" transform=\"translate(0 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-5b\" transform=\"translate(63.899994 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3c3\" transform=\"translate(102.913666 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-5d\" transform=\"translate(166.292572 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(205.306244 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(237.093353 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(276.107025 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(337.386322 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(365.169525 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(426.692963 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(487.97226 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(527.181244 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(588.362885 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(629.476166 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(657.259369 0.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(712.239838 0.015625)\"/>\n     </g>\n    </g>\n    <g id=\"patch_23\">\n     <path d=\"M 430.996307 53.457812 \nL 450.996307 53.457812 \nL 450.996307 46.457812 \nL 430.996307 46.457812 \nz\n\" style=\"fill: #3cb371; fill-opacity: 0.501961; stroke: #3cb371; stroke-opacity: 0.501961; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"text_33\">\n     <!-- $\\sqrt{\\mathrm{Var}[\\mu]}$ (epistemic) -->\n     <g transform=\"translate(458.996307 53.457812) scale(0.1 -0.1)\">\n      <use xlink:href=\"#STIXSizeOneSym-Regular-221a\" transform=\"translate(0 -0.09375) scale(0.693173)\"/>\n      <use xlink:href=\"#DejaVuSans-56\" transform=\"translate(85.829376 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(154.237579 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(215.516876 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-5b\" transform=\"translate(256.630157 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(295.643829 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-5d\" transform=\"translate(359.266876 0.765625)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(410.780548 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(442.567657 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(481.581329 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(543.104767 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(606.581329 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(634.364532 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(686.464142 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(725.673126 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" transform=\"translate(787.196564 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(884.608673 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(912.391876 0.34375)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(967.372345 0.34375)\"/>\n      <path d=\"M 73.329376 100.5 \nL 73.329376 106.75 \nL 410.780548 106.75 \nL 410.780548 100.5 \nL 73.329376 100.5 \nz\n\"/>\n     </g>\n    </g>\n    <g id=\"line2d_36\">\n     <path d=\"M 579.696307 16.9 \nL 589.696307 16.9 \nL 599.696307 16.9 \n\" style=\"fill: none; stroke: #000000; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_34\">\n     <!-- $\\hat \\mu$ -->\n     <g transform=\"translate(607.696307 20.4) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-302\" transform=\"translate(62.699219 12.015625)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(0 0.828125)\"/>\n     </g>\n    </g>\n    <g id=\"line2d_37\">\n     <path d=\"M 579.696307 31.598437 \nL 589.696307 31.598437 \nL 599.696307 31.598437 \n\" style=\"fill: none; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_35\">\n     <!-- true mean -->\n     <g transform=\"translate(607.696307 35.098437) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"143.701172\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"205.224609\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"237.011719\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"334.423828\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"395.947266\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"457.226562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_38\">\n     <path d=\"M 579.696307 46.276562 \nL 589.696307 46.276562 \nL 599.696307 46.276562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #888888; stroke-opacity: 0.501961; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_36\">\n     <!-- true variance -->\n     <g transform=\"translate(607.696307 49.776562) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"143.701172\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"205.224609\"/>\n      <use xlink:href=\"#DejaVuSans-76\" x=\"237.011719\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"296.191406\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"357.470703\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"398.583984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"426.367188\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"487.646484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"551.025391\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"606.005859\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0613878dd6\">\n   <rect x=\"35.304688\" y=\"33.2464\" width=\"304.363636\" height=\"221.76\"/>\n  </clipPath>\n  <clipPath id=\"p4130a5dda9\">\n   <rect x=\"400.541051\" y=\"33.2464\" width=\"304.363636\" height=\"221.76\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"Now that you've executed the code,\n\n(a) study the figures and report your observations. What does the regularizer improve exactly?\n\n**WRITE YOUR ANSWER HERE** <br>\nThe regularizer improves the estimates of aleatoric and epistemic uncertainty. <br>\n\nThe epistemic uncertainty is high in the gray regions since there are no data points. Without the regularizer, the model predicts a low epistemic uncertainty in these regions. By adding the regularizer, we get the correct epistemic uncertainty estimates for these regions.\n\nThe aleatoric uncertainty is high in the noisy parts of the white region. Without the regularizer, the model predicts a low aleatoric uncertainty in the noisy regions. By adding the regularizer, we get the correct aleatoric uncertainty estimate for the noisy regions.\n\n\n(b) The provided regularizer $\\lfloor (y - \\mu_0)^2 \\rfloor \\left( \\frac{\\beta}{\\alpha - 1} \\right)^{-1} + 2\\alpha$ mirrors another well known negative log-likelihood function closely. State which one and how this might help obtain accurate uncertainty forecasts.\n\n**WRITE YOUR ANSWER HERE**\n\n(c) Propose a novel regularizer based on the interpretations of the estimated parameters $\\{\\mu_0, \\nu, \\alpha, \\beta\\}$ seen in the lecture. You may further use $y, \\hat y$ in your formulation. Argue why your proposed regularizer should be a reasonable choice. You can test your proposition using the code above. (Sound argumentation will net full points, even if your regularizer does not improve performance empirically.)\n\n**WRITE YOUR ANSWER HERE**","metadata":{}},{"cell_type":"markdown","source":"You did it! This was the last exercise of the course. I hope you had a great time! Best of luck on the exam!","metadata":{"_uuid":"432b839d-69fb-447b-bd6a-0132bfd37f57","_cell_guid":"c280149b-4196-41dd-9e8f-bfd6994b1e05","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}